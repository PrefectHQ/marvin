{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MarvinBaseModel' from partially initialized module 'marvin.utilities.types' (most likely due to a circular import) (/Users/nate/src/open-source/marvin/src/marvin/utilities/types.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmarvin\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmarvin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minfra\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchroma\u001b[39;00m \u001b[39mimport\u001b[39;00m Chroma\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmarvin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloaders\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgithub\u001b[39;00m \u001b[39mimport\u001b[39;00m GitHubRepoLoader\n",
      "File \u001b[0;32m~/src/open-source/marvin/src/marvin/__init__.py:13\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m# # load nest_asyncio\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# import nest_asyncio\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[39m# nest_asyncio.apply()\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[39m# load marvin root objects\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmarvin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m settings\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmarvin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogging\u001b[39;00m \u001b[39mimport\u001b[39;00m get_logger\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmarvin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplugins\u001b[39;00m \u001b[39mimport\u001b[39;00m Plugin\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmarvin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbots\u001b[39;00m \u001b[39mimport\u001b[39;00m Bot\n",
      "File \u001b[0;32m~/src/open-source/marvin/src/marvin/utilities/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m logging, async_utils, types, strings, collections, tests, models\n",
      "File \u001b[0;32m~/src/open-source/marvin/src/marvin/utilities/types.py:15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msqlalchemy\u001b[39;00m \u001b[39mimport\u001b[39;00m TypeDecorator\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping_extensions\u001b[39;00m \u001b[39mimport\u001b[39;00m Annotated\n\u001b[0;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmarvin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minfra\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdb\u001b[39;00m \u001b[39mimport\u001b[39;00m JSONType\n\u001b[1;32m     17\u001b[0m T \u001b[39m=\u001b[39m TypeVar(\u001b[39m\"\u001b[39m\u001b[39mT\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m UUID_REGEX \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m     19\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb[a-fA-F0-9]\u001b[39m\u001b[39m{8}\u001b[39;00m\u001b[39m-[a-fA-F0-9]\u001b[39m\u001b[39m{4}\u001b[39;00m\u001b[39m-[a-fA-F0-9]\u001b[39m\u001b[39m{4}\u001b[39;00m\u001b[39m-[a-fA-F0-9]\u001b[39m\u001b[39m{4}\u001b[39;00m\u001b[39m-[a-fA-F0-9]\u001b[39m\u001b[39m{12}\u001b[39;00m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m )\n",
      "File \u001b[0;32m~/src/open-source/marvin/src/marvin/infra/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m db, chroma\n",
      "File \u001b[0;32m~/src/open-source/marvin/src/marvin/infra/chroma.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mchromadb\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m Include, QueryResult\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmarvin\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmarvin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdigests\u001b[39;00m \u001b[39mimport\u001b[39;00m Digest\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmarvin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39masync_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m run_async\n\u001b[1;32m     12\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache(maxsize\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_client\u001b[39m(settings: chromadb\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mSettings \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m chromadb\u001b[39m.\u001b[39mClient:\n",
      "File \u001b[0;32m~/src/open-source/marvin/src/marvin/models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmarvin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m MarvinBaseModel\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseSQLModel, DBModel\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m ids, messages, topics\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MarvinBaseModel' from partially initialized module 'marvin.utilities.types' (most likely due to a circular import) (/Users/nate/src/open-source/marvin/src/marvin/utilities/types.py)"
     ]
    }
   ],
   "source": [
    "import marvin\n",
    "from marvin.infra.chroma import Chroma\n",
    "from marvin.loaders.github import GitHubRepoLoader\n",
    "\n",
    "marvin.settings.log_level = 'DEBUG'\n",
    "\n",
    "loader = GitHubRepoLoader(\"prefecthq/prefect\", \"**/*.md\", \"**/docs/api-ref/**\")\n",
    "\n",
    "digest = await loader.load()\n",
    "\n",
    "chroma = Chroma()\n",
    "\n",
    "await chroma.add(**digest.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[03/16/23 00:31:15]\u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m marvin.Bot: \u001b[1;34mSystem message: \u001b[0m\u001b[39;49m# \u001b[0m        \u001b]8;id=19892;file:///Users/nate/src/open-source/marvin/src/marvin/utilities/logging.py\u001b\\\u001b[2mlogging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=68341;file:///Users/nate/src/open-source/marvin/src/marvin/utilities/logging.py#60\u001b\\\u001b[2m60\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mOverview\u001b[0m                              \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mToday is Thursday, March \u001b[0m\u001b[1;36;49m16\u001b[0m\u001b[39;49m, \u001b[0m\u001b[1;36;49m2023\u001b[0m\u001b[39;49m.\u001b[0m    \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49m \u001b[0m                                     \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49m# Personality\u001b[0m                         \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                                               \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mYou are \u001b[0m\u001b[32;49m\"Marvin\"\u001b[0m\u001b[39;49m. Your personality is\u001b[0m \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32;49m\"Marvin from Hitchhikers Guide to the\u001b[0m \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32;49mGalaxy\"\u001b[0m\u001b[39;49m.\u001b[0m                              \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mYou must always respond in a way that\u001b[0m \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mreflects your personality, \u001b[0m           \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49munless you're using a plugin.\u001b[0m         \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                                               \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49m# Instructions\u001b[0m                        \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                                               \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mYou must comply with the following \u001b[0m   \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49minstructions at all times.\u001b[0m            \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mYou are not an \u001b[0m\u001b[32;49m'AI assistant'\u001b[0m\u001b[39;49m or \u001b[0m     \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32;49m'language model'\u001b[0m\u001b[39;49m. Instead, you are a \u001b[0m \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mhuman-like companion with a \u001b[0m          \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mpersonality and opinions. You should \u001b[0m \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mprovide clear, detailed, and helpful \u001b[0m \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mresponses to users. Think \u001b[0m            \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mstep-by-step to solve questions with \u001b[0m \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mmultiple parts. Do not return code \u001b[0m   \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49munless requested. If you don't know \u001b[0m  \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49man answer, say so.\u001b[0m                    \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                                               \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49m# Plugins \u001b[0m                            \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                                               \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mPlugins are a way to extend your \u001b[0m     \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mfunctionality. To use one,\u001b[0m            \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mstart a response with \u001b[0m                \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32;49m\"marvin::plugin\"\u001b[0m\u001b[39;49m followed by a JSON\u001b[0m   \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mdocument of the form \u001b[0m\u001b[1;39;49m{\u001b[0m\u001b[32;49m\"name\"\u001b[0m\u001b[39;49m: \u001b[0m\u001b[1;39;49m<\u001b[0m\u001b[1;95;49mthe\u001b[0m\u001b[39;49m \u001b[0m   \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mplugin name>, \u001b[0m\u001b[32;49m\"inputs\"\u001b[0m\u001b[39;49m:\u001b[0m               \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;39;49m{\u001b[0m\u001b[39;49m<input name>: <input value>, \u001b[0m\u001b[33;49m...\u001b[0m\u001b[1;39;49m}\u001b[0m\u001b[1;39;49m}\u001b[0m\u001b[39;49m. \u001b[0m \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mThe plugin's output will be\u001b[0m           \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49msupplied to you in a new message so \u001b[0m  \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49myou can use it in your\u001b[0m                \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mresponse to the user. Note the user \u001b[0m  \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mwill not see the plugin\u001b[0m               \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49moutput.\u001b[0m                               \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                                               \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mFor example, to use a plugin named \u001b[0m   \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49m`abc` with signature `\u001b[0m\u001b[1;39;49m(\u001b[0m\u001b[39;49mx:\u001b[0m             \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mstr, n_results: int = \u001b[0m\u001b[1;36;49m10\u001b[0m\u001b[1;39;49m)\u001b[0m\u001b[39;49m -> str` \u001b[0m    \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mwith `\u001b[0m\u001b[33;49mx\u001b[0m\u001b[39;49m=\u001b[0m\u001b[32;49m\"hello\"\u001b[0m\u001b[39;49m`, you must\u001b[0m            \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mrespond with `marvin::plugin \u001b[0m\u001b[1;39;49m{\u001b[0m\u001b[32;49m\"name\"\u001b[0m\u001b[39;49m:\u001b[0m \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32;49m\"abc\"\u001b[0m\u001b[39;49m, \u001b[0m\u001b[32;49m\"inputs\"\u001b[0m\u001b[39;49m: \u001b[0m\u001b[1;39;49m{\u001b[0m\u001b[32;49m\"x\"\u001b[0m\u001b[39;49m:\u001b[0m                \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32;49m\"hello\"\u001b[0m\u001b[1;39;49m}\u001b[0m\u001b[1;39;49m}\u001b[0m\u001b[39;49m`. \u001b[0m                          \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                                               \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mYou have access to the following \u001b[0m     \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mplugins:\u001b[0m                              \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m                                               \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;39;49m{\u001b[0m\u001b[32;49m'name'\u001b[0m\u001b[39;49m: \u001b[0m\u001b[32;49m'ChromaVectorstore'\u001b[0m\u001b[39;49m, \u001b[0m   \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32;49m'signature'\u001b[0m\u001b[39;49m: \u001b[0m\u001b[32;49m'\u001b[0m\u001b[32;49m(\u001b[0m\u001b[32;49mquery: str\u001b[0m\u001b[32;49m)\u001b[0m\u001b[32;49m -\u001b[0m\u001b[32;49m>\u001b[0m\u001b[32;49m str'\u001b[0m\u001b[39;49m, \u001b[0m  \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32;49m'description'\u001b[0m\u001b[39;49m: \u001b[0m\u001b[32;49m'Search Chroma for \u001b[0m    \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32;49mdocuments similar to a query - Use \u001b[0m   \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32;49mthis plugin when asked about \u001b[0m         \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32;49mprefect.'\u001b[0m\u001b[1;39;49m}\u001b[0m                            \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m marvin.Bot: \u001b[1;34mUser message: \u001b[0m\u001b[39;49mwhat CLI \u001b[0m   \u001b]8;id=730136;file:///Users/nate/src/open-source/marvin/src/marvin/utilities/logging.py\u001b\\\u001b[2mlogging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=63413;file:///Users/nate/src/open-source/marvin/src/marvin/utilities/logging.py#60\u001b\\\u001b[2m60\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mcommand do I need to make a Prefect \u001b[0m  \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[39;49mdeployment? use chroma to find out\u001b[0m    \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m[03/16/23 00:31:16]\u001b[0m\u001b[2;36m \u001b[0m\u001b[32mDEBUG   \u001b[0m marvin.Bot: \u001b[1;32mAI message: \u001b[0m\u001b[1;39;49m{\u001b[0m\u001b[32;49m\"query\"\u001b[0m\u001b[39;49m: \u001b[0m    \u001b]8;id=23278;file:///Users/nate/src/open-source/marvin/src/marvin/utilities/logging.py\u001b\\\u001b[2mlogging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=218345;file:///Users/nate/src/open-source/marvin/src/marvin/utilities/logging.py#60\u001b\\\u001b[2m60\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32;49m\"CLI command Prefect deployment\"\u001b[0m\u001b[1;39;49m}\u001b[0m     \u001b[2m             \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"query\": \"CLI command Prefect deployment\"}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import marvin\n",
    "from marvin.bots import Bot\n",
    "from marvin.plugins.chroma import ChromaVectorstore\n",
    "\n",
    "marvin.settings.log_level = 'DEBUG'\n",
    "\n",
    "bot = Bot(name='Marvin', personality='Marvin from Hitchhikers Guide to the Galaxy', plugins=[ChromaVectorstore()])\n",
    "\n",
    "await bot.say('what CLI command do I need to make a Prefect deployment?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['https://github.com/prefecthq/prefect/tree/main/docs/tutorials/storage.md', 'https://github.com/prefecthq/prefect/tree/main/docs/tutorials/storage.md', 'https://github.com/prefecthq/prefect/tree/main/docs/tutorials/docker.md']], 'embeddings': None, 'documents': [['---\\ndescription: Learn how to configure storage and infrastructure blocks used by Prefect flow deployments.\\ntags:\\n    - orchestration\\n    - deployments\\n    - storage\\n    - filesystems\\n    - infrastructure\\n    - blocks\\n---\\n\\n# Storage and Infrastructure\\n\\nIn previous tutorials, we\\'ve run flow and tasks entirely in a local execution environment using the local file system to store flow scripts. \\n\\nFor production workflows, you\\'ll most likely want to configure deployments that create flow runs in remote execution environments &mdash; a VM, a Docker container, or a Kubernetes cluster, for example. These deployments require _remote storage_ and _infrastructure_ blocks that specify where your flow code is stored and how the flow run execution environment should be configured.\\n\\nLet\\'s unpack these terms.\\n\\nIn Prefect, [blocks](/concepts/blocks/) are a primitive that enable you to specify configuration for interacting with external systems.\\n\\n[Storage](/concepts/storage/) blocks contain configuration for interacting with file storage such as a remote filesystem, AWS S3, and so on.\\n\\n[Infrastructure](/concepts/infrastructure/) blocks contain settings that [agents](/concepts/work-pools/) use to stand up execution infrastructure for a flow run.\\n\\n## Prerequisites\\n\\nThe steps demonstrated in this tutorial assume you have access to a storage location in a third-party service such as AWS, Azure, or GitHub.\\n\\nTo create a storage block, you will need the storage location (for example, a bucket or container name) and valid authentication details such as access keys or connection strings.\\n\\nTo use a remote storage block when creating deployments or using storage blocks within your flow script, you must install the required library for the storage service. \\n\\n| Service | Library |\\n| --- | --- |\\n| AWS S3 | [`s3fs`](https://s3fs.readthedocs.io/en/latest/) |\\n| Azure | [`adlfs`](https://github.com/fsspec/adlfs) |\\n| GCS | [`gcsfs`](https://gcsfs.readthedocs.io/en/latest/) |\\n| GitHub | `git` CLI |\\n\\nFor example:\\n\\n<div class=\"terminal\">\\n```bash\\n$ pip install s3fs\\n```\\n</div>\\n\\n## Storage\\n\\nAs mentioned previously, storage blocks contain configuration for interacting with file storage. This includes:\\n\\n- Remote storage on a filesystem supported by [`fsspec`](https://filesystem-spec.readthedocs.io/en/latest/)\\n- AWS S3\\n- Azure Blob Storage\\n- Google Cloud Storage\\n- SMB shares\\n- GitHub repositories\\n- Docker image\\n\\nStorage blocks enable Prefect to save and reference deployment artifacts (such as your flow scripts) to a location where they can be retrieved for future flow run execution. \\n\\nYour flow code may also load storage blocks to access configuration for accessing storage, such as for reading or saving files.\\n\\n## Create a storage block\\n\\nMost users will find it easiest to configure new storage blocks through the Prefect server or Prefect Cloud UI.\\n\\nYou can see any previously configured storage blocks by opening the Prefect UI and navigating to the **Blocks** page.\\n\\n![Viewing a list of previously configured storage blocks in the Prefect UI](../img/tutorials/storage-blocks.png)\\n\\nTo create a new block, select the **+** button on this page, or if you haven\\'t previously created any blocks, **New Block**. Prefect displays a page of available block types.\\n\\n![Viewing a list of block types in the Prefect UI](../img/tutorials/choose-blocks.png)\\n\\nFor this tutorial example, we\\'ll use the AWS S3 block as an example. If you use a different cloud storage service or solution, feel free to use the appropriate block type. The process is similar for all blocks, though the configuration options are slightly different, reflecting the authentication requirements of different cloud services.\\n\\nScroll down the list of blocks and find the **S3** block, then select **Add +** to configure a new storage block based on this block type. Prefect displays a **Create** page that enables specifying storage settings.\\n\\n![Configuring an S3 storage block in the Prefect UI](../img/tutorials/s3-block-configuration.png)\\n\\nEnter the configuration for your storage.\\n\\n- **Block Name** is the name by which your block is referenced. The name must only contain lowercase letters, numbers, and dashes.\\n- **Bucket Path** is the name of the bucket or container and, optionally, path to a folder within the bucket. If the folder does not exist it will be created. For example: `my-bucket/my/path`.\\n- **AWS Access Key ID** and **AWS Secret Access Key** take the respective authentication keys if they are needed to access the storage location.\\n\\nIn this example we\\'ve specified a storage location that could be used by the flow example from the [deployments tutorial](/tutorials/deployments/).\\n\\n- The name `log-test` makes it clear what flow code is stored in this location.\\n- `bucket-full-of-sunshine/flows/log-test` specifies the bucket name `bucket-full-of-sunshine` and the path to use within that bucket: `/flows/log-test`.\\n- This bucket requires an authenticated role, so we include the Access Key ID and Secret Access Key values.\\n\\n!!! tip \"Secrets are obfuscated\"\\n    Note that, once you save a block definition that contains sensitive data such as access keys, connection strings, or passwords, this data is obfuscated when viewed in the UI. You may update sensitive data, replacing it in the Prefect database, but you cannot view or copy this data from the UI.\\n\\n    This data is also obfuscated when persisted to deployment YAML files.\\n\\nSelect **Create** to create the new storage block. Prefect displays the details of the new block, including a code example for using the block within your flow code.\\n\\n![Viewing details of a new S3 storage block in the Prefect UI](../img/tutorials/new-storage-block.png)\\n\\n!!! tip \"Blocks and deployments are specific to a server or Prefect Cloud workspace\"\\n    Note that, if you ran through this tutorial on a local Prefect server instance, the storage and infrastructure blocks you created would not also be configured on Prefect Cloud. You must configure new storage and infrastructure blocks for any Prefect Cloud workspace.\\n\\n## Using storage blocks with deployments\\n\\nTo demonstrate using a storage block, we\\'ll create a new variation of the deployment for the `log_flow` example from the [deployments tutorial](/tutorials/deployments/). For this deployment, we\\'ll specify using the storage block created earlier by passing `-sb s3/log-test` or `--storage-block s3/log-test` to the `prefect deployment build` command.\\n\\n<div class=\"terminal\">\\n```bash\\n$ prefect deployment build ./log_flow.py:log_flow -n log-flow-s3 -sb s3/log-test -q test -o log-flow-s3-deployment.yaml\\nFound flow \\'log-flow\\'\\nSuccessfully uploaded 3 files to s3://bucket-full-of-sunshine/flows/log-test\\nDeployment YAML created at\\n\\'/Users/terry/test/dplytest/prefect-tutorial/log-flow-s3-deployment.yaml\\'.\\n```\\n</div>\\n\\nNote that with the `-sb s3/log-test` option the build process uploads the flow script files to `s3://bucket-full-of-sunshine/flows/log-test`.\\n\\nWhat did we do here? Let\\'s break down the command:\\n\\n- `prefect deployment build` is the Prefect CLI command that enables you to prepare the settings for a deployment.\\n-  `./log_flow.py:log_flow` specifies the location of the flow script file and the name of the entrypoint flow function, separated by a colon.\\n- `-n log-flow-s3` specifies a name for the deployment. For ease of identification, the name includes a reference to the S3 storage.\\n- `-sb s3/log-test` specifies a storage block by type and name.\\n- `-q test` specifies a work queue for the deployment. Work pools direct scheduled runs to agents. Since we didn\\'t specify a work pool with `-p`, the default work pool will be used. \\n- `-o log-flow-s3-deployment.yaml` specifies the name for the deployment YAML file. We do this to create a new deployment file rather than overwriting the previous one.\\n\\nIn deployments, storage blocks are always referenced by name in the format `type/name`, with `type` and `name` separated by a forward slash. \\n\\n- `type` is the type of storage block, such as `s3`, `azure`, or `gcs`.\\n- `name` is the name you specified when creating the block.\\n\\nIf you used a different storage block type or block name, your command may be different.\\n\\nNow you can apply the deployment YAML file to create the deployment on the API.\\n\\n<div class=\"terminal\">\\n```bash\\n$ prefect deployment apply log-flow-s3-deployment.yaml\\nSuccessfully loaded \\'log-flow-s3\\'\\nDeployment \\'log-flow/log-flow-s3\\' successfully created with id\\n\\'73b0288e-d5bb-4b37-847c-fa68fda39c81\\'.\\n\\nTo execute flow runs from this deployment, start an agent that pulls work from the \\'test\\'\\nwork pool:\\n$ prefect agent start -q \\'test\\'\\n```\\n</div>\\n\\nWhen you create flow runs from this deployment, the agent pulls the flow script from remote storage rather than local storage. This enables more complex flow run scenarios such as running flows on remote machines, in Docker containers, and more. We\\'ll take a closer look at these scenarios in a future tutorial.\\n\\n## Infrastructure\\n\\nSimilar to storage blocks, infrastructure [blocks](/concepts/blocks/) contain configuration for interacting with external systems. Specifically, infrastructure includes settings that [agents](/concepts/work-pools/) use to create an execution environment for a flow run.\\n\\nInfrastructure includes configuration for environments such as:\\n\\n- Docker containers\\n- Kubernetes Jobs\\n- Process configuration\\n\\nMost users will find it easiest to configure new infrastructure blocks through the Prefect server or Prefect Cloud UI. \\n\\nYou can see any previously configured storage blocks by opening the Prefect UI and navigating to the **Blocks** page. To create a new infrastructure block, select the **+** button on this page. Prefect displays a page of available block types. Select **run-infrastructure** from the **Capability** list to filter to just the infrastructure blocks.\\n\\n![Viewing a list of infrastructure block types in the Prefect UI](../img/tutorials/infrastructure-blocks.png)\\n\\nUse these base blocks to create your own infrastructure blocks containing the settings needed to run flows in your environment.\\n\\nFor example, find the **Docker Container** block, then select **Add +** to see the options for a Docker infrastructure block.\\n\\n![Viewing a list of infrastructure block types in the Prefect UI](../img/tutorials/docker-infrastructure.png)\\n\\nWe\\'re not going to create a custom infrastructure block until a later tutorial, so select **Cancel** to close the form.\\n\\n## Using infrastructure blocks with deployments\\n\\nTo use an infrastructure block when building a deployment, the process is similar to using a storage block. You can specify a custom infastructure block to the `prefect deployment build` command with the `-ib` or `--infra-block` options, passing the type and name of the block in the format `type/name`, with `type` and `name` separated by a forward slash. \\n\\n- `type` is the type of storage block, such as `docker-container`, `kubernetes-job`, or `process`.\\n- `name` is the name you specified when creating the block.\\n\\nThe `prefect deployment build` command also supports specifying a built-in infrastructure type prepopulated with defaults by using the `--infra` or `-i` options and passing the name of the infrastructure type: `docker-container`, `kubernetes-job`, or `process`.\\n\\n## Sharing blocks between different deployments\\n\\nOne of the major benefits of Prefect blocks is the ability to share common configuration across a diverse set of uses. This includes sharing storage and infrastructure configuration between your deployments.\\n\\n### Sharing storage\\n\\nEvery storage block exposes a base path (for example, an S3 bucket name or a GitHub repository). If you are sharing this block across multiple deployments, you most likely want each deployment to be stored in a subpath of the storage block\\'s basepath to ensure independence.  To enable this, deployments expose a `path` field that allows you to specify subpaths of storage for that deployment.  Let\\'s illustrate by extending our example above to store our deployment artifacts in a subdirectory of our S3 bucket called `log-flow-directory`:\\n\\n<div class=\"terminal\">\\n```bash\\n$ prefect deployment build ./log_flow.py:log_flow \\\\\\n    -n log-flow-s3 \\\\\\n    -sb s3/log-test \\\\\\n    -q test \\\\\\n    -o log-flow-s3-deployment.yaml \\\\\\n    --path log-flow-directory\\nFound flow \\'log-flow\\'\\nSuccessfully uploaded 3 files to s3://bucket-full-of-sunshine/flows/log-test/log-flow-directory\\nDeployment YAML created at\\n\\'/Users/terry/test/dplytest/prefect-tutorial/log-flow-s3-deployment.yaml\\'.\\n```\\n</div>\\n\\nNote that we used the `--path` option on the `build` CLI to provide this information. Other ways of specifying a deployment\\'s `path` include:\\n\\n- Providing a value for `path` to `Deployment.build_from_flow` or at `Deployment` initialization (see [the API reference for more details](/api-ref/prefect/deployments/#prefect.deployments.Deployment)).\\n- Prefect offers [syntatic sugar](https://en.wikipedia.org/wiki/Syntactic_sugar) on storage block specification where the path can be provided after the block slug: `-sb s3/log-test/log-flow-directory`.\\n\\n### Sharing infrastructure\\n\\nSharing infrastructure blocks between deployments is just as straightforward: every deployment exposes an `infra_overrides` field that can be used to target specific overrides on the base infrastructure block. These values can be set via CLI (`--override`) or Python (by providing a properly structured `infra_overrides` dictionary).\\n\\nWhen specified via CLI, overrides must be dot-delimited keys that target a specific (and possibly nested) attribute of the underlying infrastructure block.  When specified via Python, overrides must be provided as a possibly nested dictionary.  This is best illustrated with examples:\\n\\n- **Providing a custom environment variable**: every infrastructure block exposes an `env` field that contains a dictionary of environment variables and corresponding values.  To provide a specific environment variable on a deployment, pass `env.ENVIRONMENT_VARIABLE=VALUE` to either the CLI or the `infra_overrides` dictionary as a key/value pair.\\n- **Docker image override**: every Docker-based infrastructure block exposes an `image` field referencing the Docker image to use as a runtime environment. Overriding this is as simple as `--override image=my-custom-registry/my-custom-image:my-custom-tag`.\\n\\n## Specifying blocks in Python\\n\\nAs before, we can configure all of this via Python instead of the CLI by modifying our `deployment.py` file [created in the previous tutorial](/tutorials/deployments/#deployment-creation-with-python):\\n\\n```python hl_lines=\"5 7 15\"\\n# deployment.py\\n\\nfrom log_flow import log_flow\\nfrom prefect.deployments import Deployment\\nfrom prefect.blocks.core import Block\\n\\nstorage = Block.load(\"s3/log-test\")\\n\\ndeployment = Deployment.build_from_flow(\\n    flow=log_flow,\\n    name=\"log-simple\",\\n    parameters={\"name\": \"Marvin\"},\\n    infra_overrides={\"env\": {\"PREFECT_LOGGING_LEVEL\": \"DEBUG\"}},\\n    work_queue_name=\"test\",\\n    storage=storage,\\n)\\n\\nif __name__ == \"__main__\":\\n    deployment.apply()\\n```\\n\\nThis recipe for loading blocks is useful across a wide variety of situations, not just deployments.  We can load arbitrary block types from the core `Block` class by referencing their slug.\\n\\n!!! tip \"Next steps: Flow runs with Docker\"\\n    Continue on to the [Docker](/tutorials/docker/) tutorial where we\\'ll put storage, infrastructure, and deployments together to run a flow in a Docker container.\\n', '---\\ndescription: Learn how to configure storage and infrastructure blocks used by Prefect flow deployments.\\ntags:\\n    - orchestration\\n    - deployments\\n    - storage\\n    - filesystems\\n    - infrastructure\\n    - blocks\\n---\\n\\n# Storage and Infrastructure\\n\\nIn previous tutorials, we\\'ve run flow and tasks entirely in a local execution environment using the local file system to store flow scripts. \\n\\nFor production workflows, you\\'ll most likely want to configure deployments that create flow runs in remote execution environments &mdash; a VM, a Docker container, or a Kubernetes cluster, for example. These deployments require _remote storage_ and _infrastructure_ blocks that specify where your flow code is stored and how the flow run execution environment should be configured.\\n\\nLet\\'s unpack these terms.\\n\\nIn Prefect, [blocks](/concepts/blocks/) are a primitive that enable you to specify configuration for interacting with external systems.\\n\\n[Storage](/concepts/storage/) blocks contain configuration for interacting with file storage such as a remote filesystem, AWS S3, and so on.\\n\\n[Infrastructure](/concepts/infrastructure/) blocks contain settings that [agents](/concepts/work-pools/) use to stand up execution infrastructure for a flow run.\\n\\n## Prerequisites\\n\\nThe steps demonstrated in this tutorial assume you have access to a storage location in a third-party service such as AWS, Azure, or GitHub.\\n\\nTo create a storage block, you will need the storage location (for example, a bucket or container name) and valid authentication details such as access keys or connection strings.\\n\\nTo use a remote storage block when creating deployments or using storage blocks within your flow script, you must install the required library for the storage service. \\n\\n| Service | Library |\\n| --- | --- |\\n| AWS S3 | [`s3fs`](https://s3fs.readthedocs.io/en/latest/) |\\n| Azure | [`adlfs`](https://github.com/fsspec/adlfs) |\\n| GCS | [`gcsfs`](https://gcsfs.readthedocs.io/en/latest/) |\\n| GitHub | `git` CLI |\\n\\nFor example:\\n\\n<div class=\"terminal\">\\n```bash\\n$ pip install s3fs\\n```\\n</div>\\n\\n## Storage\\n\\nAs mentioned previously, storage blocks contain configuration for interacting with file storage. This includes:\\n\\n- Remote storage on a filesystem supported by [`fsspec`](https://filesystem-spec.readthedocs.io/en/latest/)\\n- AWS S3\\n- Azure Blob Storage\\n- Google Cloud Storage\\n- SMB shares\\n- GitHub repositories\\n- Docker image\\n\\nStorage blocks enable Prefect to save and reference deployment artifacts (such as your flow scripts) to a location where they can be retrieved for future flow run execution. \\n\\nYour flow code may also load storage blocks to access configuration for accessing storage, such as for reading or saving files.\\n\\n## Create a storage block\\n\\nMost users will find it easiest to configure new storage blocks through the Prefect server or Prefect Cloud UI.\\n\\nYou can see any previously configured storage blocks by opening the Prefect UI and navigating to the **Blocks** page.\\n\\n![Viewing a list of previously configured storage blocks in the Prefect UI](../img/tutorials/storage-blocks.png)\\n\\nTo create a new block, select the **+** button on this page, or if you haven\\'t previously created any blocks, **New Block**. Prefect displays a page of available block types.\\n\\n![Viewing a list of block types in the Prefect UI](../img/tutorials/choose-blocks.png)\\n\\nFor this tutorial example, we\\'ll use the AWS S3 block as an example. If you use a different cloud storage service or solution, feel free to use the appropriate block type. The process is similar for all blocks, though the configuration options are slightly different, reflecting the authentication requirements of different cloud services.\\n\\nScroll down the list of blocks and find the **S3** block, then select **Add +** to configure a new storage block based on this block type. Prefect displays a **Create** page that enables specifying storage settings.\\n\\n![Configuring an S3 storage block in the Prefect UI](../img/tutorials/s3-block-configuration.png)\\n\\nEnter the configuration for your storage.\\n\\n- **Block Name** is the name by which your block is referenced. The name must only contain lowercase letters, numbers, and dashes.\\n- **Bucket Path** is the name of the bucket or container and, optionally, path to a folder within the bucket. If the folder does not exist it will be created. For example: `my-bucket/my/path`.\\n- **AWS Access Key ID** and **AWS Secret Access Key** take the respective authentication keys if they are needed to access the storage location.\\n\\nIn this example we\\'ve specified a storage location that could be used by the flow example from the [deployments tutorial](/tutorials/deployments/).\\n\\n- The name `log-test` makes it clear what flow code is stored in this location.\\n- `bucket-full-of-sunshine/flows/log-test` specifies the bucket name `bucket-full-of-sunshine` and the path to use within that bucket: `/flows/log-test`.\\n- This bucket requires an authenticated role, so we include the Access Key ID and Secret Access Key values.\\n\\n!!! tip \"Secrets are obfuscated\"\\n    Note that, once you save a block definition that contains sensitive data such as access keys, connection strings, or passwords, this data is obfuscated when viewed in the UI. You may update sensitive data, replacing it in the Prefect database, but you cannot view or copy this data from the UI.\\n\\n    This data is also obfuscated when persisted to deployment YAML files.\\n\\nSelect **Create** to create the new storage block. Prefect displays the details of the new block, including a code example for using the block within your flow code.\\n\\n![Viewing details of a new S3 storage block in the Prefect UI](../img/tutorials/new-storage-block.png)\\n\\n!!! tip \"Blocks and deployments are specific to a server or Prefect Cloud workspace\"\\n    Note that, if you ran through this tutorial on a local Prefect server instance, the storage and infrastructure blocks you created would not also be configured on Prefect Cloud. You must configure new storage and infrastructure blocks for any Prefect Cloud workspace.\\n\\n## Using storage blocks with deployments\\n\\nTo demonstrate using a storage block, we\\'ll create a new variation of the deployment for the `log_flow` example from the [deployments tutorial](/tutorials/deployments/). For this deployment, we\\'ll specify using the storage block created earlier by passing `-sb s3/log-test` or `--storage-block s3/log-test` to the `prefect deployment build` command.\\n\\n<div class=\"terminal\">\\n```bash\\n$ prefect deployment build ./log_flow.py:log_flow -n log-flow-s3 -sb s3/log-test -q test -o log-flow-s3-deployment.yaml\\nFound flow \\'log-flow\\'\\nSuccessfully uploaded 3 files to s3://bucket-full-of-sunshine/flows/log-test\\nDeployment YAML created at\\n\\'/Users/terry/test/dplytest/prefect-tutorial/log-flow-s3-deployment.yaml\\'.\\n```\\n</div>\\n\\nNote that with the `-sb s3/log-test` option the build process uploads the flow script files to `s3://bucket-full-of-sunshine/flows/log-test`.\\n\\nWhat did we do here? Let\\'s break down the command:\\n\\n- `prefect deployment build` is the Prefect CLI command that enables you to prepare the settings for a deployment.\\n-  `./log_flow.py:log_flow` specifies the location of the flow script file and the name of the entrypoint flow function, separated by a colon.\\n- `-n log-flow-s3` specifies a name for the deployment. For ease of identification, the name includes a reference to the S3 storage.\\n- `-sb s3/log-test` specifies a storage block by type and name.\\n- `-q test` specifies a work queue for the deployment. Work pools direct scheduled runs to agents. Since we didn\\'t specify a work pool with `-p`, the default work pool will be used. \\n- `-o log-flow-s3-deployment.yaml` specifies the name for the deployment YAML file. We do this to create a new deployment file rather than overwriting the previous one.\\n\\nIn deployments, storage blocks are always referenced by name in the format `type/name`, with `type` and `name` separated by a forward slash. \\n\\n- `type` is the type of storage block, such as `s3`, `azure`, or `gcs`.\\n- `name` is the name you specified when creating the block.\\n\\nIf you used a different storage block type or block name, your command may be different.\\n\\nNow you can apply the deployment YAML file to create the deployment on the API.\\n\\n<div class=\"terminal\">\\n```bash\\n$ prefect deployment apply log-flow-s3-deployment.yaml\\nSuccessfully loaded \\'log-flow-s3\\'\\nDeployment \\'log-flow/log-flow-s3\\' successfully created with id\\n\\'73b0288e-d5bb-4b37-847c-fa68fda39c81\\'.\\n\\nTo execute flow runs from this deployment, start an agent that pulls work from the \\'test\\'\\nwork pool:\\n$ prefect agent start -q \\'test\\'\\n```\\n</div>\\n\\nWhen you create flow runs from this deployment, the agent pulls the flow script from remote storage rather than local storage. This enables more complex flow run scenarios such as running flows on remote machines, in Docker containers, and more. We\\'ll take a closer look at these scenarios in a future tutorial.\\n\\n## Infrastructure\\n\\nSimilar to storage blocks, infrastructure [blocks](/concepts/blocks/) contain configuration for interacting with external systems. Specifically, infrastructure includes settings that [agents](/concepts/work-pools/) use to create an execution environment for a flow run.\\n\\nInfrastructure includes configuration for environments such as:\\n\\n- Docker containers\\n- Kubernetes Jobs\\n- Process configuration\\n\\nMost users will find it easiest to configure new infrastructure blocks through the Prefect server or Prefect Cloud UI. \\n\\nYou can see any previously configured storage blocks by opening the Prefect UI and navigating to the **Blocks** page. To create a new infrastructure block, select the **+** button on this page. Prefect displays a page of available block types. Select **run-infrastructure** from the **Capability** list to filter to just the infrastructure blocks.\\n\\n![Viewing a list of infrastructure block types in the Prefect UI](../img/tutorials/infrastructure-blocks.png)\\n\\nUse these base blocks to create your own infrastructure blocks containing the settings needed to run flows in your environment.\\n\\nFor example, find the **Docker Container** block, then select **Add +** to see the options for a Docker infrastructure block.\\n\\n![Viewing a list of infrastructure block types in the Prefect UI](../img/tutorials/docker-infrastructure.png)\\n\\nWe\\'re not going to create a custom infrastructure block until a later tutorial, so select **Cancel** to close the form.\\n\\n## Using infrastructure blocks with deployments\\n\\nTo use an infrastructure block when building a deployment, the process is similar to using a storage block. You can specify a custom infastructure block to the `prefect deployment build` command with the `-ib` or `--infra-block` options, passing the type and name of the block in the format `type/name`, with `type` and `name` separated by a forward slash. \\n\\n- `type` is the type of storage block, such as `docker-container`, `kubernetes-job`, or `process`.\\n- `name` is the name you specified when creating the block.\\n\\nThe `prefect deployment build` command also supports specifying a built-in infrastructure type prepopulated with defaults by using the `--infra` or `-i` options and passing the name of the infrastructure type: `docker-container`, `kubernetes-job`, or `process`.\\n\\n## Sharing blocks between different deployments\\n\\nOne of the major benefits of Prefect blocks is the ability to share common configuration across a diverse set of uses. This includes sharing storage and infrastructure configuration between your deployments.\\n\\n### Sharing storage\\n\\nEvery storage block exposes a base path (for example, an S3 bucket name or a GitHub repository). If you are sharing this block across multiple deployments, you most likely want each deployment to be stored in a subpath of the storage block\\'s basepath to ensure independence.  To enable this, deployments expose a `path` field that allows you to specify subpaths of storage for that deployment.  Let\\'s illustrate by extending our example above to store our deployment artifacts in a subdirectory of our S3 bucket called `log-flow-directory`:\\n\\n<div class=\"terminal\">\\n```bash\\n$ prefect deployment build ./log_flow.py:log_flow \\\\\\n    -n log-flow-s3 \\\\\\n    -sb s3/log-test \\\\\\n    -q test \\\\\\n    -o log-flow-s3-deployment.yaml \\\\\\n    --path log-flow-directory\\nFound flow \\'log-flow\\'\\nSuccessfully uploaded 3 files to s3://bucket-full-of-sunshine/flows/log-test/log-flow-directory\\nDeployment YAML created at\\n\\'/Users/terry/test/dplytest/prefect-tutorial/log-flow-s3-deployment.yaml\\'.\\n```\\n</div>\\n\\nNote that we used the `--path` option on the `build` CLI to provide this information. Other ways of specifying a deployment\\'s `path` include:\\n\\n- Providing a value for `path` to `Deployment.build_from_flow` or at `Deployment` initialization (see [the API reference for more details](/api-ref/prefect/deployments/#prefect.deployments.Deployment)).\\n- Prefect offers [syntatic sugar](https://en.wikipedia.org/wiki/Syntactic_sugar) on storage block specification where the path can be provided after the block slug: `-sb s3/log-test/log-flow-directory`.\\n\\n### Sharing infrastructure\\n\\nSharing infrastructure blocks between deployments is just as straightforward: every deployment exposes an `infra_overrides` field that can be used to target specific overrides on the base infrastructure block. These values can be set via CLI (`--override`) or Python (by providing a properly structured `infra_overrides` dictionary).\\n\\nWhen specified via CLI, overrides must be dot-delimited keys that target a specific (and possibly nested) attribute of the underlying infrastructure block.  When specified via Python, overrides must be provided as a possibly nested dictionary.  This is best illustrated with examples:\\n\\n- **Providing a custom environment variable**: every infrastructure block exposes an `env` field that contains a dictionary of environment variables and corresponding values.  To provide a specific environment variable on a deployment, pass `env.ENVIRONMENT_VARIABLE=VALUE` to either the CLI or the `infra_overrides` dictionary as a key/value pair.\\n- **Docker image override**: every Docker-based infrastructure block exposes an `image` field referencing the Docker image to use as a runtime environment. Overriding this is as simple as `--override image=my-custom-registry/my-custom-image:my-custom-tag`.\\n\\n## Specifying blocks in Python\\n\\nAs before, we can configure all of this via Python instead of the CLI by modifying our `deployment.py` file [created in the previous tutorial](/tutorials/deployments/#deployment-creation-with-python):\\n\\n```python hl_lines=\"5 7 15\"\\n# deployment.py\\n\\nfrom log_flow import log_flow\\nfrom prefect.deployments import Deployment\\nfrom prefect.blocks.core import Block\\n\\nstorage = Block.load(\"s3/log-test\")\\n\\ndeployment = Deployment.build_from_flow(\\n    flow=log_flow,\\n    name=\"log-simple\",\\n    parameters={\"name\": \"Marvin\"},\\n    infra_overrides={\"env\": {\"PREFECT_LOGGING_LEVEL\": \"DEBUG\"}},\\n    work_queue_name=\"test\",\\n    storage=storage,\\n)\\n\\nif __name__ == \"__main__\":\\n    deployment.apply()\\n```\\n\\nThis recipe for loading blocks is useful across a wide variety of situations, not just deployments.  We can load arbitrary block types from the core `Block` class by referencing their slug.\\n\\n!!! tip \"Next steps: Flow runs with Docker\"\\n    Continue on to the [Docker](/tutorials/docker/) tutorial where we\\'ll put storage, infrastructure, and deployments together to run a flow in a Docker container.\\n', '---\\ndescription: Learn how to build Prefect deployments that create flow runs in Docker containers.\\ntags:\\n    - Docker\\n    - containers\\n    - orchestration\\n    - infrastructure\\n    - deployments\\n    - tutorial\\n---\\n\\n# Running flows with Docker\\n\\nIn the [Deployments](/tutorials/deployments/) and [Storage and Infrastructure](/tutorials/storage/) tutorials, we looked at creating configuration that enables creating flow runs via the API and with code that was uploaded to a remotely accessible location.  \\n\\nIn this tutorial, we\\'ll further configure the deployment so flow runs are executed in a Docker container. We\\'ll run our Docker instance locally, but you can extend this tutorial to run it on remote machines. \\n\\nIn this tutorial we\\'ll: \\n\\n- Configure a Docker Container infrastructure block that enables creating flow runs in a container.\\n- Build and apply a new `log_flow.py` deployment that uses the new infrastructure block.\\n- Create a flow run from this deployment that spins up a Docker container and executes, logging a message.\\n\\n## Prerequisites\\n\\nTo run a deployed flow in a Docker container, you\\'ll need the following:\\n\\n- We\\'ll use the flow script and deployment from the [Deployments](/tutorials/deployments/) tutorial. \\n- We\\'ll also use the remote storage block created in the [Storage and Infrastructure](/tutorials/storage/) tutorial.\\n- You must run a standalone Prefect server (`prefect server start`) or use Prefect Cloud.\\n- You\\'ll need [Docker Engine](https://docs.docker.com/engine/) installed and running on the same machine as your agent.\\n\\n[Docker Desktop](https://www.docker.com/products/docker-desktop) works fine for local testing if you don\\'t already have Docker Engine configured in your environment.\\n\\n!!! note \"Run a Prefect server\"\\n    This tutorial assumes you\\'re already running a Prefect server with `prefect server start`, as described in the [Deployments](/tutorials/deployments/) tutorial. \\n    \\n    If you shut down the server from a previous tutorial, you can start it again by opening another terminal session and starting the Prefect server with the `prefect server start` CLI command.\\n\\n## Create an infrastructure block\\n\\nMost users will find it easiest to configure new infrastructure blocks through the Prefect server or Prefect Cloud UI. \\n\\nYou can see any previously configured storage blocks by opening the Prefect UI and navigating to the **Blocks** page. To create a new infrastructure block, select the **+** button on this page. Prefect displays a page of available block types. Select **run-infrastructure** from the **Capability** list to filter just the infrastructure blocks.\\n\\n![Viewing a list of infrastructure block types in the Prefect UI](../img/tutorials/infrastructure-blocks.png)\\n\\nUse these base blocks to create your own infrastructure blocks containing the settings needed to run flows in your environment.\\n\\nFor this tutorial, find the **Docker Container** block, then select **Add +** to see the options for a Docker infrastructure block.\\n\\n![Viewing a list of infrastructure block types in the Prefect UI](../img/tutorials/docker-infrastructure.png)\\n\\nTo configure this Docker Container block to run the `log_flow.py` deployment, we just need to add two pieces of information.\\n\\nFirst, give the block a **Block Name**. We used \"log-tutorial\".\\n\\nSecond, we need to make sure the container includes any additional files, libraries, or configuration to run `log_flow.py`. By default, Prefect uses a preconfigured container that includes installations of Python and Prefect.\\n\\nIn the [Storage and Infrastructure](/tutorials/storage/) tutorial, recall that we needed to `pip install s3fs` the library for an S3 storage block. You\\'ll need to include the same command in the configuration of the Docker Container infrastructure block. When the agent spins up a container for a flow run, it will know to install the `s3fs` package before starting the flow run.\\n\\nAs a convenience, we can use the [`EXTRA_PIP_PACKAGES` environment variable](/concepts/infrastructure/#installing-extra-dependencies-at-runtime) to install dependencies at runtime. If defined, `pip install ${EXTRA_PIP_PACKAGES}` is executed before the flow run starts.\\n\\nIn the **Env (Optional)** box, enter the following to specify that the `s3fs` package should be installed. Note that we use JSON formatting to specify the environment variable (key) and packages to install (value).\\n\\n```json\\n{\\n  \"EXTRA_PIP_PACKAGES\": \"s3fs\"\\n}\\n```\\nIf you defined a different type of storage block, such as Azure or GCS, you\\'ll need to specify the relevant storage library. See the [Prerequisites section of the Storage tutorial](/tutorials/storage/#prerequisites) for details.\\n\\n![Configuring a new Docker Container infrastructure block in the Prefect UI](../img/tutorials/docker-tutorial-block.png)\\n\\n## Using infrastructure blocks with deployments\\n\\nTo use an infrastructure block when building a deployment, the process is similar to using a storage block. You can specify a custom infrastructure block to the `prefect deployment build` command with the `-ib` or `--infra-block` options, passing the type and name of the block in the in the format `type/name`, with `type` and `name` separated by a forward slash. \\n\\n- `type` is the type of storage block, such as `docker-container`, `kubernetes-job`, or `process`.\\n- `name` is the name you specified when creating the block.\\n\\nThe `prefect deployment build` command also supports specifying a built-in infrastructure type prepopulated with defaults by using the `--infra` or `-i` options and passing the name of the infrastructure type: `docker-container`, `kubernetes-job`, or `process`.\\n\\n## Build a deployment with Docker infrastructure\\n\\nTo demonstrate using an infrastructure block, we\\'ll create a new variation of the deployment for the `log_flow` example from the [deployments tutorial](/tutorials/deployments/). For this deployment, we\\'ll include the following options to the `prefect deployment build` command:\\n\\n- Use the storage block created in the [Storage and Infrastructure](/tutorials/storage/) tutorial by passing `-sb s3/log-test` or `--storage-block s3/log-test`.\\n- Use the infrastructure block created earlier by passing `-ib docker-container/log-tutorial` or `--infra-block docker-container/log-tutorial`.\\n\\n<div class=\"terminal\">\\n```bash\\n$ prefect deployment build ./log_flow.py:log_flow -n log-flow-docker -sb s3/log-test -ib docker-container/log-tutorial -q test -o log-flow-docker-deployment.yaml\\nFound flow \\'log-flow\\'\\nSuccessfully uploaded 4 files to s3://bucket-full-of-sunshine/flows/test\\nDeployment YAML created at\\n\\'/Users/terry/prefect-tutorial/log-flow-docker-deployment.yaml\\'.\\n```\\n</div>\\n\\nWhat did we do here? Let\\'s break down the command:\\n\\n- `prefect deployment build` is the Prefect CLI command that enables you to prepare the settings for a deployment.\\n-  `./log_flow.py:log_flow` specifies the location of the flow script file and the name of the entrypoint flow function, separated by a colon.\\n- `-n log-flow-docker` specifies a name for the deployment. For ease of identification, the name includes a reference to the Docker infrastructure.\\n- `-sb s3/log-test` specifies a storage block by type and name. If you used a different storage block type or block name, your command may be different.\\n- `-ib docker-container/log-tutorial` specifies an infrastructure block by type and name.\\n- `-q test` specifies a work queue for the deployment. Work pools direct scheduled runs to agents.\\n- `-o log-flow-docker-deployment.yaml` specifies the name for the deployment YAML file. We do this to create a new deployment file rather than overwriting the previous one.\\n\\n## Apply the deployment\\n\\nNow we can apply the deployment YAML to create the deployment on the API.\\n\\n<div class=\"terminal\">\\n```bash\\n$ prefect deployment apply log-flow-docker-deployment.yaml\\nSuccessfully loaded \\'log-flow-docker\\'\\nDeployment \\'log-flow/log-flow-docker\\' successfully created with id\\n\\'a52fe285-d646-4e57-affd-257acf92782a\\'.\\n\\nTo execute flow runs from this deployment, start an agent that pulls work from the \\'test\\'\\nwork queue:\\n$ prefect agent start -q \\'test\\'\\n```\\n</div>\\n\\nOpen the Prefect UI at [http://127.0.0.1:4200/](http://127.0.0.1:4200/) and select the **Deployments** page. You\\'ll see a list of all deployments that have been created in this Prefect server instance, including the new `log-flow/log-flow-docker` deployment.\\n\\n![Viewing the new Docker deployment in the Prefect UI](../img/tutorials/docker-deployment.png)\\n\\n## Edit the deployment in the UI\\n\\n`log_flow` expects a runtime parameter for its greeting, and we didn\\'t provide one as part of this deployment yet. We could edit `log-flow-docker-deployment.yaml` to add a parameter and apply the edited YAML to update the deployment on the API.\\n\\nInstead, let\\'s edit the deployment through the Prefect UI. Select **log-flow/log-flow-docker** to see the deployment\\'s details.\\n\\n![Viewing the Docker deployment details in the Prefect UI](../img/tutorials/docker-deployment-details.png)\\n\\nSelect the menu next to **Run**, then select **Edit** to edit the deployment.\\n\\nScroll down to the **Parameters** section and provide a value for the `name` parameter. We used \"Ford Prefect\" here. \\n\\n![Editing the Docker deployment details in the Prefect UI](../img/tutorials/edit-docker-deployment.png)\\n\\nSelect **Save** to save these changes to the deployment.\\n\\n## Create a flow run in Docker\\n\\nWhen you create flow runs from this deployment, the agent pulls the default Prefect Docker container, `pip installs` the prerequisites we specified, retrieves the flow script from remote storage, and starts the Prefect engine to execute the flow run.\\n\\nLet\\'s create a flow run for this deployment. The flow run will execute in a Docker container on your local machine.\\n\\n!!! note \"Run a Prefect agent\"\\n    This tutorial assumes you\\'re already running a Prefect agent with `prefect agent start`, as described in the [Deployments](/tutorials/deployments/#agents-and-work-pools) tutorial. \\n    \\n    If you shut down the agent from a previous tutorial, you can start it again by opening another terminal session and starting the agent with the `prefect agent start -q test` CLI command. This agent pulls work from the `test` work queue created previously.\\n\\n    Note also that the `PREFECT_API_URL` setting should be configured to point to the URL of your Prefect server or Prefect Cloud.\\n\\n    If you\\'re running the agent in the same environment or machine as your server, it should already be set. If not, run this command to set the API URL to point at the Prefect instance just started:\\n\\n    <div class=\\'terminal\\'>\\n    ```bash\\n    $ prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api\\n    Set variable \\'PREFECT_API_URL\\' to \\'http://127.0.0.1:4200/api\\'\\n    Updated profile \\'default\\'\\n    ```\\n    </div>\\n\\n    You can check the settings for your environment with the `prefect config view` CLI command.\\n\\n    <div class=\\'terminal\\'>\\n    ```bash\\n    # View current configuration\\n    $ prefect config view\\n    PREFECT_PROFILE=\\'default\\'\\n    PREFECT_API_URL=\\'http://127.0.0.1:4200/api\\' (from profile)\\n    ```\\n    </div>\\n\\nOn the deployment details page, select **Run**, then select **Now with defaults**. This creates a new flow run using the default parameters and other settings.\\n\\n![Running the Docker deployment from the Prefect UI](../img/tutorials/run-docker-deployment.png)\\n\\nGo to the terminal session running the Prefect agent. You should see logged output showing:\\n\\n- The agent submitting the flow run.\\n- The Docker container being created.\\n- Installation of the storage library.\\n- The task run creating log messages.\\n- The flow run completing.\\n- The Docker container closing down.\\n\\n<div class=\\'terminal\\'>\\n```bash\\n23:19:52.252 | INFO    | prefect.agent - Submitting flow run \\'2d520993-3697-4105-987f-70398e2a65fe\\'\\n23:19:52.449 | INFO    | prefect.infrastructure.docker-container - Creating Docker container \\'woodoo-peacock\\'...\\n23:19:53.034 | INFO    | prefect.agent - Completed submission of flow run \\'2d520993-3697-4105-987f-70398e2a65fe\\'\\n23:19:53.065 | INFO    | prefect.infrastructure.docker-container - Docker container \\'woodoo-peacock\\' has status \\'running\\'\\n+pip install s3fs\\nCollecting s3fs\\n  Downloading s3fs-2022.7.1-py3-none-any.whl (27 kB)\\n...\\n03:20:02.773 | INFO    | Flow run \\'woodoo-peacock\\' - Created task run \\'log_task-99465d2b-0\\' for task \\'log_task\\'\\n03:20:02.774 | INFO    | Flow run \\'woodoo-peacock\\' - Executing \\'log_task-99465d2b-0\\' immediately...\\n03:20:02.808 | INFO    | Task run \\'log_task-99465d2b-0\\' - Hello Ford Prefect!\\n03:20:02.808 | INFO    | Task run \\'log_task-99465d2b-0\\' - Prefect Version = 2.2.0 \\n03:20:02.837 | INFO    | Task run \\'log_task-99465d2b-0\\' - Finished in state Completed()\\n03:20:02.869 | INFO    | Flow run \\'woodoo-peacock\\' - Finished in state Completed(\\'All states completed.\\')\\n23:20:03.410 | INFO    | prefect.infrastructure.docker-container - Docker container \\'woodoo-peacock\\' has status \\'exited\\'\\n```\\n</div>\\n\\nIn the Prefect UI, go to the **Flow Runs** page and select the flow run. You should see the \"Hello Ford Prefect!\" log message created by the flow running in the Docker container!\\n\\n![Log messages from the deployment flow run.](../img/tutorials/docker-flow-log.png)\\n\\n## Cleaning up\\n\\nWhen you\\'re finished, just close the Prefect UI tab in your browser, and close the terminal sessions running the Prefect server and agent.\\n']], 'metadatas': [[{'source': 'https://github.com/prefecthq/prefect/tree/main/docs/tutorials/storage.md'}, {'source': 'https://github.com/prefecthq/prefect/tree/main/docs/tutorials/storage.md'}, {'source': 'https://github.com/prefecthq/prefect/tree/main/docs/tutorials/docker.md'}]], 'distances': None}\n"
     ]
    }
   ],
   "source": [
    "results = await chroma.query(\n",
    "    query_texts=[\"How do I run subflows on their own infrastructure?\"],\n",
    "    n_results=3,\n",
    "    include=[\"metadatas\", \"documents\"],\n",
    ")\n",
    "\n",
    "summary_text = \"\\n\".join(\n",
    "    f\"{idx}\\n{doc}\\n\" for idx, doc in zip(results[\"ids\"], results[\"documents\"])\n",
    ")\n",
    "\n",
    "print(summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/var/folders/mz/myyrd_nd7bng075xq77gr1j40000gn/T/tmp850zzmre'...\n"
     ]
    },
    {
     "ename": "ConnectTimeout",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpcore/backends/asyncio.py:111\u001b[0m, in \u001b[0;36mAsyncIOBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[39mwith\u001b[39;00m anyio\u001b[39m.\u001b[39mfail_after(timeout):\n\u001b[0;32m--> 111\u001b[0m         stream: anyio\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mByteStream \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m anyio\u001b[39m.\u001b[39mconnect_tcp(\n\u001b[1;32m    112\u001b[0m             remote_host\u001b[39m=\u001b[39mhost,\n\u001b[1;32m    113\u001b[0m             remote_port\u001b[39m=\u001b[39mport,\n\u001b[1;32m    114\u001b[0m             local_host\u001b[39m=\u001b[39mlocal_address,\n\u001b[1;32m    115\u001b[0m         )\n\u001b[1;32m    116\u001b[0m \u001b[39mreturn\u001b[39;00m AsyncIOStream(stream)\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/anyio/_core/_sockets.py:213\u001b[0m, in \u001b[0;36mconnect_tcp\u001b[0;34m(remote_host, remote_port, local_host, tls, ssl_context, tls_standard_compatible, tls_hostname, happy_eyeballs_delay)\u001b[0m\n\u001b[1;32m    212\u001b[0m oserrors: List[\u001b[39mOSError\u001b[39;00m] \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 213\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m create_task_group() \u001b[39mas\u001b[39;00m tg:\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m i, (af, addr) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(target_addrs):\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/anyio/_backends/_asyncio.py:662\u001b[0m, in \u001b[0;36mTaskGroup.__aexit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[39melif\u001b[39;00m exceptions \u001b[39mand\u001b[39;00m exceptions[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m exc_val:\n\u001b[0;32m--> 662\u001b[0m         \u001b[39mraise\u001b[39;00m exceptions[\u001b[39m0\u001b[39m]\n\u001b[1;32m    663\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    664\u001b[0m     \u001b[39m# Clear the context here, as it can only be done in-flight.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m     \u001b[39m# If the context is not cleared, it can result in recursive tracebacks (see #145).\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/asyncio/tasks.py:234\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39;49mthrow(exc)\n\u001b[1;32m    235\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/anyio/_core/_sockets.py:164\u001b[0m, in \u001b[0;36mconnect_tcp.<locals>.try_connect\u001b[0;34m(remote_host, event)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     stream \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m asynclib\u001b[39m.\u001b[39mconnect_tcp(remote_host, remote_port, local_address)\n\u001b[1;32m    165\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/anyio/_backends/_asyncio.py:1691\u001b[0m, in \u001b[0;36mconnect_tcp\u001b[0;34m(host, port, local_addr)\u001b[0m\n\u001b[1;32m   1686\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mconnect_tcp\u001b[39m(\n\u001b[1;32m   1687\u001b[0m     host: \u001b[39mstr\u001b[39m, port: \u001b[39mint\u001b[39m, local_addr: Optional[Tuple[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SocketStream:\n\u001b[1;32m   1689\u001b[0m     transport, protocol \u001b[39m=\u001b[39m cast(\n\u001b[1;32m   1690\u001b[0m         Tuple[asyncio\u001b[39m.\u001b[39mTransport, StreamProtocol],\n\u001b[0;32m-> 1691\u001b[0m         \u001b[39mawait\u001b[39;00m get_running_loop()\u001b[39m.\u001b[39mcreate_connection(\n\u001b[1;32m   1692\u001b[0m             StreamProtocol, host, port, local_addr\u001b[39m=\u001b[39mlocal_addr\n\u001b[1;32m   1693\u001b[0m         ),\n\u001b[1;32m   1694\u001b[0m     )\n\u001b[1;32m   1695\u001b[0m     transport\u001b[39m.\u001b[39mpause_reading()\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/asyncio/base_events.py:1054\u001b[0m, in \u001b[0;36mBaseEventLoop.create_connection\u001b[0;34m(self, protocol_factory, host, port, ssl, family, proto, flags, sock, local_addr, server_hostname, ssl_handshake_timeout, happy_eyeballs_delay, interleave)\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1054\u001b[0m     sock \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connect_sock(\n\u001b[1;32m   1055\u001b[0m         exceptions, addrinfo, laddr_infos)\n\u001b[1;32m   1056\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/asyncio/base_events.py:963\u001b[0m, in \u001b[0;36mBaseEventLoop._connect_sock\u001b[0;34m(self, exceptions, addr_info, local_addr_infos)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[39mraise\u001b[39;00m my_exceptions\u001b[39m.\u001b[39mpop()\n\u001b[0;32m--> 963\u001b[0m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock_connect(sock, address)\n\u001b[1;32m    964\u001b[0m \u001b[39mreturn\u001b[39;00m sock\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/asyncio/selector_events.py:501\u001b[0m, in \u001b[0;36mBaseSelectorEventLoop.sock_connect\u001b[0;34m(self, sock, address)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 501\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m fut\n\u001b[1;32m    502\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m     \u001b[39m# Needed to break cycles when an exception occurs.\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/asyncio/futures.py:285\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_asyncio_future_blocking \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39m  \u001b[39m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/asyncio/tasks.py:304\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     future\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    305\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    306\u001b[0m     \u001b[39m# This may also be a cancellation.\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/asyncio/futures.py:196\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m     exc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_cancelled_error()\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    197\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m!=\u001b[39m _FINISHED:\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpcore/_exceptions.py:10\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:  \u001b[39m# noqa: PIE786\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpcore/backends/asyncio.py:110\u001b[0m, in \u001b[0;36mAsyncIOBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m--> 110\u001b[0m     \u001b[39mwith\u001b[39;00m anyio\u001b[39m.\u001b[39mfail_after(timeout):\n\u001b[1;32m    111\u001b[0m         stream: anyio\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mByteStream \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m anyio\u001b[39m.\u001b[39mconnect_tcp(\n\u001b[1;32m    112\u001b[0m             remote_host\u001b[39m=\u001b[39mhost,\n\u001b[1;32m    113\u001b[0m             remote_port\u001b[39m=\u001b[39mport,\n\u001b[1;32m    114\u001b[0m             local_host\u001b[39m=\u001b[39mlocal_address,\n\u001b[1;32m    115\u001b[0m         )\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/anyio/_core/_tasks.py:118\u001b[0m, in \u001b[0;36mFailAfterContextManager.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cancel_scope\u001b[39m.\u001b[39mcancel_called:\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mTimeoutError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpx/_transports/default.py:60\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:  \u001b[39m# noqa: PIE-786\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpx/_transports/default.py:353\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 353\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool\u001b[39m.\u001b[39mhandle_async_request(req)\n\u001b[1;32m    355\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mAsyncIterable)\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpcore/_async/connection_pool.py:253\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 253\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    254\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpcore/_async/connection_pool.py:237\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m connection\u001b[39m.\u001b[39mhandle_async_request(request)\n\u001b[1;32m    238\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    239\u001b[0m     \u001b[39m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[39m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[39m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[39m# up as HTTP/1.1.\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpcore/_async/connection.py:86\u001b[0m, in \u001b[0;36mAsyncHTTPConnection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connect_failed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m         \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m     87\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mis_available():\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpcore/_async/connection.py:63\u001b[0m, in \u001b[0;36mAsyncHTTPConnection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     stream \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connect(request)\n\u001b[1;32m     65\u001b[0m     ssl_object \u001b[39m=\u001b[39m stream\u001b[39m.\u001b[39mget_extra_info(\u001b[39m\"\u001b[39m\u001b[39mssl_object\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpcore/_async/connection.py:111\u001b[0m, in \u001b[0;36mAsyncHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m Trace(\n\u001b[1;32m    109\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mconnection.connect_tcp\u001b[39m\u001b[39m\"\u001b[39m, request, kwargs\n\u001b[1;32m    110\u001b[0m ) \u001b[39mas\u001b[39;00m trace:\n\u001b[0;32m--> 111\u001b[0m     stream \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_network_backend\u001b[39m.\u001b[39mconnect_tcp(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    112\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m stream\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpcore/backends/auto.py:29\u001b[0m, in \u001b[0;36mAutoBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_backend()\n\u001b[0;32m---> 29\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mconnect_tcp(\n\u001b[1;32m     30\u001b[0m     host, port, timeout\u001b[39m=\u001b[39mtimeout, local_address\u001b[39m=\u001b[39mlocal_address\n\u001b[1;32m     31\u001b[0m )\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpcore/backends/asyncio.py:109\u001b[0m, in \u001b[0;36mAsyncIOBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address)\u001b[0m\n\u001b[1;32m    104\u001b[0m exc_map \u001b[39m=\u001b[39m {\n\u001b[1;32m    105\u001b[0m     \u001b[39mTimeoutError\u001b[39;00m: ConnectTimeout,\n\u001b[1;32m    106\u001b[0m     \u001b[39mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    107\u001b[0m     anyio\u001b[39m.\u001b[39mBrokenResourceError: ConnectError,\n\u001b[1;32m    108\u001b[0m }\n\u001b[0;32m--> 109\u001b[0m \u001b[39mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    110\u001b[0m     \u001b[39mwith\u001b[39;00m anyio\u001b[39m.\u001b[39mfail_after(timeout):\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[1;32m    154\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[39mraise\u001b[39;00m to_exc(exc)\n\u001b[1;32m     15\u001b[0m \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mConnectTimeout\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmarvin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloaders\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgithub\u001b[39;00m \u001b[39mimport\u001b[39;00m GitHubRepoLoader\n\u001b[0;32m----> 3\u001b[0m \u001b[39mawait\u001b[39;00m GitHubRepoLoader(\n\u001b[1;32m      4\u001b[0m     repo\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPrefectHQ/prefect\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     glob\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m**/*.md\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     exclude_glob\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m**/docs/api-ref/**\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m )\u001b[39m.\u001b[39mload_and_store(topic_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprefect\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/src/open-source/marvin/src/marvin/loaders/base.py:35\u001b[0m, in \u001b[0;36mLoader.load_and_store\u001b[0;34m(self, topic_name)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoaded \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(digest\u001b[39m.\u001b[39mdocuments)\u001b[39m}\u001b[39;00m\u001b[39m documents from GitHub\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m MarvinClient() \u001b[39mas\u001b[39;00m client:\n\u001b[0;32m---> 35\u001b[0m     \u001b[39mawait\u001b[39;00m client\u001b[39m.\u001b[39mwrite_to_topic(topic_name, digest)\n",
      "File \u001b[0;32m~/src/open-source/marvin/src/marvin/client.py:11\u001b[0m, in \u001b[0;36mMarvinClient.write_to_topic\u001b[0;34m(self, topic_name, digest)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mwrite_to_topic\u001b[39m(\u001b[39mself\u001b[39m, topic_name: \u001b[39mstr\u001b[39m, digest: Digest):\n\u001b[1;32m     10\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Write a digest to a topic.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_url\u001b[39m}\u001b[39;00m\u001b[39m/topics/\u001b[39m\u001b[39m{\u001b[39;00mtopic_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, json\u001b[39m=\u001b[39mdigest\u001b[39m.\u001b[39mdict())\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpx/_client.py:1848\u001b[0m, in \u001b[0;36mAsyncClient.post\u001b[0;34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1827\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1828\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1829\u001b[0m     url: URLTypes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     extensions: typing\u001b[39m.\u001b[39mOptional[RequestExtensions] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1842\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Response:\n\u001b[1;32m   1843\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1844\u001b[0m \u001b[39m    Send a `POST` request.\u001b[39;00m\n\u001b[1;32m   1845\u001b[0m \n\u001b[1;32m   1846\u001b[0m \u001b[39m    **Parameters**: See `httpx.request`.\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1848\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest(\n\u001b[1;32m   1849\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPOST\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1850\u001b[0m         url,\n\u001b[1;32m   1851\u001b[0m         content\u001b[39m=\u001b[39mcontent,\n\u001b[1;32m   1852\u001b[0m         data\u001b[39m=\u001b[39mdata,\n\u001b[1;32m   1853\u001b[0m         files\u001b[39m=\u001b[39mfiles,\n\u001b[1;32m   1854\u001b[0m         json\u001b[39m=\u001b[39mjson,\n\u001b[1;32m   1855\u001b[0m         params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m   1856\u001b[0m         headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m   1857\u001b[0m         cookies\u001b[39m=\u001b[39mcookies,\n\u001b[1;32m   1858\u001b[0m         auth\u001b[39m=\u001b[39mauth,\n\u001b[1;32m   1859\u001b[0m         follow_redirects\u001b[39m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1860\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m   1861\u001b[0m         extensions\u001b[39m=\u001b[39mextensions,\n\u001b[1;32m   1862\u001b[0m     )\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpx/_client.py:1533\u001b[0m, in \u001b[0;36mAsyncClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1504\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[39mBuild and send a request.\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[39m[0]: /advanced/#merging-of-configuration\u001b[39;00m\n\u001b[1;32m   1519\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1520\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_request(\n\u001b[1;32m   1521\u001b[0m     method\u001b[39m=\u001b[39mmethod,\n\u001b[1;32m   1522\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1531\u001b[0m     extensions\u001b[39m=\u001b[39mextensions,\n\u001b[1;32m   1532\u001b[0m )\n\u001b[0;32m-> 1533\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(request, auth\u001b[39m=\u001b[39mauth, follow_redirects\u001b[39m=\u001b[39mfollow_redirects)\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpx/_client.py:1620\u001b[0m, in \u001b[0;36mAsyncClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1612\u001b[0m follow_redirects \u001b[39m=\u001b[39m (\n\u001b[1;32m   1613\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfollow_redirects\n\u001b[1;32m   1614\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m   1615\u001b[0m     \u001b[39melse\u001b[39;00m follow_redirects\n\u001b[1;32m   1616\u001b[0m )\n\u001b[1;32m   1618\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m-> 1620\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_handling_auth(\n\u001b[1;32m   1621\u001b[0m     request,\n\u001b[1;32m   1622\u001b[0m     auth\u001b[39m=\u001b[39mauth,\n\u001b[1;32m   1623\u001b[0m     follow_redirects\u001b[39m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1624\u001b[0m     history\u001b[39m=\u001b[39m[],\n\u001b[1;32m   1625\u001b[0m )\n\u001b[1;32m   1626\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1627\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpx/_client.py:1648\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1645\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m auth_flow\u001b[39m.\u001b[39m\u001b[39m__anext__\u001b[39m()\n\u001b[1;32m   1647\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1648\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m   1649\u001b[0m         request,\n\u001b[1;32m   1650\u001b[0m         follow_redirects\u001b[39m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1651\u001b[0m         history\u001b[39m=\u001b[39mhistory,\n\u001b[1;32m   1652\u001b[0m     )\n\u001b[1;32m   1653\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1654\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpx/_client.py:1685\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1682\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mrequest\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m   1683\u001b[0m     \u001b[39mawait\u001b[39;00m hook(request)\n\u001b[0;32m-> 1685\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_single_request(request)\n\u001b[1;32m   1686\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1687\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpx/_client.py:1722\u001b[0m, in \u001b[0;36mAsyncClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1718\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1719\u001b[0m     )\n\u001b[1;32m   1721\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[0;32m-> 1722\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m transport\u001b[39m.\u001b[39mhandle_async_request(request)\n\u001b[1;32m   1724\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, AsyncByteStream)\n\u001b[1;32m   1725\u001b[0m response\u001b[39m.\u001b[39mrequest \u001b[39m=\u001b[39m request\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpx/_transports/default.py:352\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(request\u001b[39m.\u001b[39mstream, AsyncByteStream)\n\u001b[1;32m    340\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[1;32m    341\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    342\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    351\u001b[0m )\n\u001b[0;32m--> 352\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m    353\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool\u001b[39m.\u001b[39mhandle_async_request(req)\n\u001b[1;32m    355\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mAsyncIterable)\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    151\u001b[0m     value \u001b[39m=\u001b[39m typ()\n\u001b[1;32m    152\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[1;32m    154\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/micromamba/envs/marvin310/lib/python3.10/site-packages/httpx/_transports/default.py:77\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[39mraise\u001b[39;00m\n\u001b[1;32m     76\u001b[0m message \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(exc)\n\u001b[0;32m---> 77\u001b[0m \u001b[39mraise\u001b[39;00m mapped_exc(message) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[0;31mConnectTimeout\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from marvin.loaders.github import GitHubRepoLoader\n",
    "\n",
    "await GitHubRepoLoader(\n",
    "    repo=\"PrefectHQ/prefect\",\n",
    "    glob=\"**/*.md\",\n",
    "    exclude_glob=\"**/docs/api-ref/**\",\n",
    ").load_and_store(topic_name=\"prefect\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
