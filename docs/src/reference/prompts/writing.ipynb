{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Prompts\n",
    "With Marvin, you prompt with code to define dynamic prompts using code, eliminating the need for cumbersome template management. With this approach, you can easily create reusable and modular prompts, streamlining the development process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a world-class expert on python. When asked questions about python, you answer correctly.'},\n",
       " {'role': 'user',\n",
       "  'content': 'I need to know how to write a function to find the nth Fibonacci number.'},\n",
       " {'role': 'assistant', 'content': \"Let's think step by step.\"}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from marvin.prompts.library import System, User, ChainOfThought\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class ExpertSystem(System):\n",
    "    content: str = (\n",
    "        \"You are a world-class expert on {{topic}}. \"\n",
    "        \"When asked questions about {{topic}}, you answer correctly.\"\n",
    "    )\n",
    "    topic: Optional[str]\n",
    "\n",
    "\n",
    "prompt = (\n",
    "    ExpertSystem(topic=\"python\")\n",
    "    | User(\"I need to know how to write a function to find the nth Fibonacci number.\")\n",
    "    | ChainOfThought()  # Tell the LLM to think step by step\n",
    ")\n",
    "\n",
    "prompt.dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Templating Prompts\n",
    "\n",
    "In many applications, templating is unavoidable. In these cases, Marvin's optional templating engine simplifies the process of sharing context across prompts to an unprecedented level. By passing native Python types or Pydantic objects into the rendering engine, you can seamlessly establish context for entire conversations. This feature enables effortless information flow and context continuity throughout the prompt interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a world-class expert on rust. When asked questions about rust, you answer correctly.'},\n",
       " {'role': 'user',\n",
       "  'content': 'I need to know how to write a function in rust to find the nth Fibonacci number.'},\n",
       " {'role': 'assistant', 'content': \"Let's think step by step.\"}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from marvin.prompts.library import System, User, ChainOfThought\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class ExpertSystem(System):\n",
    "    content: str = (\n",
    "        \"You are a world-class expert on {{topic}}. \"\n",
    "        \"When asked questions about {{topic}}, you answer correctly.\"\n",
    "    )\n",
    "    topic: Optional[str]\n",
    "\n",
    "\n",
    "prompt = (\n",
    "    ExpertSystem()\n",
    "    | User(\n",
    "        \"I need to know how to write a function in {{topic}} to find the nth Fibonacci\"\n",
    "        \" number.\"\n",
    "    )\n",
    "    | ChainOfThought()  # Tell the LLM to think step by step\n",
    ")\n",
    "\n",
    "prompt.dict(topic=\"rust\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, the prompt sets up a simulated conversation where the system establishes the user's expertise level in Python. The user then expresses their need to understand how to write a function in Python. To facilitate a step-by-step thought process, the ChainOfThought() function is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from marvin.prompts.library import System\n",
    "\n",
    "\n",
    "class ReActPattern(System):\n",
    "    content = \"\"\"\n",
    "    You run in a loop of Thought, Action, PAUSE, Observation.\n",
    "    At the end of the loop you output an Answer\n",
    "    Use Thought to describe your thoughts about the question you have been asked.\n",
    "    Use Action to run one of the actions available to you - then return PAUSE.\n",
    "    Observation will be the result of running those actions.\n",
    "  \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marvin provides simple, opinionated components so that you can subclass and customize\n",
    "prompts the same way you would for code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you chose to, you may query a table whose schema is defined below:\n",
      "\n",
      "- last_login: Date and time of user's last login\n",
      "- date_created: Date and time when the user record was created\n",
      "- date_last_purchase: Date and time of user's last purchase\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from marvin.prompts.library import System\n",
    "import pydantic\n",
    "\n",
    "\n",
    "class ColumnInfo(pydantic.BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "\n",
    "\n",
    "class SQLTableDescription(System):\n",
    "    content = \"\"\"\n",
    "    If you chose to, you may query a table whose schema is defined below:\n",
    "    \n",
    "    {% for column in columns %}\n",
    "    - {{ column.name }}: {{ column.description }}\n",
    "    {% endfor %}\n",
    "    \"\"\"\n",
    "\n",
    "    columns: list[ColumnInfo] = pydantic.Field(\n",
    "        ..., description=\"name, description pairs of SQL Schema\"\n",
    "    )\n",
    "\n",
    "\n",
    "UserQueryPrompt = SQLTableDescription(\n",
    "    columns=[\n",
    "        ColumnInfo(name=\"last_login\", description=\"Date and time of user's last login\"),\n",
    "        ColumnInfo(\n",
    "            name=\"date_created\",\n",
    "            description=\"Date and time when the user record was created\",\n",
    "        ),\n",
    "        ColumnInfo(\n",
    "            name=\"date_last_purchase\",\n",
    "            description=\"Date and time of user's last purchase\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(UserQueryPrompt.read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering Prompts\n",
    "\n",
    "A chain of prompts is turned into messages with the `render_prompts` function. This funciton has a few responsibilities in addition to generating messages: it renders templates using runtime variables, sorts messages, and trims prompts to fit into a model's context window. The last two actions depend on the optional `position` and `priority` attributes of each prompt. \n",
    "\n",
    "For example, the `System` prompt defines `position=0` and `priority=1` to indicate that system prompts should be rendered first and given high priority when trimming the context. (As an optimization, `render_prompt` automatically combines multiple system messages into a single message.) `ChainOfThought()` has a `position=-1` to indicate it should be the last message. If position is not set explicitly, then prompts will take the order they are added to the chain. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
