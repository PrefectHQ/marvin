{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Classifier\n",
    "\n",
    "AI Classifiers are a high-level component, or building block, of Marvin. Like all Marvin components, they are completely standalone: you're free to use them with or without the rest of Marvin."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition abstract\">\n",
    "  <p class=\"admonition-title\">What it does</p>\n",
    "  <p>\n",
    "    <code>@ai_classifier</code> is a decorator that lets you use LLMs to choose options, tools, or classify input. \n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CustomerIntent.BILLING_ACCOUNTS: 3>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from marvin import ai_classifier\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "@ai_classifier\n",
    "class CustomerIntent(Enum):\n",
    "    \"\"\"Classifies the incoming users intent\"\"\"\n",
    "\n",
    "    SALES = 1\n",
    "    TECHNICAL_SUPPORT = 2\n",
    "    BILLING_ACCOUNTS = 3\n",
    "    PRODUCT_INFORMATION = 4\n",
    "    RETURNS_REFUNDS = 5\n",
    "    ORDER_STATUS = 6\n",
    "    ACCOUNT_CANCELLATION = 7\n",
    "    OPERATOR_CUSTOMER_SERVICE = 0\n",
    "\n",
    "\n",
    "CustomerIntent(\"I got double charged, can you help me out?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition info\">\n",
    "  <p class=\"admonition-title\">How it works</p>\n",
    "  <p>\n",
    "    Marvin enumerates your options, and uses a <a href=\"https://twitter.com/AAAzzam/status/1669753721574633473\">clever logit bias trick</a> to force an LLM to deductively choose the index of the best option given your provided input. It then returns the choice associated with that index.\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "  <p class=\"admonition-title\">When to use</p>\n",
    "  <p>\n",
    "    <ol>\n",
    "    <li> Best for classification tasks when no training data is available. \n",
    "    <li> Best for writing classifiers that need deduction or inference.\n",
    "    </ol>\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"admonition warning\">\n",
    "  <p class=\"admonition-title\">OpenAI compatibility</p>\n",
    "  <p> The technique that AI Classifiers use for speed and correctness is only available through the OpenAI API at this time. Therefore, AI Classifiers can only be used with OpenAI-compatible LLMs, including the Azure OpenAI service.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an AI Classifier\n",
    "\n",
    "AI Classifiers are Python `Enums`, or classes that can represent one of many possible options. To build an effective AI Classifier, be as specific as possible with your class name, docstring, option names, and option values.\n",
    "\n",
    "To build a minimal AI Classifier, decorate any standard enum, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sentiment.POSITIVE: 'POSITIVE'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from marvin import ai_classifier\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "@ai_classifier\n",
    "class Sentiment(Enum):\n",
    "    POSITIVE = \"POSITIVE\"\n",
    "    NEGATIVE = \"NEGATIVE\"\n",
    "\n",
    "\n",
    "Sentiment(\"That looks great!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because AI Classifiers are enums, you can use any enum construction you want, including the all-caps string approach above, integer values, `enum.auto()`, or complex values. The only thing to remember is that the class you build *is* essentially the instruction that gets sent to the LLM, so the more information you provide, the better your classifier will behave.\n",
    "\n",
    "For example, you may want to have a classifier that has a Python object (like an AI Model!) as its value, but still need to provide instruction hints to the LLM. One way to achieve that is to add descriptions to your classifier's values that will become visible to the LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating!\n"
     ]
    }
   ],
   "source": [
    "# dummy objects that stand in for complex tools\n",
    "WebSearch = lambda: print(\"Searching!\")\n",
    "Calculator = lambda: print(\"Calculating!\")\n",
    "Translator = lambda: print(\"Translating!\")\n",
    "\n",
    "\n",
    "@ai_classifier\n",
    "class Router(Enum):\n",
    "    translate = dict(tool=Translator, description=\"A translator tool\")\n",
    "    web_search = dict(tool=WebSearch, description=\"A web search tool\")\n",
    "    calculator = dict(tool=Calculator, description=\"A calculator tool\")\n",
    "\n",
    "\n",
    "result = Router(\"Whats 2+2?\")\n",
    "result.value[\"tool\"]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring an AI Classifier\n",
    "\n",
    "In addition to how you define the AI classifier itself, there are two ways to control its behavior at runtime: `instructions` and `model`.\n",
    "\n",
    "### Providing instructions\n",
    "You can control an AI classifier's behavior by providing instructions. This can either be provided globally as the classifier's docstring or on a per-call basis when you instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sentiment.POSITIVE: 1>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@ai_classifier\n",
    "class Sentiment(Enum):\n",
    "    \"\"\"\n",
    "    Score the sentiment of provided text.\n",
    "    \"\"\"\n",
    "\n",
    "    POSITIVE = 1\n",
    "    NEGATIVE = -1\n",
    "\n",
    "\n",
    "Sentiment(\"Everything is awesome!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sentiment.NEGATIVE: -1>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@ai_classifier\n",
    "class Sentiment(Enum):\n",
    "    \"\"\"\n",
    "    How would a very very sad person rate the text?\n",
    "    \"\"\"\n",
    "\n",
    "    POSITIVE = 1\n",
    "    NEGATIVE = -1\n",
    "\n",
    "\n",
    "Sentiment(\"Everything is awesome!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions can also be provided for each call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sentiment.NEGATIVE: -1>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@ai_classifier\n",
    "class Sentiment(Enum):\n",
    "    POSITIVE = 1\n",
    "    NEGATIVE = -1\n",
    "\n",
    "\n",
    "Sentiment(\"Everything is awesome!\", instructions=\"It's opposite day!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the LLM\n",
    "By default, `@ai_classifier` uses the global LLM settings. To specify a particular LLM, pass it as an argument to the decorator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sentiment.POSITIVE: 1>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from marvin.llms import chat_llm\n",
    "\n",
    "\n",
    "@ai_classifier(model=chat_llm(\"openai/gpt-3.5-turbo-0613\"))\n",
    "class Sentiment(Enum):\n",
    "    POSITIVE = 1\n",
    "    NEGATIVE = -1\n",
    "\n",
    "\n",
    "Sentiment(\"Everything is awesome!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "#### üöÖ Bulletproof\n",
    "\n",
    "`ai_classifier` will always output one of the options you've given it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AppRoute.USER_PROFILE: '/user-profile'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from marvin import ai_classifier\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "@ai_classifier\n",
    "class AppRoute(Enum):\n",
    "    \"\"\"Represents distinct routes command bar for a different application\"\"\"\n",
    "\n",
    "    USER_PROFILE = \"/user-profile\"\n",
    "    SEARCH = \"/search\"\n",
    "    NOTIFICATIONS = \"/notifications\"\n",
    "    SETTINGS = \"/settings\"\n",
    "    HELP = \"/help\"\n",
    "    CHAT = \"/chat\"\n",
    "    DOCS = \"/docs\"\n",
    "    PROJECTS = \"/projects\"\n",
    "    WORKSPACES = \"/workspaces\"\n",
    "\n",
    "\n",
    "AppRoute(\"update my name\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üèÉ Fast\n",
    "\n",
    "`ai_classifier` only asks your LLM to output one token, so it's blazing fast - on the order of ~200ms in testing.\n",
    "\n",
    "#### ü´° Deterministic\n",
    "\n",
    "`ai_classifier` will be deterministic so long as the underlying model and options does not change."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
