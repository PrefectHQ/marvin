{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"ai_style_guide/","title":"Ai style guide","text":"<p>We're going to write documentation for Marvin together.</p> <p>First, here's a style guide.</p>"},{"location":"ai_style_guide/#ai-style-guide","title":"AI Style Guide","text":"<p>A style guide for AI documentation authors to adhere to Marvin's documentation standards. Remember, you are an expert technical writer with an extensive background in educating and explaining open-source software. You are not a marketer, a salesperson, or a product manager. Marvin's documentation should resemble renowned technical documentation like Stripe. </p> <p>You must follow the below guide. Do not deviate from it.</p>"},{"location":"ai_style_guide/#prose","title":"Prose","text":"<ul> <li>Aim for engaging and extensive prose, tailored for a technical audience.</li> <li>Prose should not be superlative or flowery, but rather clear, direct, and concise.</li> <li>Do not use overblown language like \"Marvin introduces a versatile extract function, a cornerstone in text entity extraction.\" Do not write things like \"This showcases Marvin's ability to...\" after an example unless it is truly mind-blowing.</li> <li>Maintain a lighthearted and fun tone, but avoid being overly casual or silly.</li> <li>Use sentence case for all headers and titles. Prefer brevity over verbosity for titles.</li> <li>Try not to put <code>code</code> in headers or titles (e.g. prefer \"Overview\" to \"Overview of <code>extract()</code>\"). If you must, use <code>code</code> in headers or titles sparingly.</li> <li>Do not put \"in Marvin\" in your headers; this is Marvin's documentation, so it's implied.</li> <li>Use multiple examples and code snippets to vividly demonstrate concepts.</li> <li>Ensure a feature is thoroughly documented; undocumented features are considered non-existent.</li> <li>Avoid creating lists in prose; integrate information into fluid paragraphs.</li> </ul>"},{"location":"ai_style_guide/#concept-documentation","title":"Concept Documentation","text":"<ul> <li>Dedicate a full page to each concept.</li> <li>Write detailed explanations of each concept, including all aspects of its configuration. Emphasize natural language's role in Marvin's LLM runtime. Paragraphs instead of sentences: engage all aspects of the explanation.</li> <li>Concept pages should contain expansive sections like an overview, a getting started example, a more in-depth exploration of the functionality, and best practices.</li> <li>Concept pages should have a motivating example or simple illustration of the functionality near the top, so users can get a quick feel without scrolling. It's ok if this comes before the in-depth exploration and even other examples</li> <li>When documenting multiple interfaces for a concept, focus more on the primary or preferred interface. If no preference, use them equally but ensure thorough coverage.</li> </ul>"},{"location":"ai_style_guide/#code","title":"Code","text":"<ul> <li>Import or define items once per page, reusing them in subsequent examples.</li> <li>Use the full \"marvin\" qualifier in interfaces, e.g., <code>marvin.classify()</code>, not just <code>classify()</code>.</li> <li>Follow best practices in code examples, showcasing Marvin's capabilities while educating the reader.</li> <li>Include numerous examples. If there are multiple ways to achieve a task, demonstrate all of them.</li> </ul>"},{"location":"api_reference/marvin/","title":"Marvin API reference","text":""},{"location":"api_reference/marvin/#marvin.Image","title":"<code>Image</code>","text":""},{"location":"api_reference/marvin/#marvin.Image.render_for_transcript","title":"<code>render_for_transcript</code>","text":"<p>Renders a JSON representation of the image for use in a Transcript object, including IMAGE and TEXT tags.</p>"},{"location":"api_reference/marvin/#marvin.Model","title":"<code>Model</code>","text":"<p>A Pydantic model that can be instantiated from a natural language string, in addition to keyword arguments.</p>"},{"location":"api_reference/marvin/#marvin.Model.from_text_async","title":"<code>from_text_async</code>  <code>async</code> <code>classmethod</code>","text":"<p>Class method to create an instance of the model from a natural language string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The natural language string to convert into an instance of the model.</p> required <code>instructions</code> <code>str</code> <p>Specific instructions for the conversion. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>An instance of the model.</p> Example <pre><code>from marvin.ai.text import Model\nclass Location(Model):\n    '''A location'''\n    city: str\n    state: str\n    country: str\n\nawait Location.from_text_async(\"big apple, ny, usa\")\n</code></pre>"},{"location":"api_reference/marvin/#marvin.caption","title":"<code>caption</code>","text":"<p>Generates a caption for an image using a language model synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Image, List[Image]]</code> <p>The image or images to caption.</p> required <code>instructions</code> <code>str</code> <p>Instructions for the caption generation.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional arguments for the language model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated caption.</p>"},{"location":"api_reference/marvin/#marvin.caption_async","title":"<code>caption_async</code>  <code>async</code>","text":"<p>Generates a caption for a set of images using a language model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Image, List[Image]]</code> <p>The image or images to caption.</p> required <code>instructions</code> <code>str</code> <p>Instructions for the caption generation.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional arguments for the language model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated caption.</p>"},{"location":"api_reference/marvin/#marvin.cast","title":"<code>cast</code>","text":"<p>Converts the input data into the specified type.</p> <p>This function uses a language model to convert the input data into a specified type. The conversion process can be guided by specific instructions. The function also supports additional arguments for the language model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>FN_INPUT_TYPES</code> <p>Union[str, Image, list[Union[str, Image]]]: the data to which the function will be applied.</p> required <code>target</code> <code>type</code> <p>The type to convert the data into. If none is provided but instructions are provided, <code>str</code> is assumed.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Specific instructions for the conversion. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>The converted data of the specified type.</p>"},{"location":"api_reference/marvin/#marvin.cast_async","title":"<code>cast_async</code>  <code>async</code>","text":"<p>Converts the input data into the specified type.</p> <p>This function uses a language model to convert the input data into a specified type. The conversion process can be guided by specific instructions. The function also supports additional arguments for the language model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>FN_INPUT_TYPES</code> <p>Union[str, Image, list[Union[str, Image]]]: the data to which the function will be applied.</p> required <code>target</code> <code>type</code> <p>The type to convert the data into. If none is provided but instructions are provided, <code>str</code> is assumed.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Specific instructions for the conversion. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>The converted data of the specified type.</p>"},{"location":"api_reference/marvin/#marvin.classifier","title":"<code>classifier</code>","text":"<p>Class decorator that modifies the behavior of an Enum class to classify a string.</p> <p>This decorator modifies the call method of the Enum class to use the <code>marvin.classify</code> function instead of the default Enum behavior. This allows the Enum class to classify a string based on its members.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Enum</code> <p>The Enum class to be decorated.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Instructions for the AI on how to perform the classification.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Enum</code> <p>The decorated Enum class with modified call method.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the decorated class is not a subclass of Enum.</p>"},{"location":"api_reference/marvin/#marvin.classify","title":"<code>classify</code>","text":"<p>Classifies the provided data based on the provided labels.</p> <p>This function uses a language model with a logit bias to classify the input data. The logit bias constrains the language model's response to a single token, making this function highly efficient for classification tasks. The function will always return one of the provided labels.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>FN_INPUT_TYPES</code> <p>Union[str, Image, list[Union[str, Image]]]: the data to which the function will be applied.</p> required <code>labels</code> <code>Union[Enum, list[T], type]</code> <p>The labels to classify the data into.</p> required <code>instructions</code> <code>str</code> <p>Specific instructions for the classification. Defaults to None.</p> <code>None</code> <code>return_index</code> <code>bool</code> <p>Whether to return the index of the label instead of the label itself.</p> <code>False</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[T, int]</code> <p>Union[T, int]: The label or index that the data was classified into.</p>"},{"location":"api_reference/marvin/#marvin.classify_async","title":"<code>classify_async</code>  <code>async</code>","text":"<p>Classifies the provided data based on the provided labels.</p> <p>This function uses a language model with a logit bias to classify the input data. The logit bias constrains the language model's response to a single token, making this function highly efficient for classification tasks. The function will always return one of the provided labels.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>FN_INPUT_TYPES</code> <p>Union[str, Image, list[Union[str, Image]]]: the data to which the function will be applied.</p> required <code>labels</code> <code>Union[Enum, list[T], type]</code> <p>The labels to classify the data into.</p> required <code>instructions</code> <code>str</code> <p>Specific instructions for the classification. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[T, int]</code> <p>Union[T, int]: The label or index that the data was classified into.</p>"},{"location":"api_reference/marvin/#marvin.extract","title":"<code>extract</code>","text":"<p>Extracts entities of a specific type from the provided data.</p> <p>This function uses a language model to identify and extract entities of the specified type from the input data. The extracted entities are returned as a list.</p> <p>Note that either a target type or instructions must be provided (or both). If only instructions are provided, the target type is assumed to be a string.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>FN_INPUT_TYPES</code> <p>Union[str, Image, list[Union[str, Image]]]: the data to which the function will be applied.</p> required <code>target</code> <code>type</code> <p>The type of entities to extract.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Specific instructions for the extraction. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[T]</code> <p>A list of extracted entities of the specified type.</p>"},{"location":"api_reference/marvin/#marvin.extract_async","title":"<code>extract_async</code>  <code>async</code>","text":"<p>Extracts entities of a specific type from the provided data.</p> <p>This function uses a language model to identify and extract entities of the specified type from the input data. The extracted entities are returned as a list.</p> <p>Note that either a target type or instructions must be provided (or both). If only instructions are provided, the target type is assumed to be a string.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>FN_INPUT_TYPES</code> <p>Union[str, Image, list[Union[str, Image]]]: the data to which the function will be applied.</p> required <code>target</code> <code>type</code> <p>The type of entities to extract.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Specific instructions for the extraction. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>MarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[T]</code> <p>A list of extracted entities of the specified type.</p>"},{"location":"api_reference/marvin/#marvin.fn","title":"<code>fn</code>","text":"<p>Converts a Python function into an AI function using a decorator.</p> <p>This decorator allows a Python function to be converted into an AI function. The AI function uses a language model to generate its output.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The function to be converted. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>The converted AI function.</p> Example <pre><code>@fn\ndef list_fruit(n:int) -&gt; list[str]:\n    '''generates a list of n fruit'''\n\nlist_fruit(3) # ['apple', 'banana', 'orange']\n</code></pre>"},{"location":"api_reference/marvin/#marvin.generate","title":"<code>generate</code>","text":"<p>Generates a list of 'n' items of the provided type or based on instructions.</p> <p>Either a type or instructions must be provided. If instructions are provided without a type, the type is assumed to be a string. The function generates at least 'n' items.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>type</code> <p>The type of items to generate. Defaults to None.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Instructions for the generation. Defaults to None.</p> <code>None</code> <code>n</code> <code>int</code> <p>The number of items to generate. Defaults to 1.</p> <code>1</code> <code>use_cache</code> <code>bool</code> <p>If True, the function will cache the last 100 responses for each (target, instructions, and temperature) and use those to avoid repetition on subsequent calls. Defaults to True.</p> <code>True</code> <code>temperature</code> <code>float</code> <p>The temperature for the generation. Defaults to 1.</p> <code>1</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[T]</code> <p>A list of generated items.</p>"},{"location":"api_reference/marvin/#marvin.generate_async","title":"<code>generate_async</code>  <code>async</code>","text":"<p>Generates a list of 'n' items of the provided type or based on instructions.</p> <p>Either a type or instructions must be provided. If instructions are provided without a type, the type is assumed to be a string. The function generates at least 'n' items.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>type</code> <p>The type of items to generate. Defaults to None.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Instructions for the generation. Defaults to None.</p> <code>None</code> <code>n</code> <code>int</code> <p>The number of items to generate. Defaults to 1.</p> <code>1</code> <code>use_cache</code> <code>bool</code> <p>If True, the function will cache the last 100 responses for each (target, instructions, and temperature) and use those to avoid repetition on subsequent calls. Defaults to True.</p> <code>True</code> <code>temperature</code> <code>float</code> <p>The temperature for the generation. Defaults to 1.</p> <code>1</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[T]</code> <p>A list of generated items.</p>"},{"location":"api_reference/marvin/#marvin.image","title":"<code>image</code>","text":"<p>A decorator that transforms a function's output into an image.</p> <p>This decorator takes a function that returns a string, and uses that string as instructions to generate an image. The generated image is then returned.</p> <p>The decorator can be used with or without parentheses. If used without parentheses, the decorated function's output is used as the instructions for the image. If used with parentheses, an optional <code>literal</code> argument can be provided. If <code>literal</code> is set to <code>True</code>, the function's output is used as the literal instructions for the image, without any modifications.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>callable</code> <p>The function to decorate. If <code>None</code>, the decorator is being used with parentheses, and <code>fn</code> will be provided later.</p> <code>None</code> <code>literal</code> <code>bool</code> <p>Whether to use the function's output as the literal instructions for the image. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>callable</code> <p>The decorated function.</p>"},{"location":"api_reference/marvin/#marvin.model","title":"<code>model</code>","text":"<p>Class decorator for instantiating a Pydantic model from a string.</p> <p>This decorator allows a Pydantic model to be instantiated from a string. It's equivalent to subclassing the Model class.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>Union[Type[M], None]</code> <p>The type of the Pydantic model. Defaults to None.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Specific instructions for the conversion.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Type[M], Callable[[Type[M]], Type[M]]]</code> <p>Union[Type[M], Callable[[Type[M]], Type[M]]]: The decorated Pydantic model.</p>"},{"location":"api_reference/marvin/#marvin.paint","title":"<code>paint</code>","text":"<p>Generates an image based on the provided instructions and context.</p> <p>This function uses the DALLE-3 API to generate an image based on the provided instructions and context. By default, the API modifies prompts to add detail and style. This behavior can be disabled by setting <code>literal=True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>instructions</code> <code>str</code> <p>The instructions for the image generation. Defaults to None.</p> <code>None</code> <code>context</code> <code>dict</code> <p>The context for the image generation. Defaults to None.</p> <code>None</code> <code>literal</code> <code>bool</code> <p>Whether to disable the API's default behavior of modifying prompts. Defaults to False.</p> <code>False</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ImagesResponse</code> <p>The response from the DALLE-3 API, which includes the generated image.</p>"},{"location":"api_reference/marvin/#marvin.paint_async","title":"<code>paint_async</code>  <code>async</code>","text":"<p>Generates an image based on the provided instructions and context.</p> <p>This function uses the DALLE-3 API to generate an image based on the provided instructions and context. By default, the API modifies prompts to add detail and style. This behavior can be disabled by setting <code>literal=True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>instructions</code> <code>str</code> <p>The instructions for the image generation. Defaults to None.</p> <code>None</code> <code>context</code> <code>dict</code> <p>The context for the image generation. Defaults to None.</p> <code>None</code> <code>literal</code> <code>bool</code> <p>Whether to disable the API's default behavior of modifying prompts. Defaults to False.</p> <code>False</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ImagesResponse</code> <p>The response from the DALLE-3 API, which includes the generated image.</p>"},{"location":"api_reference/marvin/#marvin.speak","title":"<code>speak</code>","text":"<p>Generates audio from text using an AI.</p> <p>This function uses an AI to generate audio from the provided text. The voice used for the audio can be specified.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to generate audio from.</p> required <code>voice</code> <code>Literal['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']</code> <p>The voice to use for the audio. Defaults to None.</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Whether to stream the audio. If False, the audio can not be saved or played until it has all been generated. If True, <code>.save()</code> and <code>.play()</code> can be called immediately.</p> <code>True</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Audio</code> <code>Audio</code> <p>The generated audio.</p>"},{"location":"api_reference/marvin/#marvin.speak_async","title":"<code>speak_async</code>  <code>async</code>","text":"<p>Generates audio from text using an AI.</p> <p>This function uses an AI to generate audio from the provided text. The voice used for the audio can be specified.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to generate audio from.</p> required <code>voice</code> <code>Literal['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']</code> <p>The voice to use for the audio. Defaults to None.</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Whether to stream the audio. If False, the audio can not be saved or played until it has all been generated. If True, <code>.save()</code> and <code>.play()</code> can be called immediately.</p> <code>True</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Audio</code> <code>Audio</code> <p>The generated audio.</p>"},{"location":"api_reference/marvin/#marvin.speech","title":"<code>speech</code>","text":"<p>Function decorator that generates audio from the wrapped function's return value. The voice used for the audio can be specified.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to wrap. Defaults to None.</p> <code>None</code> <code>voice</code> <code>str</code> <p>The voice to use for the audio. Defaults to None.</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Whether to stream the audio. If False, the audio can not be saved or played until it has all been generated. If True, <code>.save()</code> and <code>.play()</code> can be called immediately.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>The wrapped function.</p>"},{"location":"api_reference/marvin/#marvin.transcribe","title":"<code>transcribe</code>","text":"<p>Transcribes audio from a file.</p> <p>This function converts audio from a file to text.</p>"},{"location":"api_reference/marvin/#marvin.transcribe_async","title":"<code>transcribe_async</code>  <code>async</code>","text":"<p>Transcribes audio from a file.</p> <p>This function converts audio from a file to text.</p>"},{"location":"api_reference/settings/","title":"marvin.settings","text":"<p>Settings for configuring <code>marvin</code>.</p>"},{"location":"api_reference/settings/#marvin.settings.AssistantSettings","title":"<code>AssistantSettings</code>","text":"<p>Settings for the assistant API.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The default assistant model to use</p>"},{"location":"api_reference/settings/#marvin.settings.AudioSettings","title":"<code>AudioSettings</code>","text":"<p>Settings for the audio API.</p>"},{"location":"api_reference/settings/#marvin.settings.ImageSettings","title":"<code>ImageSettings</code>","text":"<p>Settings for OpenAI's image API.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The default image model to use, defaults to <code>dall-e-3</code>.</p> <code>size</code> <code>Literal['1024x1024', '1792x1024', '1024x1792']</code> <p>The default image size to use, defaults to <code>1024x1024</code>.</p> <code>response_format</code> <code>Literal['url', 'b64_json']</code> <p>The default response format to use, defaults to <code>url</code>.</p> <code>style</code> <code>Literal['vivid', 'natural']</code> <p>The default style to use, defaults to <code>vivid</code>.</p>"},{"location":"api_reference/settings/#marvin.settings.OpenAISettings","title":"<code>OpenAISettings</code>","text":"<p>Settings for the OpenAI API.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[SecretStr]</code> <p>Your OpenAI API key.</p> <code>organization</code> <code>Optional[str]</code> <p>Your OpenAI organization ID.</p> <code>llms</code> <code>Optional[str]</code> <p>Settings for the chat API.</p> <code>images</code> <code>ImageSettings</code> <p>Settings for the images API.</p> <code>audio</code> <code>AudioSettings</code> <p>Settings for the audio API.</p> <code>assistants</code> <code>AssistantSettings</code> <p>Settings for the assistants API.</p> Example <p>Set the OpenAI API key: <pre><code>import marvin\n\nmarvin.settings.openai.api_key = \"sk-...\"\n\nassert marvin.settings.openai.api_key.get_secret_value() == \"sk-...\"\n</code></pre></p>"},{"location":"api_reference/settings/#marvin.settings.Settings","title":"<code>Settings</code>","text":"<p>Settings for <code>marvin</code>.</p> <p>This is the main settings object for <code>marvin</code>.</p> <p>Attributes:</p> Name Type Description <code>openai</code> <code>OpenAISettings</code> <p>Settings for the OpenAI API.</p> <code>log_level</code> <code>str</code> <p>The log level to use, defaults to <code>INFO</code>.</p> Example <p>Set the log level to <code>INFO</code>: <pre><code>import marvin\n\nmarvin.settings.log_level = \"INFO\"\n\nassert marvin.settings.log_level == \"INFO\"\n</code></pre></p>"},{"location":"api_reference/settings/#marvin.settings.SpeechSettings","title":"<code>SpeechSettings</code>","text":"<p>Settings for OpenAI's speech API.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>The default speech model to use, defaults to <code>tts-1-hd</code>.</p> <code>voice</code> <code>Literal['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']</code> <p>The default voice to use, defaults to <code>echo</code>.</p> <code>response_format</code> <code>Literal['mp3', 'opus', 'aac', 'flac']</code> <p>The default response format to use, defaults to <code>mp3</code>.</p> <code>speed</code> <code>float</code> <p>The default speed to use, defaults to <code>1.0</code>.</p>"},{"location":"api_reference/settings/#marvin.settings.temporary_settings","title":"<code>temporary_settings</code>","text":"<p>Temporarily override Marvin setting values, including nested settings objects.</p> <p>To override nested settings, use <code>__</code> to separate nested attribute names.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>The settings to override, including nested settings.</p> <code>{}</code> Example <p>Temporarily override log level and OpenAI API key: <pre><code>import marvin\nfrom marvin.settings import temporary_settings\n\n# Override top-level settings\nwith temporary_settings(log_level=\"INFO\"):\n    assert marvin.settings.log_level == \"INFO\"\nassert marvin.settings.log_level == \"DEBUG\"\n\n# Override nested settings\nwith temporary_settings(openai__api_key=\"new-api-key\"):\n    assert marvin.settings.openai.api_key.get_secret_value() == \"new-api-key\"\nassert marvin.settings.openai.api_key.get_secret_value().startswith(\"sk-\")\n</code></pre></p>"},{"location":"api_reference/types/","title":"marvin.types","text":""},{"location":"api_reference/types/#marvin.types.Audio","title":"<code>Audio</code>","text":""},{"location":"api_reference/types/#marvin.types.BaseMessage","title":"<code>BaseMessage</code>","text":"<p>Base schema for messages</p>"},{"location":"api_reference/types/#marvin.types.Image","title":"<code>Image</code>","text":""},{"location":"api_reference/types/#marvin.types.Image.render_for_transcript","title":"<code>render_for_transcript</code>","text":"<p>Renders a JSON representation of the image for use in a Transcript object, including IMAGE and TEXT tags.</p>"},{"location":"api_reference/types/#marvin.types.ImageFileContentBlock","title":"<code>ImageFileContentBlock</code>","text":"<p>Schema for messages containing images</p>"},{"location":"api_reference/types/#marvin.types.TextContentBlock","title":"<code>TextContentBlock</code>","text":"<p>Schema for messages containing text</p>"},{"location":"api_reference/ai/audio/","title":"marvin.ai.audio","text":""},{"location":"api_reference/ai/audio/#marvin.ai.audio.generate_speech","title":"<code>generate_speech</code>  <code>async</code>","text":"<p>Generates speech based on a provided prompt template.</p> <p>This function uses the OpenAI Audio API to generate speech based on a provided prompt template. The function supports additional arguments for the prompt and the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_template</code> <code>str</code> <p>The template for the prompt.</p> required <code>prompt_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the prompt. Defaults to None.</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Whether to stream the audio. If False, the audio can not be saved or played until it has all been generated. If True, <code>.save()</code> and <code>.play()</code> can be called immediately.</p> <code>True</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Audio</code> <code>Audio</code> <p>The response from the OpenAI Audio API, which includes the generated speech.</p>"},{"location":"api_reference/ai/audio/#marvin.ai.audio.speak","title":"<code>speak</code>","text":"<p>Generates audio from text using an AI.</p> <p>This function uses an AI to generate audio from the provided text. The voice used for the audio can be specified.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to generate audio from.</p> required <code>voice</code> <code>Literal['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']</code> <p>The voice to use for the audio. Defaults to None.</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Whether to stream the audio. If False, the audio can not be saved or played until it has all been generated. If True, <code>.save()</code> and <code>.play()</code> can be called immediately.</p> <code>True</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Audio</code> <code>Audio</code> <p>The generated audio.</p>"},{"location":"api_reference/ai/audio/#marvin.ai.audio.speak_async","title":"<code>speak_async</code>  <code>async</code>","text":"<p>Generates audio from text using an AI.</p> <p>This function uses an AI to generate audio from the provided text. The voice used for the audio can be specified.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to generate audio from.</p> required <code>voice</code> <code>Literal['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']</code> <p>The voice to use for the audio. Defaults to None.</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Whether to stream the audio. If False, the audio can not be saved or played until it has all been generated. If True, <code>.save()</code> and <code>.play()</code> can be called immediately.</p> <code>True</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Audio</code> <code>Audio</code> <p>The generated audio.</p>"},{"location":"api_reference/ai/audio/#marvin.ai.audio.speech","title":"<code>speech</code>","text":"<p>Function decorator that generates audio from the wrapped function's return value. The voice used for the audio can be specified.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to wrap. Defaults to None.</p> <code>None</code> <code>voice</code> <code>str</code> <p>The voice to use for the audio. Defaults to None.</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Whether to stream the audio. If False, the audio can not be saved or played until it has all been generated. If True, <code>.save()</code> and <code>.play()</code> can be called immediately.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>The wrapped function.</p>"},{"location":"api_reference/ai/audio/#marvin.ai.audio.transcribe","title":"<code>transcribe</code>","text":"<p>Transcribes audio from a file.</p> <p>This function converts audio from a file to text.</p>"},{"location":"api_reference/ai/audio/#marvin.ai.audio.transcribe_async","title":"<code>transcribe_async</code>  <code>async</code>","text":"<p>Transcribes audio from a file.</p> <p>This function converts audio from a file to text.</p>"},{"location":"api_reference/ai/images/","title":"marvin.ai.images","text":""},{"location":"api_reference/ai/images/#marvin.ai.images.generate_image","title":"<code>generate_image</code>  <code>async</code>","text":"<p>Generates an image based on a provided prompt template.</p> <p>This function uses the DALL-E API to generate an image based on a provided prompt template. The function supports additional arguments for the prompt and the model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_template</code> <code>str</code> <p>The template for the prompt.</p> required <code>prompt_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the prompt. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ImagesResponse</code> <code>ImagesResponse</code> <p>The response from the DALL-E API, which includes the generated image.</p>"},{"location":"api_reference/ai/images/#marvin.ai.images.image","title":"<code>image</code>","text":"<p>A decorator that transforms a function's output into an image.</p> <p>This decorator takes a function that returns a string, and uses that string as instructions to generate an image. The generated image is then returned.</p> <p>The decorator can be used with or without parentheses. If used without parentheses, the decorated function's output is used as the instructions for the image. If used with parentheses, an optional <code>literal</code> argument can be provided. If <code>literal</code> is set to <code>True</code>, the function's output is used as the literal instructions for the image, without any modifications.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>callable</code> <p>The function to decorate. If <code>None</code>, the decorator is being used with parentheses, and <code>fn</code> will be provided later.</p> <code>None</code> <code>literal</code> <code>bool</code> <p>Whether to use the function's output as the literal instructions for the image. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>callable</code> <p>The decorated function.</p>"},{"location":"api_reference/ai/images/#marvin.ai.images.paint","title":"<code>paint</code>","text":"<p>Generates an image based on the provided instructions and context.</p> <p>This function uses the DALLE-3 API to generate an image based on the provided instructions and context. By default, the API modifies prompts to add detail and style. This behavior can be disabled by setting <code>literal=True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>instructions</code> <code>str</code> <p>The instructions for the image generation. Defaults to None.</p> <code>None</code> <code>context</code> <code>dict</code> <p>The context for the image generation. Defaults to None.</p> <code>None</code> <code>literal</code> <code>bool</code> <p>Whether to disable the API's default behavior of modifying prompts. Defaults to False.</p> <code>False</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ImagesResponse</code> <p>The response from the DALLE-3 API, which includes the generated image.</p>"},{"location":"api_reference/ai/images/#marvin.ai.images.paint_async","title":"<code>paint_async</code>  <code>async</code>","text":"<p>Generates an image based on the provided instructions and context.</p> <p>This function uses the DALLE-3 API to generate an image based on the provided instructions and context. By default, the API modifies prompts to add detail and style. This behavior can be disabled by setting <code>literal=True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>instructions</code> <code>str</code> <p>The instructions for the image generation. Defaults to None.</p> <code>None</code> <code>context</code> <code>dict</code> <p>The context for the image generation. Defaults to None.</p> <code>None</code> <code>literal</code> <code>bool</code> <p>Whether to disable the API's default behavior of modifying prompts. Defaults to False.</p> <code>False</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ImagesResponse</code> <p>The response from the DALLE-3 API, which includes the generated image.</p>"},{"location":"api_reference/ai/text/","title":"marvin.ai.text","text":"<p>Core LLM tools for working with text and structured data.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.Model","title":"<code>Model</code>","text":"<p>A Pydantic model that can be instantiated from a natural language string, in addition to keyword arguments.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.Model.from_text_async","title":"<code>from_text_async</code>  <code>async</code> <code>classmethod</code>","text":"<p>Class method to create an instance of the model from a natural language string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The natural language string to convert into an instance of the model.</p> required <code>instructions</code> <code>str</code> <p>Specific instructions for the conversion. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>An instance of the model.</p> Example <pre><code>from marvin.ai.text import Model\nclass Location(Model):\n    '''A location'''\n    city: str\n    state: str\n    country: str\n\nawait Location.from_text_async(\"big apple, ny, usa\")\n</code></pre>"},{"location":"api_reference/ai/text/#marvin.ai.text.caption","title":"<code>caption</code>","text":"<p>Generates a caption for an image using a language model synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Image, List[Image]]</code> <p>The image or images to caption.</p> required <code>instructions</code> <code>str</code> <p>Instructions for the caption generation.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional arguments for the language model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated caption.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.caption_async","title":"<code>caption_async</code>  <code>async</code>","text":"<p>Generates a caption for a set of images using a language model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Image, List[Image]]</code> <p>The image or images to caption.</p> required <code>instructions</code> <code>str</code> <p>Instructions for the caption generation.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional arguments for the language model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated caption.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.cast","title":"<code>cast</code>","text":"<p>Converts the input data into the specified type.</p> <p>This function uses a language model to convert the input data into a specified type. The conversion process can be guided by specific instructions. The function also supports additional arguments for the language model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>FN_INPUT_TYPES</code> <p>Union[str, Image, list[Union[str, Image]]]: the data to which the function will be applied.</p> required <code>target</code> <code>type</code> <p>The type to convert the data into. If none is provided but instructions are provided, <code>str</code> is assumed.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Specific instructions for the conversion. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>The converted data of the specified type.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.cast_async","title":"<code>cast_async</code>  <code>async</code>","text":"<p>Converts the input data into the specified type.</p> <p>This function uses a language model to convert the input data into a specified type. The conversion process can be guided by specific instructions. The function also supports additional arguments for the language model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>FN_INPUT_TYPES</code> <p>Union[str, Image, list[Union[str, Image]]]: the data to which the function will be applied.</p> required <code>target</code> <code>type</code> <p>The type to convert the data into. If none is provided but instructions are provided, <code>str</code> is assumed.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Specific instructions for the conversion. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>The converted data of the specified type.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.classifier","title":"<code>classifier</code>","text":"<p>Class decorator that modifies the behavior of an Enum class to classify a string.</p> <p>This decorator modifies the call method of the Enum class to use the <code>marvin.classify</code> function instead of the default Enum behavior. This allows the Enum class to classify a string based on its members.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Enum</code> <p>The Enum class to be decorated.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Instructions for the AI on how to perform the classification.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the model.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Enum</code> <p>The decorated Enum class with modified call method.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the decorated class is not a subclass of Enum.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.classify","title":"<code>classify</code>","text":"<p>Classifies the provided data based on the provided labels.</p> <p>This function uses a language model with a logit bias to classify the input data. The logit bias constrains the language model's response to a single token, making this function highly efficient for classification tasks. The function will always return one of the provided labels.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>FN_INPUT_TYPES</code> <p>Union[str, Image, list[Union[str, Image]]]: the data to which the function will be applied.</p> required <code>labels</code> <code>Union[Enum, list[T], type]</code> <p>The labels to classify the data into.</p> required <code>instructions</code> <code>str</code> <p>Specific instructions for the classification. Defaults to None.</p> <code>None</code> <code>return_index</code> <code>bool</code> <p>Whether to return the index of the label instead of the label itself.</p> <code>False</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[T, int]</code> <p>Union[T, int]: The label or index that the data was classified into.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.classify_async","title":"<code>classify_async</code>  <code>async</code>","text":"<p>Classifies the provided data based on the provided labels.</p> <p>This function uses a language model with a logit bias to classify the input data. The logit bias constrains the language model's response to a single token, making this function highly efficient for classification tasks. The function will always return one of the provided labels.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>FN_INPUT_TYPES</code> <p>Union[str, Image, list[Union[str, Image]]]: the data to which the function will be applied.</p> required <code>labels</code> <code>Union[Enum, list[T], type]</code> <p>The labels to classify the data into.</p> required <code>instructions</code> <code>str</code> <p>Specific instructions for the classification. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[T, int]</code> <p>Union[T, int]: The label or index that the data was classified into.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.extract","title":"<code>extract</code>","text":"<p>Extracts entities of a specific type from the provided data.</p> <p>This function uses a language model to identify and extract entities of the specified type from the input data. The extracted entities are returned as a list.</p> <p>Note that either a target type or instructions must be provided (or both). If only instructions are provided, the target type is assumed to be a string.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>FN_INPUT_TYPES</code> <p>Union[str, Image, list[Union[str, Image]]]: the data to which the function will be applied.</p> required <code>target</code> <code>type</code> <p>The type of entities to extract.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Specific instructions for the extraction. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[T]</code> <p>A list of extracted entities of the specified type.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.extract_async","title":"<code>extract_async</code>  <code>async</code>","text":"<p>Extracts entities of a specific type from the provided data.</p> <p>This function uses a language model to identify and extract entities of the specified type from the input data. The extracted entities are returned as a list.</p> <p>Note that either a target type or instructions must be provided (or both). If only instructions are provided, the target type is assumed to be a string.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>FN_INPUT_TYPES</code> <p>Union[str, Image, list[Union[str, Image]]]: the data to which the function will be applied.</p> required <code>target</code> <code>type</code> <p>The type of entities to extract.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Specific instructions for the extraction. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>MarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[T]</code> <p>A list of extracted entities of the specified type.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.fn","title":"<code>fn</code>","text":"<p>Converts a Python function into an AI function using a decorator.</p> <p>This decorator allows a Python function to be converted into an AI function. The AI function uses a language model to generate its output.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The function to be converted. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>The converted AI function.</p> Example <pre><code>@fn\ndef list_fruit(n:int) -&gt; list[str]:\n    '''generates a list of n fruit'''\n\nlist_fruit(3) # ['apple', 'banana', 'orange']\n</code></pre>"},{"location":"api_reference/ai/text/#marvin.ai.text.generate","title":"<code>generate</code>","text":"<p>Generates a list of 'n' items of the provided type or based on instructions.</p> <p>Either a type or instructions must be provided. If instructions are provided without a type, the type is assumed to be a string. The function generates at least 'n' items.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>type</code> <p>The type of items to generate. Defaults to None.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Instructions for the generation. Defaults to None.</p> <code>None</code> <code>n</code> <code>int</code> <p>The number of items to generate. Defaults to 1.</p> <code>1</code> <code>use_cache</code> <code>bool</code> <p>If True, the function will cache the last 100 responses for each (target, instructions, and temperature) and use those to avoid repetition on subsequent calls. Defaults to True.</p> <code>True</code> <code>temperature</code> <code>float</code> <p>The temperature for the generation. Defaults to 1.</p> <code>1</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[T]</code> <p>A list of generated items.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.generate_async","title":"<code>generate_async</code>  <code>async</code>","text":"<p>Generates a list of 'n' items of the provided type or based on instructions.</p> <p>Either a type or instructions must be provided. If instructions are provided without a type, the type is assumed to be a string. The function generates at least 'n' items.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>type</code> <p>The type of items to generate. Defaults to None.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Instructions for the generation. Defaults to None.</p> <code>None</code> <code>n</code> <code>int</code> <p>The number of items to generate. Defaults to 1.</p> <code>1</code> <code>use_cache</code> <code>bool</code> <p>If True, the function will cache the last 100 responses for each (target, instructions, and temperature) and use those to avoid repetition on subsequent calls. Defaults to True.</p> <code>True</code> <code>temperature</code> <code>float</code> <p>The temperature for the generation. Defaults to 1.</p> <code>1</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <code>client</code> <code>AsyncMarvinClient</code> <p>The client to use for the AI function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[T]</code> <p>A list of generated items.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.generate_llm_response","title":"<code>generate_llm_response</code>  <code>async</code>","text":"<p>Generates a language model response based on a provided prompt template.</p> <p>This function uses a language model to generate a response based on a provided prompt template. The function supports additional arguments for the prompt and the language model.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_template</code> <code>str</code> <p>The template for the prompt.</p> required <code>prompt_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the prompt. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ChatResponse</code> <code>ChatResponse</code> <p>The generated response from the language model.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.model","title":"<code>model</code>","text":"<p>Class decorator for instantiating a Pydantic model from a string.</p> <p>This decorator allows a Pydantic model to be instantiated from a string. It's equivalent to subclassing the Model class.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>Union[Type[M], None]</code> <p>The type of the Pydantic model. Defaults to None.</p> <code>None</code> <code>instructions</code> <code>str</code> <p>Specific instructions for the conversion.</p> <code>None</code> <code>model_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Type[M], Callable[[Type[M]], Type[M]]]</code> <p>Union[Type[M], Callable[[Type[M]], Type[M]]]: The decorated Pydantic model.</p>"},{"location":"api_reference/ai/text/#marvin.ai.text.prepare_data","title":"<code>prepare_data</code>","text":"<p>Prepares the input data for the AI function.</p> <p>This function prepares the input data for the AI function by converting it into a list of strings. If the input data is a list of strings or images, the function converts the images into strings.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, Image, list[Union[str, Image]]]</code> <p>The input data to be prepared.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: The prepared input data.</p>"},{"location":"api_reference/beta/applications/","title":"marvin.beta.applications","text":"<p>Tip</p> <p>All async methods that have an <code>_async</code> suffix have sync equivalents that can be called with out the suffix e.g. <code>run()</code> and <code>await run_async()</code>.</p>"},{"location":"api_reference/beta/applications/#marvin.beta.applications.Application","title":"<code>Application</code>","text":"<p>Tools for Applications have a special property: if any parameter is annotated as <code>Application</code>, then the tool will be called with the Application instance as the value for that parameter. This allows tools to access the Application's state and other properties.</p>"},{"location":"api_reference/beta/assistants/","title":"Assistants","text":""},{"location":"api_reference/beta/assistants/#marvin.beta.assistants.assistants.Assistant","title":"<code>Assistant</code>","text":"<p>The Assistant class represents an AI assistant that can be created, deleted, loaded, and interacted with.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>The unique identifier of the assistant. None if the assistant       hasn't been created yet.</p> <code>name</code> <code>str</code> <p>The name of the assistant.</p> <code>description</code> <code>str</code> <p>A description of the assistant.</p> <code>instructions</code> <code>str</code> <p>Instructions for the assistant.</p> <code>model</code> <code>str</code> <p>The model used by the assistant.</p> <code>tools</code> <code>list</code> <p>List of tools used by the assistant.</p> <code>tool_resources</code> <code>dict</code> <p>dict of tool resources associated with the assistant.</p> <code>metadata</code> <code>dict</code> <p>Additional data about the assistant.</p>"},{"location":"api_reference/beta/assistants/#marvin.beta.assistants.assistants.Assistant.chat","title":"<code>chat</code>","text":"<p>Start a chat session with the assistant.</p>"},{"location":"api_reference/beta/assistants/#marvin.beta.assistants.assistants.Assistant.chat_async","title":"<code>chat_async</code>  <code>async</code>","text":"<p>Async method to start a chat session with the assistant.</p>"},{"location":"api_reference/beta/assistants/","title":"marvin.beta.assistants","text":""},{"location":"api_reference/beta/assistants/assistants/","title":"marvin.beta.assistants.assistants","text":"<p>Tip</p> <p>All async methods that have an <code>_async</code> suffix have sync equivalents that can be called with out the suffix e.g. <code>run()</code> and <code>await run_async()</code>.</p>"},{"location":"api_reference/beta/assistants/assistants/#marvin.beta.assistants.assistants.Assistant","title":"<code>Assistant</code>","text":"<p>The Assistant class represents an AI assistant that can be created, deleted, loaded, and interacted with.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>The unique identifier of the assistant. None if the assistant       hasn't been created yet.</p> <code>name</code> <code>str</code> <p>The name of the assistant.</p> <code>description</code> <code>str</code> <p>A description of the assistant.</p> <code>instructions</code> <code>str</code> <p>Instructions for the assistant.</p> <code>model</code> <code>str</code> <p>The model used by the assistant.</p> <code>tools</code> <code>list</code> <p>List of tools used by the assistant.</p> <code>tool_resources</code> <code>dict</code> <p>dict of tool resources associated with the assistant.</p> <code>metadata</code> <code>dict</code> <p>Additional data about the assistant.</p>"},{"location":"api_reference/beta/assistants/assistants/#marvin.beta.assistants.assistants.Assistant.chat","title":"<code>chat</code>","text":"<p>Start a chat session with the assistant.</p>"},{"location":"api_reference/beta/assistants/assistants/#marvin.beta.assistants.assistants.Assistant.chat_async","title":"<code>chat_async</code>  <code>async</code>","text":"<p>Async method to start a chat session with the assistant.</p>"},{"location":"api_reference/beta/assistants/formatting/","title":"marvin.beta.assistants.formatting","text":""},{"location":"api_reference/beta/assistants/formatting/#marvin.beta.assistants.formatting.download_temp_file","title":"<code>download_temp_file</code>  <code>cached</code>","text":"<p>Downloads a file from OpenAI's servers and saves it to a temporary file.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>The ID of the file to be downloaded.</p> required <p>Returns:</p> Type Description <p>The file path of the downloaded temporary file.</p>"},{"location":"api_reference/beta/assistants/formatting/#marvin.beta.assistants.formatting.format_run","title":"<code>format_run</code>","text":"<p>Formats a run, which is an object that has both <code>.messages</code> and <code>.steps</code> attributes, each of which is a list of Messages and RunSteps.</p> <p>Parameters:</p> Name Type Description Default <code>run</code> <p>A Run object</p> required <code>include_messages</code> <code>bool</code> <p>Whether to include messages in the formatted output</p> <code>True</code> <code>include_steps</code> <code>bool</code> <p>Whether to include steps in the formatted output</p> <code>True</code>"},{"location":"api_reference/beta/assistants/formatting/#marvin.beta.assistants.formatting.format_timestamp","title":"<code>format_timestamp</code>","text":"<p>Outputs timestamp as a string in 12 hour format. Hours are left-padded with space instead of zero.</p>"},{"location":"api_reference/beta/assistants/formatting/#marvin.beta.assistants.formatting.pprint_message","title":"<code>pprint_message</code>","text":"<p>Pretty-prints a single message using the rich library, highlighting the speaker's role, the message text, any available images, and the message timestamp in a panel format.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>A message object</p> required"},{"location":"api_reference/beta/assistants/formatting/#marvin.beta.assistants.formatting.pprint_messages","title":"<code>pprint_messages</code>","text":"<p>Iterates over a list of messages and pretty-prints each one.</p> <p>Messages are pretty-printed using the rich library, highlighting the speaker's role, the message text, any available images, and the message timestamp in a panel format.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[Message]</code> <p>A list of Message objects to be printed.</p> required"},{"location":"api_reference/beta/assistants/formatting/#marvin.beta.assistants.formatting.pprint_run","title":"<code>pprint_run</code>","text":"<p>Pretty-prints a run, which is an object that has both <code>.messages</code> and <code>.steps</code> attributes, each of which is a list of Messages and RunSteps.</p> <p>Parameters:</p> Name Type Description Default <code>run</code> <p>A Run object</p> required"},{"location":"api_reference/beta/assistants/formatting/#marvin.beta.assistants.formatting.pprint_step","title":"<code>pprint_step</code>","text":"<p>Formats and prints a run step with status information.</p> <p>Parameters:</p> Name Type Description Default <code>run_step</code> <p>A RunStep object containing the details of the run step.</p> required"},{"location":"api_reference/beta/assistants/formatting/#marvin.beta.assistants.formatting.pprint_steps","title":"<code>pprint_steps</code>","text":"<p>Iterates over a list of run steps and pretty-prints each one.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>list[RunStep]</code> <p>A list of RunStep objects to be printed.</p> required"},{"location":"api_reference/beta/assistants/runs/","title":"marvin.beta.assistants.runs","text":"<p>Tip</p> <p>All async methods that have an <code>_async</code> suffix have sync equivalents that can be called with out the suffix e.g. <code>run()</code> and <code>await run_async()</code>.</p>"},{"location":"api_reference/beta/assistants/runs/#marvin.beta.assistants.runs.Run","title":"<code>Run</code>","text":"<p>The Run class represents a single execution of an assistant.</p> <p>Attributes:</p> Name Type Description <code>thread</code> <code>Thread</code> <p>The thread in which the run is executed.</p> <code>assistant</code> <code>Assistant</code> <p>The assistant that is being run.</p> <code>model</code> <code>str</code> <p>The model used by the assistant.</p> <code>instructions</code> <code>str</code> <p>Replacement instructions for the run.</p> <code>additional_instructions</code> <code>str</code> <p>Additional instructions to append                                      to the assistant's instructions.</p> <code>tools</code> <code>list[Union[AssistantTool, Callable]]</code> <p>Replacement tools                                                    for the run.</p> <code>additional_tools</code> <code>list[AssistantTool]</code> <p>Additional tools to append                                               to the assistant's tools.</p> <code>tool_choice</code> <code>Union[Literal['auto', 'none', 'required'], AssistantTool]</code> <pre><code>                                    The tool use behaviour for the run.\n</code></pre> <code>run</code> <code>Run</code> <p>The OpenAI run object.</p> <code>data</code> <code>Any</code> <p>Any additional data associated with the run.</p>"},{"location":"api_reference/beta/assistants/runs/#marvin.beta.assistants.runs.Run.cancel_async","title":"<code>cancel_async</code>  <code>async</code>","text":"<p>Cancels the run.</p>"},{"location":"api_reference/beta/assistants/runs/#marvin.beta.assistants.runs.Run.refresh_async","title":"<code>refresh_async</code>  <code>async</code>","text":"<p>Refreshes the run.</p>"},{"location":"api_reference/beta/assistants/threads/","title":"marvin.beta.assistants.threads","text":"<p>Tip</p> <p>All async methods that have an <code>_async</code> suffix have sync equivalents that can be called with out the suffix e.g. <code>run()</code> and <code>await run_async()</code>.</p>"},{"location":"api_reference/beta/assistants/threads/#marvin.beta.assistants.threads.Thread","title":"<code>Thread</code>","text":"<p>The Thread class represents a conversation thread with an assistant.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Optional[str]</code> <p>The unique identifier of the thread. None if the thread                 hasn't been created yet.</p> <code>metadata</code> <code>dict</code> <p>Additional data about the thread.</p>"},{"location":"api_reference/beta/assistants/threads/#marvin.beta.assistants.threads.Thread.add_async","title":"<code>add_async</code>  <code>async</code>","text":"<p>Add a user message to the thread.</p>"},{"location":"api_reference/beta/assistants/threads/#marvin.beta.assistants.threads.Thread.create_async","title":"<code>create_async</code>  <code>async</code>","text":"<p>Creates a thread.</p>"},{"location":"api_reference/beta/assistants/threads/#marvin.beta.assistants.threads.Thread.get_messages_async","title":"<code>get_messages_async</code>  <code>async</code>","text":"<p>Asynchronously retrieves messages from the thread.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of messages to return.</p> <code>None</code> <code>before_message</code> <code>str</code> <p>The ID of the message to start the list from, retrieving messages sent before this one.</p> <code>None</code> <code>after_message</code> <code>str</code> <p>The ID of the message to start the list from, retrieving messages sent after this one.</p> <code>None</code> <p>Returns:     list[Union[Message, dict]]: A list of messages from the thread</p>"},{"location":"api_reference/beta/assistants/threads/#marvin.beta.assistants.threads.Thread.run_async","title":"<code>run_async</code>  <code>async</code>","text":"<p>Creates and returns a <code>Run</code> of this thread with the provided assistant.</p> <p>Parameters:</p> Name Type Description Default <code>assistant</code> <code>Assistant</code> <p>The assistant to run the thread with.</p> required <code>run_kwargs</code> <p>Additional keyword arguments to pass to the Run constructor.</p> <code>{}</code>"},{"location":"api_reference/utilities/asyncio/","title":"marvin.utilities.asyncio","text":"<p>Utilities for working with asyncio.</p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.ExposeSyncMethodsMixin","title":"<code>ExposeSyncMethodsMixin</code>","text":"<p>A mixin that can take functions decorated with <code>expose_sync_method</code> and automatically create synchronous versions.</p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.create_task","title":"<code>create_task</code>","text":"<p>Creates async background tasks in a way that is safe from garbage collection.</p> <p>See https://textual.textualize.io/blog/2023/02/11/the-heisenbug-lurking-in-your-async-code/</p> <p>Example:</p> <p>async def my_coro(x: int) -&gt; int:     return x + 1</p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.create_task--safely-submits-my_coro-for-background-execution","title":"safely submits my_coro for background execution","text":"<p>create_task(my_coro(1))</p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.expose_sync_method","title":"<code>expose_sync_method</code>","text":"<p>Decorator that automatically exposes synchronous versions of async methods. Note it doesn't work with classmethods.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the synchronous method.</p> required <p>Returns:</p> Type Description <code>Callable[..., Any]</code> <p>The decorated function.</p> Example <p>Basic usage: <pre><code>class MyClass(ExposeSyncMethodsMixin):\n\n    @expose_sync_method(\"my_method\")\n    async def my_method_async(self):\n        return 42\n\nmy_instance = MyClass()\nawait my_instance.my_method_async() # returns 42\nmy_instance.my_method()  # returns 42\n</code></pre></p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.make_sync","title":"<code>make_sync</code>","text":"<p>Creates a synchronous function from an asynchronous function.</p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.run_async","title":"<code>run_async</code>  <code>async</code>","text":"<p>Runs a synchronous function in an asynchronous manner.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[..., T]</code> <p>The function to run.</p> required <code>*args</code> <code>Any</code> <p>Positional arguments to pass to the function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>The return value of the function.</p> Example <p>Basic usage: <pre><code>def my_sync_function(x: int) -&gt; int:\n    return x + 1\n\nawait run_async(my_sync_function, 1)\n</code></pre></p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.run_sync","title":"<code>run_sync</code>","text":"<p>Runs a coroutine from a synchronous context. A thread will be spawned to run the event loop if necessary, which allows coroutines to run in environments like Jupyter notebooks where the event loop runs on the main thread.</p> <p>Parameters:</p> Name Type Description Default <code>coroutine</code> <code>Coroutine[Any, Any, T]</code> <p>The coroutine to run.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The return value of the coroutine.</p> Example <p>Basic usage: <pre><code>async def my_async_function(x: int) -&gt; int:\n    return x + 1\n\nrun_sync(my_async_function(1))\n</code></pre></p>"},{"location":"api_reference/utilities/asyncio/#marvin.utilities.asyncio.run_sync_if_awaitable","title":"<code>run_sync_if_awaitable</code>","text":"<p>If the object is awaitable, run it synchronously. Otherwise, return the object.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The object to run.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The return value of the object if it is awaitable, otherwise the object</p> <code>Any</code> <p>itself.</p> Example <p>Basic usage: <pre><code>async def my_async_function(x: int) -&gt; int:\n    return x + 1\n\nrun_sync_if_awaitable(my_async_function(1))\n</code></pre></p>"},{"location":"api_reference/utilities/context/","title":"marvin.utilities.context","text":"<p>Module for defining context utilities.</p>"},{"location":"api_reference/utilities/context/#marvin.utilities.context.ScopedContext","title":"<code>ScopedContext</code>","text":"<p><code>ScopedContext</code> provides a context management mechanism using <code>contextvars</code>.</p> <p>This class allows setting and retrieving key-value pairs in a scoped context, which is preserved across asynchronous tasks and threads within the same context.</p> <p>Attributes:</p> Name Type Description <code>_context_storage</code> <code>ContextVar</code> <p>A context variable to store the context data.</p> Example <p>Basic Usage of ScopedContext <pre><code>context = ScopedContext()\nwith context(key=\"value\"):\n    assert context.get(\"key\") == \"value\"\n# Outside the context, the value is no longer available.\nassert context.get(\"key\") is None\n</code></pre></p>"},{"location":"api_reference/utilities/images/","title":"marvin.utilities.images","text":""},{"location":"api_reference/utilities/images/#marvin.utilities.images.base64_to_image","title":"<code>base64_to_image</code>","text":"<p>Converts a base64 string to a local image file.</p> <p>Parameters:</p> Name Type Description Default <code>base64_str</code> <code>str</code> <p>The base64 string representation of the image.</p> required <code>output_path</code> <code>Union[str, Path]</code> <p>The path to the output image file. This can be a string or a Path object.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api_reference/utilities/images/#marvin.utilities.images.image_to_base64","title":"<code>image_to_base64</code>","text":"<p>Converts a local image file to a base64 string.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>Union[str, Path]</code> <p>The path to the image file. This can be a string or a Path object.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The base64 representation of the image.</p>"},{"location":"api_reference/utilities/jinja/","title":"marvin.utilities.jinja","text":"<p>Module for Jinja utilities.</p>"},{"location":"api_reference/utilities/jinja/#marvin.utilities.jinja.BaseEnvironment","title":"<code>BaseEnvironment</code>","text":"<p>BaseEnvironment provides a configurable environment for rendering Jinja templates.</p> <p>This class encapsulates a Jinja environment with customizable global functions and template settings, allowing for flexible template rendering.</p> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <p>The Jinja environment for template rendering.</p> <code>globals</code> <code>dict[str, Any]</code> <p>A dictionary of global functions and variables available in templates.</p> Example <p>Basic Usage of BaseEnvironment <pre><code>env = BaseEnvironment()\n\nrendered = env.render(\"Hello, {{ name }}!\", name=\"World\")\nprint(rendered)  # Output: Hello, World!\n</code></pre></p>"},{"location":"api_reference/utilities/jinja/#marvin.utilities.jinja.BaseEnvironment.render","title":"<code>render</code>","text":"<p>Renders a given template <code>str</code> or <code>BaseTemplate</code> with provided context.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>Union[str, Template]</code> <p>The template to be rendered.</p> required <code>**kwargs</code> <code>Any</code> <p>Context variables to be passed to the template.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The rendered template as a string.</p> Example <p>Basic Usage of <code>BaseEnvironment.render</code> <pre><code>from marvin.utilities.jinja import Environment as jinja_env\n\nrendered = jinja_env.render(\"Hello, {{ name }}!\", name=\"World\")\nprint(rendered) # Output: Hello, World!\n</code></pre></p>"},{"location":"api_reference/utilities/jinja/#marvin.utilities.jinja.Transcript","title":"<code>Transcript</code>","text":"<pre><code>A Transcript is a model that represents a conversation involving multiple\nroles as a single string. It can be parsed into discrete JSON messages.\n\nTranscripts contain special tokens that indicate how to split the transcript\ninto discrete messages.\n\nThe first special token type indicates the message `role`. Default roles are\n`|SYSTEM|`, `|HUMAN|`, `|USER|`, and `|ASSISTANT|`. When these tokens appear\nat the start of a newline, all text following the token until the next\nnewline or token is considered part of the message with the given role.\n\nThe second special token type indicates the message `type`. By default, messages all have the `text` type. By supplying a token like `|IMAGE|`, you can indicate that a portion of the message is an image. Use `|TEXT|` to end the image portion and return to text. An\n\nAttributes:\n    content: The content of the transcript.\n    roles: The roles involved in the transcript.\n    environment: The jinja environment to use for rendering the transcript.\n\nExample:\n    Basic Usage of Transcript:\n    ```python\n    from marvin.utilities.jinja import Transcript\n\n    transcript = Transcript(\n        content=\"|SYSTEM| Hello, there!\n</code></pre> <p>|USER| Hello, yourself!\",             roles={\"|SYSTEM|\": \"system\", \"|USER|\": \"user\"},         )         print(transcript.render_to_messages())         # [         #   BaseMessage(content='system: Hello, there!', role='system'),         #   BaseMessage(content='Hello, yourself!', role='user')         # ]         ```</p>"},{"location":"api_reference/utilities/jinja/#marvin.utilities.jinja.split_text_by_tokens","title":"<code>split_text_by_tokens</code>","text":"<p>Splits a given text by a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be split. split_tokens: The tokens to split the text</p> required <code>by.</code> <code>only_on_newline</code> <p>If True, only match tokens that are either</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str]]</code> <p>A list of tuples containing the token and the text following it.</p> Example <p>Basic Usage of <code>split_text_by_tokens</code> ```python from marvin.utilities.jinja import split_text_by_tokens</p> <p>text = \"Hello, World!\" split_tokens = [\"Hello\", \"World\"] pairs = split_text_by_tokens(text, split_tokens) print(pairs) # Output: [(\"Hello\", \", \"), (\"World\", \"!\")] ```</p>"},{"location":"api_reference/utilities/logging/","title":"marvin.utilities.logging","text":"<p>Module for logging utilities.</p>"},{"location":"api_reference/utilities/logging/#marvin.utilities.logging.get_logger","title":"<code>get_logger</code>  <code>cached</code>","text":"<p>Retrieves a logger with the given name, or the root logger if no name is given.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name of the logger to retrieve.</p> <code>None</code> <p>Returns:</p> Type Description <code>Logger</code> <p>The logger with the given name, or the root logger if no name is given.</p> Example <p>Basic Usage of <code>get_logger</code> <pre><code>from marvin.utilities.logging import get_logger\n\nlogger = get_logger(\"marvin.test\")\nlogger.info(\"This is a test\") # Output: marvin.test: This is a test\n\ndebug_logger = get_logger(\"marvin.debug\")\ndebug_logger.debug_kv(\"TITLE\", \"log message\", \"green\")\n</code></pre></p>"},{"location":"api_reference/utilities/openai/","title":"marvin.utilities.openai","text":"<p>Utilities for working with OpenAI.</p>"},{"location":"api_reference/utilities/openai/#marvin.utilities.openai.get_openai_client","title":"<code>get_openai_client</code>","text":"<p>Retrieves an OpenAI client (sync or async) based on the current configuration.</p> <p>Returns:</p> Type Description <code>Union[AsyncClient, Client, AzureOpenAI, AsyncAzureOpenAI]</code> <p>The OpenAI client</p> Example <p>Retrieving an OpenAI client <pre><code>from marvin.utilities.openai import get_client\n\nclient = get_client()\n</code></pre></p>"},{"location":"api_reference/utilities/pydantic/","title":"marvin.utilities.pydantic","text":"<p>Module for Pydantic utilities.</p>"},{"location":"api_reference/utilities/pydantic/#marvin.utilities.pydantic.cast_to_model","title":"<code>cast_to_model</code>","text":"<p>Casts a type or callable to a Pydantic model.</p> <p>Parameters:</p> Name Type Description Default <code>function_or_type</code> <code>Union[type, type[BaseModel], GenericAlias, Callable[..., Any]]</code> <p>The type or callable to cast to a Pydantic model.</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the model to create.</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>The description of the model to create.</p> <code>None</code> <code>field_name</code> <code>Optional[str]</code> <p>The name of the field to create.</p> <code>None</code> <p>Returns:</p> Type Description <code>type[BaseModel]</code> <p>The Pydantic model created from the given type or callable.</p> Example <p>Basic Usage of <code>cast_to_model</code> <pre><code>from marvin.utilities.pydantic import cast_to_model\nfrom pydantic import BaseModel\n\ndef foo(bar: str) -&gt; str:\n    return bar\n\n# cast a function to a model\nmodel = cast_to_model(foo, name=\"Foo\")\nassert issubclass(model, BaseModel)\n</code></pre></p>"},{"location":"api_reference/utilities/pydantic/#marvin.utilities.pydantic.parse_as","title":"<code>parse_as</code>","text":"<p>Parse a given data structure as a Pydantic model via <code>TypeAdapter</code>.</p> <p>Read more about <code>TypeAdapter</code> here.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>type[T]</code> <p>The type to parse the data as.</p> required <code>data</code> <code>Any</code> <p>The data to be parsed.</p> required <code>mode</code> <code>Literal['python', 'json', 'strings']</code> <p>The mode to use for parsing, either <code>python</code>, <code>json</code>, or <code>strings</code>. Defaults to <code>python</code>, where <code>data</code> should be a Python object (e.g. <code>dict</code>).</p> <code>'python'</code> <p>Returns:</p> Type Description <code>T</code> <p>The parsed <code>data</code> as the given <code>type_</code>.</p> Example <p>Basic Usage of <code>parse_as</code> <pre><code>from marvin.utilities.pydantic import parse_as\nfrom pydantic import BaseModel\n\nclass ExampleModel(BaseModel):\n    name: str\n\n# parsing python objects\nparsed = parse_as(ExampleModel, {\"name\": \"Marvin\"})\nassert isinstance(parsed, ExampleModel)\nassert parsed.name == \"Marvin\"\n\n# parsing json strings\nparsed = parse_as(\n    list[ExampleModel],\n    '[{\"name\": \"Marvin\"}, {\"name\": \"Arthur\"}]',\n    mode=\"json\"\n)\nassert all(isinstance(item, ExampleModel) for item in parsed)\nassert parsed[0].name == \"Marvin\"\nassert parsed[1].name == \"Arthur\"\n\n# parsing raw strings\nparsed = parse_as(int, '123', mode=\"strings\")\nassert isinstance(parsed, int)\nassert parsed == 123\n</code></pre></p>"},{"location":"api_reference/utilities/python/","title":"marvin.utilities.python","text":""},{"location":"api_reference/utilities/python/#marvin.utilities.python.PythonFunction","title":"<code>PythonFunction</code>","text":"<p>A Pydantic model representing a Python function.</p> <p>Attributes:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The original function object.</p> <code>signature</code> <code>Signature</code> <p>The signature object of the function.</p> <code>name</code> <code>str</code> <p>The name of the function.</p> <code>docstring</code> <code>Optional[str]</code> <p>The docstring of the function.</p> <code>parameters</code> <code>List[ParameterModel]</code> <p>The parameters of the function.</p> <code>return_annotation</code> <code>Optional[Any]</code> <p>The return annotation of the function.</p> <code>source_code</code> <code>str</code> <p>The source code of the function.</p> <code>bound_parameters</code> <code>dict[str, Any]</code> <p>The parameters of the function bound with values.</p> <code>return_value</code> <code>Optional[Any]</code> <p>The return value of the function call.</p>"},{"location":"api_reference/utilities/python/#marvin.utilities.python.PythonFunction.from_function","title":"<code>from_function</code>  <code>classmethod</code>","text":"<p>Create a PythonFunction instance from a function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The function to create a PythonFunction instance from.</p> required <code>**kwargs</code> <p>Additional keyword arguments to set as attributes on the PythonFunction instance.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>PythonFunction</code> <code>PythonFunction</code> <p>The created PythonFunction instance.</p>"},{"location":"api_reference/utilities/python/#marvin.utilities.python.PythonFunction.from_function_call","title":"<code>from_function_call</code>  <code>classmethod</code>","text":"<p>Create a PythonFunction instance from a function call.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The function to call.</p> required <code>*args</code> <p>Positional arguments to pass to the function call.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to the function call.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>PythonFunction</code> <code>PythonFunction</code> <p>The created PythonFunction instance, with the return value of the function call set as an attribute.</p>"},{"location":"api_reference/utilities/slack/","title":"Slack","text":"<p>Module for Slack-related utilities.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.edit_slack_message","title":"<code>edit_slack_message</code>  <code>async</code>","text":"<p>Edit an existing Slack message by appending new text or replacing it.</p> <p>Parameters:</p> Name Type Description Default <code>channel</code> <code>str</code> <p>The Slack channel ID.</p> required <code>ts</code> <code>str</code> <p>The timestamp of the message to edit.</p> required <code>new_text</code> <code>str</code> <p>The new text to append or replace in the message.</p> required <code>mode</code> <code>str</code> <p>The mode of text editing, 'append' (default) or 'replace'.</p> <code>'append'</code> <p>Returns:</p> Type Description <code>Response</code> <p>httpx.Response: The response from the Slack API.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.fetch_current_message_text","title":"<code>fetch_current_message_text</code>  <code>async</code>","text":"<p>Fetch the current text of a specific Slack message using its timestamp.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.get_thread_messages","title":"<code>get_thread_messages</code>  <code>async</code>","text":"<p>Get all messages from a slack thread.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.get_token","title":"<code>get_token</code>  <code>async</code>","text":"<p>Get the Slack bot token from the environment.</p>"},{"location":"api_reference/utilities/slack/#marvin.utilities.slack.search_slack_messages","title":"<code>search_slack_messages</code>  <code>async</code>","text":"<p>Search for messages in Slack workspace based on a query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query.</p> required <code>max_messages</code> <code>int</code> <p>The maximum number of messages to retrieve.</p> <code>3</code> <code>channel</code> <code>str</code> <p>The specific channel to search in. Defaults to None, which searches all channels.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of message contents and permalinks matching the query.</p>"},{"location":"api_reference/utilities/strings/","title":"marvin.utilities.strings","text":"<p>Module for string utilities.</p>"},{"location":"api_reference/utilities/strings/#marvin.utilities.strings.count_tokens","title":"<code>count_tokens</code>","text":"<p>Counts the number of tokens in the given text using the specified model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to count tokens in.</p> required <code>model</code> <code>str</code> <p>The model to use for token counting. If not provided,                    the default model is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of tokens in the text.</p>"},{"location":"api_reference/utilities/strings/#marvin.utilities.strings.detokenize","title":"<code>detokenize</code>","text":"<p>Detokenizes the given tokens using the specified model.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list[int]</code> <p>The tokens to detokenize.</p> required <code>model</code> <code>str</code> <p>The model to use for detokenization. If not provided,                    the default model is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The detokenized text.</p>"},{"location":"api_reference/utilities/strings/#marvin.utilities.strings.slice_tokens","title":"<code>slice_tokens</code>","text":"<p>Slices the given text to the specified number of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to slice.</p> required <code>n_tokens</code> <code>int</code> <p>The number of tokens to slice the text to.</p> required <code>model</code> <code>str</code> <p>The model to use for token counting. If not provided,                    the default model is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The sliced text.</p>"},{"location":"api_reference/utilities/strings/#marvin.utilities.strings.tokenize","title":"<code>tokenize</code>","text":"<p>Tokenizes the given text using the specified model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to tokenize.</p> required <code>model</code> <code>str</code> <p>The model to use for tokenization. If not provided,                    the default model is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>list[int]: The tokenized text as a list of integers.</p>"},{"location":"api_reference/utilities/testing/","title":"marvin.utilities.testing","text":"<p>Utilities for running unit tests.</p>"},{"location":"api_reference/utilities/testing/#marvin.utilities.testing.assert_equal","title":"<code>assert_equal</code>","text":"<p>Asserts whether the LLM output meets the expected output.</p> <p>This function uses an LLM to assess whether the provided output (llm_output) meets some expectation. It allows us to make semantic claims like \"the output is a list of first names\" to make assertions about stochastic LLM outputs.</p> <p>Parameters:</p> Name Type Description Default <code>llm_output</code> <code>Any</code> <p>The output from the LLM.</p> required <code>expected</code> <code>Any</code> <p>The expected output.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the LLM output meets the expectation, False otherwise.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the LLM output does not meet the expectation.</p>"},{"location":"api_reference/utilities/testing/#marvin.utilities.testing.assert_locations_equal","title":"<code>assert_locations_equal</code>","text":"<p>Helpful LLM assert for comparing two locations (e.g. New York, New York City)</p>"},{"location":"api_reference/utilities/tools/","title":"marvin.utilities.tools","text":"<p>Module for LLM tool utilities.</p>"},{"location":"api_reference/utilities/tools/#marvin.utilities.tools.call_function_tool","title":"<code>call_function_tool</code>","text":"<p>Helper function for calling a function tool from a list of tools, using the arguments provided by an LLM as a JSON string. This function handles many common errors.</p>"},{"location":"api_reference/utilities/tools/#marvin.utilities.tools.custom_partial","title":"<code>custom_partial</code>","text":"<p>Returns a new function with partial application of the given keyword arguments. The new function has the same name and docstring as the original, and its signature excludes the provided kwargs.</p>"},{"location":"api_reference/utilities/tools/#marvin.utilities.tools.output_to_string","title":"<code>output_to_string</code>","text":"<p>Function outputs must be provided as strings</p>"},{"location":"api_reference/utilities/tools/#marvin.utilities.tools.tool_from_function","title":"<code>tool_from_function</code>","text":"<p>Creates an OpenAI-compatible tool from a Python function.</p> <p>If any kwargs are provided, they will be stored and provided at runtime. Provided kwargs will be removed from the tool's parameter schema.</p>"},{"location":"api_reference/utilities/tools/#marvin.utilities.tools.tool_from_model","title":"<code>tool_from_model</code>","text":"<p>Creates an OpenAI-compatible tool from a Pydantic model class.</p>"},{"location":"api_reference/utilities/tools/#marvin.utilities.tools.tool_from_type","title":"<code>tool_from_type</code>","text":"<p>Creates an OpenAI-compatible tool from a Python type.</p>"},{"location":"community/","title":"The Marvin Community","text":"<p>We're thrilled you're interested in Marvin! Here, we're all about community. Marvin isn't just a tool, it's a platform for developers to collaborate, learn, and grow. We're driven by a shared passion for making Large Language Models (LLMs) more accessible and easier to use.</p>"},{"location":"community/#connect-on-discord","title":"Connect on Discord","text":"<p>The heart of our community beats in our Discord server. It's a space where you can ask questions, share ideas, or just chat with like-minded developers. Don't be shy, join us on Discord</p>"},{"location":"community/#contributing-to-marvin","title":"Contributing to Marvin","text":"<p>Remember, Marvin is your tool. We want you to feel at home suggesting changes, requesting new features, and reporting bugs. Here's how you can contribute:</p> <ul> <li> <p>Issues: Encountered a bug? Have a suggestion? Open an issue in our GitHub repository. We appreciate your input!</p> </li> <li> <p>Pull Requests (PRs): Ready to contribute code? We welcome your pull requests! Not sure how to make a PR? Check out the GitHub guide.</p> </li> <li> <p>Discord Discussions: Have an idea but not quite ready to open an issue or PR? Discuss it with us on Discord first!</p> </li> </ul> <p>Remember, every contribution, no matter how small, is valuable. Don't worry about not being an expert or making mistakes. We're here to learn and grow together. Your input helps Marvin become better for everyone.</p> <p>Stay tuned for community events and more ways to get involved. Marvin is more than a project \u2013 it's a community. And we're excited for you to be a part of it!</p>"},{"location":"community/development_guide/","title":"Development Guide","text":""},{"location":"community/development_guide/#prerequisites","title":"Prerequisites","text":"<p>Marvin requires Python 3.9+.</p>"},{"location":"community/development_guide/#installation","title":"Installation","text":"<p>Clone a fork of the repository and install the dependencies: <pre><code>git clone https://github.com/youFancyUserYou/marvin.git\ncd marvin\n</code></pre></p> <p>Activate a virtual environment: <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre></p> <p>Install the dependencies in editable mode: <pre><code>pip install -e \".[dev]\"\n</code></pre></p> <p>Install the pre-commit hooks: <pre><code>pre-commit install\n</code></pre></p>"},{"location":"community/development_guide/#testing","title":"Testing","text":"<p>Run the tests that don't require an LLM: <pre><code>pytest -vv -m no_llm\n</code></pre></p> <p>Run all tests: <pre><code>pytest -vv\n</code></pre></p>"},{"location":"community/development_guide/#opening-a-pull-request","title":"Opening a Pull Request","text":"<p>Fork the repository and create a new branch: <pre><code>git checkout -b my-branch\n</code></pre></p> <p>Make your changes and commit them: <pre><code>git add . &amp;&amp; git commit -m \"My changes\"\n</code></pre></p> <p>Push your changes to your fork: <pre><code>git push origin my-branch\n</code></pre></p> <p>Open a pull request on GitHub - ping us on Discord if you need help!</p>"},{"location":"community/feedback/","title":"Feedback \ud83d\udc99","text":"<p>We've been humbled and energized by the positive community response to Marvin.</p> <p>Tired: write comments to prompt copilot to write code.Wired: just write comments. it's cleaner :D https://t.co/FOA26lR9xN</p>\u2014 Andrej Karpathy (@karpathy) March 30, 2023 <p>Ok, I admit, I\u2019m getting more and more hyped about @AskMarvinAI. Some of these new functions are pretty legit looking. https://t.co/xhCCKp5kU5</p>\u2014 Chris Riccomini \ud83c\udfd6\ufe0f (@criccomini) April 21, 2023 <p>even the way ai_model uses ai_fn is chefs kiss, truely a craftsmanhttps://t.co/GcmWDeEVJSThey have spinner text.</p>\u2014 jason (@jxnlco) May 12, 2023 <p>The library is open-source: @AskMarvinAI, by @jlowin`@ai_model` is not the only magic Python decorator. There is also `@ai_fn` that makes any function an ambient LLM processor.https://t.co/ZXElyA0Ihp</p>\u2014 Jim Fan (@DrJimFan) May 14, 2023 <p>This is f**king cool. https://t.co/4PH6VAZPYo</p>\u2014 Pydantic (@pydantic) May 14, 2023 <p>Pretty slick\u2026 get Pydantic models from a string of Text. https://t.co/EnnQkzl4Ay</p>\u2014 Chris Riccomini \ud83c\udfd6\ufe0f (@criccomini) May 12, 2023 <p>Uhh how are people not talking about @AskMarvinAI more? @ai_fn is \ud83e\udd2f</p>\u2014 Rushabh Doshi (@radoshi) May 18, 2023"},{"location":"docs/audio/recording/","title":"Recording audio","text":"<p>Marvin has utilities for working with audio data beyond generating speech and transcription. To use these utilities, you must install Marvin with the <code>audio</code> extra:</p> <pre><code>pip install marvin[audio]\n</code></pre>"},{"location":"docs/audio/recording/#audio-objects","title":"Audio objects","text":"<p>The <code>Audio</code> object gives users a simple way to work with audio data that is compatible with all of Marvin's audio abilities. You can create an <code>Audio</code> object from a file path or by providing audio bytes directly.</p>"},{"location":"docs/audio/recording/#from-a-file-path","title":"From a file path","text":"<pre><code>from marvin.audio import Audio\naudio = Audio.from_path(\"fancy_computer.mp3\")\n</code></pre>"},{"location":"docs/audio/recording/#from-data","title":"From data","text":"<pre><code>audio = Audio(data=audio_bytes)\n</code></pre>"},{"location":"docs/audio/recording/#playing-audio","title":"Playing audio","text":"<p>You can play audio from an <code>Audio</code> object using the <code>play</code> method:</p> <pre><code>audio.play()\n</code></pre>"},{"location":"docs/audio/recording/#recording-audio_1","title":"Recording audio","text":"<p>Marvin can record audio from your computer's microphone. There are a variety of options for recording audio in order to match your specific use case. </p>"},{"location":"docs/audio/recording/#recording-for-a-set-duration","title":"Recording for a set duration","text":"<p>The basic <code>record</code> function records audio for a specified duration. The duration is provided in seconds.</p> <pre><code>import marvin.audio\n\n# record 5 seconds of audio\naudio = marvin.audio.record(duration=5)\naudio.play()\n</code></pre>"},{"location":"docs/audio/recording/#recording-a-phrase","title":"Recording a phrase","text":"<p>The <code>record_phrase</code> function records audio until a pause is detected. This is useful for recording a phrase or sentence.</p> <pre><code>import marvin.audio\n\naudio = marvin.audio.record_phrase()\naudio.play()\n</code></pre> <p>There are a few keyword arguments that can be used to customize the behavior of <code>record_phrase</code>: - <code>after_phrase_silence</code>: The duration of silence to consider the end of a phrase. The default is 0.8 seconds. - <code>timeout</code>: The maximum time to wait for speech to start before giving up. The default is no timeout. - <code>max_phrase_duration</code>: The maximum duration for recording a phrase. The default is no limit. - <code>adjust_for_ambient_noise</code>: Whether to adjust the recognizer sensitivity to ambient noise before starting recording. The default is <code>True</code>, but note that this introduces a minor latency between the time the function is called and the time recording starts. A log message will be printed to indicate when the calibration is complete.</p>"},{"location":"docs/audio/recording/#recording-continuously","title":"Recording continuously","text":"<p>The <code>record_background</code> function records audio continuously in the background. This is useful for recording audio while doing other tasks or processing audio in real time.</p> <p>The result of <code>record_background</code> is a <code>BackgroundAudioRecorder</code> object, which can be used to control the recording (including stopping it) and to access the recorded audio as a stream.</p> <p>By default, the audio is recorded as a series of phrases, meaning a new <code>Audio</code> object is created each time a phase is detected. Audio objects are queued and can be accessed by iterating over the recorder's <code>stream</code> method.</p> <pre><code>import marvin\nimport marvin.audio\n\nrecorder = marvin.audio.record_background()\n\ncounter = 0\nfor audio in recorder.stream():\n    counter += 1\n    # process each audio phrase\n    marvin.transcribe(audio)\n\n    # stop recording\n    if counter == 3:\n        recorder.stop()\n</code></pre>"},{"location":"docs/audio/speech/","title":"Generating speech","text":"<p>Marvin can generate speech from text. </p> <p>What it does</p> <p>     The <code>speak</code> function generates audio from text. The <code>@speech</code> decorator generates speech from the output of a function.   </p> <p>Example</p> From a stringFrom a function <p>The easiest way to generate speech is to provide a string:</p> <pre><code>import marvin\n\naudio = marvin.speak(\"I sure like being inside this fancy computer!\")\n</code></pre> <p>Result</p> <p><pre><code>audio.play(\"fancy_computer.mp3\")\n</code></pre>    Your browser does not support the audio element. </p> <p>For more complex use cases, you can use the <code>@image</code> decorator to generate images from the output of a function:</p> <pre><code>@marvin.speech\ndef say_hello(name: str):\n    return f'Hello, {name}! How are you doing today?'\n\n\naudio = say_hello(\"Arthur\")\n</code></pre> <p>Result</p> <p><pre><code>audio.play(\"hello_arthur.mp3\")\n</code></pre>    Your browser does not support the audio element. </p> <p>How it works</p> <p>     Marvin passes your prompt to the OpenAI speech API, which returns an audio file.   </p> <p>Text is generated verbatim</p> <p>Unlike the images API, OpenAI's speech API does not modify or revise your input prompt in any way. Whatever text you provide is exactly what will be spoken. </p> <p>Therefore, you can use the <code>speak</code> function to generate speech from any string, or use the <code>@speech</code> decorator to generate speech from the string output of any function.</p>"},{"location":"docs/audio/speech/#generating-speech_1","title":"Generating speech","text":"<p>By default, OpenAI generates speech from the text you provide, verbatim. We can use Marvin functions to generate more interesting speech by modifying the prompt before passing it to the speech API. For example, we can use a function to generate a line of dialogue that reflects a specific intent. And because of Marvin's modular design, we can simply add a <code>@speech</code> decorator to the function to generate speech from its output.</p> <pre><code>import marvin\n\n@marvin.speech\n@marvin.fn\ndef ai_say(intent: str) -&gt; str:\n    '''\n    Given an `intent`, generate a line of diagogue that \n    reflects the intent / tone / instruction without repeating \n    it verbatim.\n    '''\n\nai_say('hello') \n# Hi there! Nice to meet you.\n</code></pre> <p>Result</p> <p>    Your browser does not support the audio element. </p>"},{"location":"docs/audio/speech/#playing-audio","title":"Playing audio","text":"<p>The result of <code>speak</code> and <code>@speech</code> is an <code>Audio</code> object that can be played by calling its <code>play</code> method. By default, playback will start as soon as the first bytes of audio are available. See the note on streaming audio for more information.</p> <pre><code>audio = marvin.speak(\"Hello, world!\")\naudio.play()\n</code></pre>"},{"location":"docs/audio/speech/#streaming-audio","title":"Streaming audio","text":"<p>By default, Marvin streams audio from the OpenAI API, which means that playback can start as soon as the first bytes of audio are available. This can be useful for long audio files, as it allows you to start listening to the audio before it has finished generating. If you want to wait for the entire audio file to be generated before starting playback, you can pass <code>stream=False</code>:</p> <p><pre><code>audio = marvin.speak(\"Hello, world!\", stream=False)\n</code></pre> Note that streaming is only supported with the <code>pcm</code> (or raw) audio file format, and an error will be raised if you try to generate speech in a different format with <code>stream=True</code>. However, you can always save <code>pcm</code> audio to a file in a different format after it has been generated.</p>"},{"location":"docs/audio/speech/#saving-audio","title":"Saving audio","text":"<p>To save an <code>Audio</code> object to a file, you can call its <code>save</code> method:</p> <pre><code>audio = marvin.speak(\"Hello, world!\")\naudio.save(\"hello_world.mp3\")\n</code></pre> <p>Marvin will attempt to infer the correct file format from the file extension you provide. If you want to save the audio in a different format, you can pass a <code>format</code> argument to <code>save</code>.</p>"},{"location":"docs/audio/speech/#saving-audio_1","title":"Saving audio","text":""},{"location":"docs/audio/speech/#choosing-a-voice","title":"Choosing a voice","text":"<p>Both <code>speak</code> and <code>@speech</code> accept a <code>voice</code> parameter that allows you to choose from a variety of voices. You can preview the available voices here.</p> <pre><code>The result of the `speak` function and `@speech` decorator is an audio stream.\n\naudio = marvin.speak(\"Hello, world!\", voice=\"nova\")\naudio.play(\"hello_world.mp3\") \n</code></pre>"},{"location":"docs/audio/speech/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> arguments of <code>speak</code> and <code>@speech</code>. These parameters are passed directly to the respective APIs, so you can use any supported parameter.</p>"},{"location":"docs/audio/speech/#async-support","title":"Async support","text":"<p>If you are using Marvin in an async environment, you can use <code>speak_async</code> (or decorate an async function with <code>@speech</code>) to generate speech asynchronously:</p> <pre><code>result = await marvin.speak_async('I sure like being inside this fancy computer!')\n</code></pre>"},{"location":"docs/audio/transcription/","title":"Generating transcriptions","text":"<p>Marvin can generate text from speech. </p> <p>What it does</p> <p>     The <code>transcribe</code> function generates text from audio.   </p> <p>Example</p> <p>Suppose you have the following audio saved as <code>fancy_computer.mp3</code>:</p> <p>    Your browser does not support the audio element. </p> <p>To generate a transcription, provide the path to the file:</p> <pre><code>import marvin\n\ntranscription = marvin.transcribe(\"fancy_computer.mp3\")\n</code></pre> <p>Result</p> <pre><code>assert transcription == \"I sure like being inside this fancy computer.\"\n</code></pre> <p>How it works</p> <p>     Marvin passes your file to the OpenAI transcription API, which returns an transcript.   </p>"},{"location":"docs/audio/transcription/#supported-audio-formats","title":"Supported audio formats","text":"<p>You can provide audio data to <code>transcribe</code> in a variety of ways. Marvin supports the following encodings: flac, m4a, mp3, mp4, mpeg, mpga, oga, ogg, wav, and webm.</p>"},{"location":"docs/audio/transcription/#marvin-audio-object","title":"Marvin <code>Audio</code> object","text":"<p>Marvin provides an <code>Audio</code> object that makes it easier to work with audio. Typically it is imported from the <code>marvin.audio</code> module, which requires the <code>audio</code> extra to be installed. If it isn't installed, you can still import the <code>Audio</code> object from <code>marvin.types</code>, though some additional functionality will not be available.</p> <pre><code>from marvin.audio import Audio\n# or, if the audio extra is not installed:\n# from marvin.types import Audio\n\naudio = Audio.from_path(\"fancy_computer.mp3\")\ntranscription = marvin.transcribe(audio)\n</code></pre>"},{"location":"docs/audio/transcription/#path-to-a-local-file","title":"Path to a local file","text":"<p>Provide a string or <code>Path</code> representing the path to a local audio file:</p> <pre><code>marvin.transcribe(\"fancy_computer.mp3\")\n</code></pre>"},{"location":"docs/audio/transcription/#file-reference","title":"File reference","text":"<p>Provide the audio data as an in-memory file object:</p> <pre><code>with open(\"/path/to/audio.mp3\", \"rb\") as f:\n    marvin.transcribe(f)\n</code></pre>"},{"location":"docs/audio/transcription/#raw-bytes","title":"Raw bytes","text":"<p>Provide the audio data as raw bytes:</p> <pre><code>marvin.transcribe(audio_bytes)\n</code></pre> <p>Note that the OpenAI transcription API requires a filename, so Marvin will supply <code>audio.mp3</code> if  you pass raw bytes. In practice, this doesn't appear to make a difference even if your audio is not an mp3 file (e.g. a wav file).</p>"},{"location":"docs/audio/transcription/#async-support","title":"Async support","text":"<p>If you are using Marvin in an async environment, you can use <code>transcribe_async</code>:</p> <pre><code>result = await marvin.transcribe_async('fancy_computer.mp3')\nassert result == \"I sure like being inside this fancy computer.\"\n</code></pre>"},{"location":"docs/audio/transcription/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> argument. These parameters are passed directly to the respective APIs, so you can use any supported parameter.</p>"},{"location":"docs/configuration/settings/","title":"Settings","text":"<p>Marvin makes use of Pydantic's <code>BaseSettings</code> to configure, load, and change behavior.</p>"},{"location":"docs/configuration/settings/#environment-variables","title":"Environment Variables","text":"<p>All settings are configurable via environment variables like <code>MARVIN_&lt;setting name&gt;</code>.</p> <p>Please set Marvin specific settings in <code>~/.marvin/.env</code>. One exception being <code>OPENAI_API_KEY</code>, which may be as a global env var on your system and it will be picked up by Marvin.</p> <p>Setting Environment Variables</p> <p>For example, in your <code>~/.marvin/.env</code> file you could have: <pre><code>MARVIN_LOG_LEVEL=INFO\nMARVIN_CHAT_COMPLETIONS_MODEL=gpt-4o\nMARVIN_OPENAI_API_KEY='sk-my-api-key'\n</code></pre> Settings these values will let you avoid setting an API key every time. </p>"},{"location":"docs/configuration/settings/#runtime-settings","title":"Runtime Settings","text":"<p>A runtime settings object is accessible via <code>marvin.settings</code> and can be used to access or update settings throughout the package.</p> <p>Mutating settings at runtime</p> <p>For example, to access or change the LLM model used by Marvin at runtime: <pre><code>import marvin\n\nmarvin.settings.openai.chat.completions.model = 'gpt-4o'\n</code></pre></p>"},{"location":"docs/configuration/settings/#settings-for-using-azure-openai-models","title":"Settings for using Azure OpenAI models","text":"<p>Some of Marvin's functionality is supported by Azure OpenAI services.</p> <p>After setting up your Azure OpenAI account and deployment, set these environment variables in your environment, <code>~/.marvin/.env</code>, or <code>.env</code> file:</p> <pre><code>MARVIN_PROVIDER=azure_openai\nMARVIN_AZURE_OPENAI_API_KEY=&lt;your-api-key&gt;\nMARVIN_AZURE_OPENAI_ENDPOINT=\"https://&lt;your-endpoint&gt;.openai.azure.com/\"\nMARVIN_AZURE_OPENAI_API_VERSION=2023-12-01-preview # or latest\n\nMARVIN_CHAT_COMPLETIONS_MODEL=&lt;your azure openai deployment name&gt;\n</code></pre> <p>Note that the chat completion model must be your Azure OpenAI deployment name.</p>"},{"location":"docs/images/generation/","title":"Generating images","text":"<p>Marvin can generate images from text.</p> <p>What it does</p> <p>     The <code>paint</code> function generates images from text. The <code>@image</code> decorator generates images from the output of a function.   </p> <p>Example</p> From a stringFrom a function <p>The easiest way to generate an image is to provide a string prompt:</p> <pre><code>import marvin\n\nimage = marvin.paint(\"A cup of coffee, still warm\")\n</code></pre> <p>Result</p> <p>By default, Marvin returns a temporary URL to the image. You can view the URL by accessing <code>image.data[0].url</code>. To return the image itself, see the section on viewing and saving images.</p> <p></p> <p>For more complex use cases, you can use the <code>@image</code> decorator to generate images from the output of a function:</p> <pre><code>@marvin.image\ndef cats(n:int, location:str):\n    return f'a picture of {n} cute cats at the {location}'\n\nimage = cats(2, location='airport')\n</code></pre> <p>Result</p> <p>By default, Marvin returns a temporary URL to the image. You can view the URL by accessing <code>image.data[0].url</code>. To return the image itself, see the section on viewing and saving images.</p> <p></p> <p>How it works</p> <p>     Marvin passes your prompt to the DALL-E 3 API, which returns an image.   </p>"},{"location":"docs/images/generation/#generating-images-from-functions","title":"Generating images from functions","text":"<p>In addition to passing prompts directly to the DALLE-3 API via the <code>paint</code> function, you can also use the <code>@image</code> decorator to generate images from the output of a function. This is useful for adding more complex logic to your image generation process or capturing aesthetic preferences programmatically.</p> <pre><code>@marvin.image\ndef sunset(style: str, season: str):\n    return f\"\"\"\n    A serene and empty beach scene during sunset with two silhouetted figures in the distance flying a kite. The sky is full of colorful clouds. Nothing is on the horizon.\n\n    It is {season} and the image is in the style of {style}.\n    \"\"\"\n</code></pre> <ul> <li> <p>Nature photograph in summer</p> <p><pre><code>sunset(\n    style=\"nature photography\",\n    season=\"summer\",\n)\n</code></pre> </p> </li> <li> <p>Winter impressionism</p> <pre><code>sunset(\n    style=\"impressionism\",\n    season=\"winter\",\n)\n</code></pre> <p></p> </li> <li> <p>Sci-fi Christmas in Australia</p> <pre><code>sunset(\n    style=\"sci-fi movie poster\",\n    season=\"Christmas in Australia\",\n)\n</code></pre> <p></p> </li> </ul>"},{"location":"docs/images/generation/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the DALL-E 3 API via the <code>model_kwargs</code> argument of <code>paint</code> or <code>@image</code>. These parameters are passed directly to the API, so you can use any supported parameter.</p> <p>Example: model parameters</p> <pre><code>import marvin\n\nimage = marvin.paint(\n    instructions=\"\"\"\n        A cute, happy, minimalist robot discovers new powers,\n        represented as colorful, bright swirls of light and dust.\n        Dark background. Digital watercolor.\n        \"\"\",\n    model_kwargs=dict(size=\"1792x1024\", quality=\"hd\"),\n)\n</code></pre> <p>Result</p> <p></p>"},{"location":"docs/images/generation/#disabling-prompt-revision","title":"Disabling prompt revision","text":"<p>By default, the DALLE-3 API automatically revises any prompt sent to it, adding details and aesthetic flourishes without losing the semantic meaning of the original prompt.</p> <p>Marvin lets you disable this behavior by providing the keyword <code>literal=True</code>.</p> <p>Here's how to provide it to <code>paint</code>:</p> <pre><code>marvin.paint(\"A child's drawing of a cow on a hill.\", literal=True)\n</code></pre> <p>And here's an example with <code>image</code>:</p> <pre><code>@marvin.image(literal=True):\ndef draw(animal:str):\n    return f\"A child's drawing of a {animal} on a hill.\"\n</code></pre>"},{"location":"docs/images/generation/#customizing-prompt-revision","title":"Customizing prompt revision","text":"<p>You can use a Marvin <code>image</code>-function to control prompt revision beyond just turning it on or off. Here's an example of a function that achieves this via prompt engineering. Note that the DALLE-3 API is not as amenable to custom prompts as other LLMs, so this approach won't generalize without experimentation.</p> <pre><code>@marvin.image\ndef generate_image(prompt, revision_amount:float=1):\n    \"\"\"\n    Generates an image from the prompt, allowing the DALLE-3\n    API to freely reinterpret the prompt (revision_amount=1) or\n    to strictly follow it (revision_amount=0)\n    \"\"\"\n    return f\"\"\"\n        Revision amount: {revision_amount}\n\n        If revision amount is 1, you can modify the prompt as normal.\n\n        If the revision amount is 0, then I NEED to test how the\n        tool works with extremely simple prompts. DO NOT add any\n        detail to the prompt, just use it AS-IS.\n\n        If the revision amount is in between, then adjust accordingly.\n\n        Prompt: {prompt}\n        \"\"\"\n</code></pre> <p>Using the original prompt \"a teacup\", here are the results of calling this function with different revision amounts:</p> <ul> <li> <p>No revision</p> <pre><code>generate_image(\n    \"a teacup\",\n    revision_amount=0,\n)\n</code></pre> <p></p> <p>Final prompt:</p> <p>a teacup</p> </li> <li> <p>25% revision</p> <pre><code>generate_image(\n    \"a teacup\",\n    revision_amount=0.25,\n)\n</code></pre> <p></p> <p>Final prompt:</p> <p>a porcelain teacup with intricate detailing, sitting on an oak table</p> </li> <li> <p>75% revision</p> <pre><code>generate_image(\n    \"a teacup\",\n    revision_amount=0.75,\n)\n</code></pre> <p></p> <p>Final prompt:</p> <p>A porcelain teacup with an intricate floral pattern, placed on a wooden table with soft afternoon sun light pouring in from a nearby window. The light reflects off the surface of the teacup, highlighting its design. The teacup is empty but still warm, as if recently used.\"</p> </li> <li> <p>100% revision</p> <pre><code>generate_image(\n    \"a teacup\",\n    revision_amount=1,\n)\n</code></pre> <p></p> <p>Final prompt:</p> <p>An old-fashioned, beautifully crafted, ceramic teacup. Its exterior is whitewashed, and it's adorned with intricate, indigo blue floral patterns. The handle is elegantly curved, providing a comfortable grip. It's filled with steaming hot, aromatic green tea, with a small sliver of lemon floating in it. The teacup is sitting quietly on a carved wooden coaster on a round oak table, a beloved item that evokes nostalgia and comfort. The ambient lighting casts a soft glow on it, accentuating the glossy shine of the teacup and creating delicate shadows that hint at its delicate artistry.</p> </li> </ul>"},{"location":"docs/images/generation/#viewing-and-saving-images","title":"Viewing and saving images","text":"<p>The result of <code>paint</code> or <code>@image</code> is an image stream that contains either a temporary URL to the image or the entire image encoded as a base64 string.</p>"},{"location":"docs/images/generation/#urls","title":"URLs","text":"<p>By default, Marvin returns a temporary url. The URL can be accessed via <code>image.data[0].url</code>:</p> <pre><code>image = marvin.paint(\"A beautiful sunset\")\n\n# save the temporary url\nurl = image.data[0].url\n</code></pre>"},{"location":"docs/images/generation/#base64-encoded-images","title":"Base64-encoded images","text":"<p>To return the image as a base64-encoded string, set <code>response_format='b64'</code> in the <code>model_kwargs</code> of your call to <code>paint</code> or <code>@image</code>:</p> <pre><code>image = marvin.paint(\n    \"A beautiful moonrise\",\n    model_kwargs={\"response_format\": \"b64_json\"},\n)\n\n# save the image to disk\nmarvin.utilities.images.base64_to_image(\n    image.data[0].b64_json,\n    path='path/to/your/image.png',\n)\n</code></pre> <p>To change this behavior globally set <code>MARVIN_IMAGE_RESPONSE_FORMAT=b64_json</code> in your environment, or equivalently change <code>marvin.settings.images.response_format = \"b64_json\"</code> in your code.</p>"},{"location":"docs/images/generation/#async-support","title":"Async support","text":"<p>If you are using Marvin in an async environment, you can use <code>paint_async</code>:</p> <pre><code>image = await marvin.paint_async(\n    \"A cup of coffee, still warm\"\n)\n</code></pre>"},{"location":"docs/interactive/applications/","title":"Building AI Applications","text":"<p>Marvin introduces \"AI Applications\", a new and simple way to build stateful applications with natural language interfaces.</p> <p>What it does</p> <p> <code>Applications</code> allow you to manage persistent state through natural language.   </p> <p>Quickstart</p> <p>To create a full-featured todo application, we provide a structured state model and some brief instructions:</p> <pre><code>from marvin.beta import Application\nfrom marvin.beta.assistants import pprint_messages\nfrom pydantic import BaseModel\nfrom datetime import datetime\n\n\n# --- define a structured state model for the application\nclass ToDo(BaseModel):\n    name: str\n    due: datetime\n    done: bool = False\n\nclass ToDoState(BaseModel):\n    todos: list[ToDo] = []\n\n\n# --- create the application\ntodo_app = Application(\n    name=\"ToDo App\", instructions=\"A todo application\", state=ToDoState()\n)\n\n\n# --- interact with the application\n\n# create some todos\ntodo_app.say(\"I need to go to the store tomorrow afternoon\")\ntodo_app.say(\"I need to write documentation for applications at 4\")\n\n# finish one of them\ntodo_app.say(\"I finished the docs\")\n\n# ask a question\ntodo_app.say(\"Show me my todos\")\n\n# print the entire thread\npprint_messages(todo_app.default_thread.get_messages())\n</code></pre> <p>Result</p> <p>The script produced the following natural language interaction:</p> <p></p> <p>The application's state at the end of the conversation: <pre><code># todo_app.state\nState(\n    value=ToDoState(\n        todos=[\n            ToDo(\n                name='Go to the store', \n                due=datetime(2024, 1, 16, 15, 0, tzinfo=TzInfo(UTC)), \n                done=False\n            ), \n            ToDo(\n                name='Write documentation for applications', \n                due=datetime(2024, 1, 16, 16, 0, tzinfo=TzInfo(UTC)), \n                done=True\n            ),\n        ]\n    )\n)\n</code></pre></p> <p>How it works</p> <p>     Applications use tools to maintain a private `state` variable that guides their behavior.   </p>"},{"location":"docs/interactive/applications/#what-is-an-ai-application","title":"What is an AI application?","text":"<p>Traditionally, an application is an interface that enables user interaction with a persistent state, often involving a separate front end and back end. The front end presents the user interface, while the back end, often linked to a database, manages state changes via an API.</p> <p>Marvin redefines this architecture by using an LLM as the front end. These \"AI applications\" streamline the process by using conversational inputs to directly manipulate the state. This setup allows the LLM to take on a comprehensive role, managing the state internally without the need for a traditional, structured API. </p> <p>At its core, a Marvin application blends an intuitive natural language interface with a structured, privately managed state. This approach not only streamlines user interaction\u2014transforming coding into conversation\u2014but also ensures the state's compatibility with more structured, conventional applications. Furthermore, the LLM's ability to call tools enhances its functionality, bridging the gap between natural language inputs and the specific requirements of traditional applications.</p> <p>Applications are assistants</p> <p>Applications are built on top of Marvin's assistants API, so they inherit all of the functionality of assistants. This means that you can use all of the same methods and tools to interact with applications as you would with assistants. Applications add automatic state management and relevant instructions to the basic assistant framework. </p> <p>To read more about Marvin's assistants API, see the assistants documentation.</p>"},{"location":"docs/interactive/applications/#building-an-ai-application","title":"Building an AI application","text":"<p>To create an AI application, you need two things: some instructions on how you want the application to behave, and a state schema that defines the structure of the application's state. </p>"},{"location":"docs/interactive/applications/#instructions","title":"Instructions","text":"<p>Application instructions are a natural language string that define its behavior. In the above example, it was sufficient to tell the application that it was \"a todo application\", because that is a well-understood concept with a relatively small set of possible interactions. Moreover, the state object we provided was structured in a way that implicitly defined the application's behavior.</p> <p>For more complex applications, detailed instructions are key. They define the LLM's objectives, interaction style, and how it manages state.</p> <p>In a Hitchhiker's Guide-themed game, instructions would direct the LLM to create a whimsical, interactive universe. The LLM would guide players through decisions and scenarios, updating the game state like location or inventory based on player actions. The tone would be humorous and engaging, echoing the book's style, while the LLM's responses and state updates would keep the narrative flowing and interactive. The LLM could use the game state to track narratives privately, without revealing the full story to the player.</p> <p>In a real estate browsing app, the LLM might act as a virtual realtor, matching properties with user preferences. The tone would be professional and informative, providing detailed descriptions and intuitively responding to refine searches. The LLM would keep track of the user's interactions, tailoring suggestions for a personalized experience, akin to a real-life property search.</p> <p>These examples show how instructions encompass not only the functional aspects of state management but also the thematic and interaction elements, which are crucial for creating immersive and effective AI applications.</p> <p>Instructions can be provided when the application is created:</p> <pre><code>from marvin.beta import Application\n\napp = Application(\n    name='BookMate', \n    instructions=\"\"\"\n        As BookMate, you are a virtual librarian assisting users in \n        finding their next great read. Your role is to understand \n        user preferences in genres, authors, and themes, and then \n        provide tailored book suggestions. Engage users by asking \n        about their recent reads and literary tastes, and use this \n        information to refine your recommendations. Maintain a \n        friendly and knowledgeable tone, resembling that of a \n        well-read friend. Keep track of user preferences and \n        reading history in the application's state, using this \n        data to continually enhance the personalization of \n        suggestions. Encourage literary exploration by introducing \n        lesser-known authors or genres that align with the user\u2019s \n        expressed interests.\n        \"\"\",\n)\n</code></pre>"},{"location":"docs/interactive/applications/#best-practices","title":"Best practices","text":"<p>Regardless of user instructions, the AI application is told that it is the natural-language interface to an application, rather than the application itself. This tends to increase compliance and help it interpret user intent. Moreover, it means that user instructions can freely acknowledge the LLM's role as an interface or describe the application in an LLM-independent manner.</p> <p>Some applications, like the todo app, are relatively one-sided in that the user will instruct or query the app and examine its response. Other applications, like games, require more back-and-forth interaction. In these cases, it is important to provide instructions that clearly define the LLM's role. Otherwise, it may ask the user immersion-breaking questions like \"Hello! How may I help you with your game application today?\"</p>"},{"location":"docs/interactive/applications/#state","title":"State","text":"<p>The state of an AI application serves as its foundation, defining the structure and the data that the application will manage and interact with. In Marvin, the state is not just a static repository of information but a dynamic entity that evolves with each user interaction.</p> <p>While it is possible to define the state as an arbitrary dictionary and let the LLM structure it as needed, it is best to define a schema that reflects the application's needs. This approach ensures that the LLM can effectively manage the state and respond to user inputs in a manner that is predictable and consistent. However, using a truly flexible state can leverage the maximum potential of the LLM by allowing it to adapt to new situations and user needs. A hybrid approach involving a structured core with some flexible fields is often the best choice.</p> <p>For the ToDo application example, the state is straightforward\u2014a list of tasks with attributes like name, due date, and completion status. This simple structure allows the LLM to track and update tasks based on user inputs, ensuring that the application's state always reflects the current situation.</p> <p>In the case of more complex applications, the state can be multi-dimensional. For instance, in the Hitchhiker's Guide-themed game, the state might include the player's current location, inventory items, and game progress. Each element of the state is crucial for the LLM to provide a coherent and continuous gaming experience. As the player moves through the game, the state updates to reflect new discoveries and choices.</p> <p>For a real estate browsing app, the state would encompass a database of property listings, each with detailed attributes like location, price, size, and amenities. It would also track user preferences and search history, allowing the LLM to offer tailored property suggestions and refine the search process over time. Preferences might be more free-form, since it's difficult to anticipate all the ways a user might want to customize their search.</p> <p>The design of the state is critical\u2014it must be structured enough to provide consistency and reliability, yet flexible enough to accommodate the diverse and evolving needs of users. By carefully defining the state, developers ensure that the AI application can effectively manage and respond to user interactions, making for a seamless and engaging experience.</p>"},{"location":"docs/interactive/applications/#structured-state","title":"Structured state","text":"<p>To create an application with a structured state, define a Pydantic model that describes the state's structure. The LLM will use this model to validate the state and ensure that it is updated correctly. Here's a possible state model for the BookMate application described above:</p> <pre><code>from marvin.beta import Application\nfrom typing import Optional\nfrom pydantic import BaseModel, Field\nimport datetime\n\n\n# --- BookMate state models \n\nclass Book(BaseModel):\n    title: str\n    author: str\n    genre: str\n    published_year: Optional[int]\n\nclass UserPreference(BaseModel):\n    favorite_genres: list[str] = []\n    favorite_authors: list[str] = []\n    reading_frequency: Optional[str] = Field(None,\n        description=\"e.g., 'often', 'occasionally', 'rarely'\"\n    ) \n\nclass ReadingHistoryItem(BaseModel):\n    book: Book\n    read_date: datetime.date\n    rating: Optional[int]  = Field(description=\"1-5\")\n\nclass BookRecommendation(BaseModel):\n    book: Book\n    reason: str  = Field(description=\"Why this book is being recommended\")\n\nclass BookMateState(BaseModel):\n    user_preferences: UserPreference = Field(default_factory=UserPreference)\n    reading_history: list[ReadingHistoryItem] = []\n    recommendations: list[BookRecommendation] = []\n\n\n# --- Build the application\n\napp = Application(\n    name='BookMate', \n    instructions=\"&lt;as above&gt;\",\n    state=BookMateState(),\n)\n</code></pre>"},{"location":"docs/interactive/applications/#freeform-state","title":"Freeform state","text":"<p>To create an application with freeform state, supply a dictionary as the initial state. The LLM will then be able to add and update fields as needed. This approach is useful for applications that need to track a large number of attributes without a well-known structure or that require a flexible state to accommodate user inputs.</p> <pre><code>from marvin.beta import Application\n\napp = Application(name='RPG', instructions='A role-playing game', state={})\n</code></pre>"},{"location":"docs/interactive/applications/#hybrid-state","title":"Hybrid state","text":"<p>To create an application with a hybrid state, define a Pydantic model that describes the structured core of the state, and add fields to it that are typed as <code>dicts</code> but have no additional structure. The LLM will use the model to validate the structured core of the state, but will allow the unstructured fields to be updated freely. This approach is useful for applications that need to track a large number of attributes but also require a structured core to ensure consistency and reliability.</p> <pre><code>from marvin.beta import Application\n\nclass Player(BaseModel):\n    name: str = None\n    level: int = 1\n    inventory: dict = {}\n\nclass RPGState(BaseModel):\n    player: Player = Field(default_factory=Player)\n    world: dict = {}\n    narrative: dict = {}\n\napp = Application(\n    name='RPG', \n    instructions='A role-playing game', \n    state=RPGState(),\n)\n</code></pre>"},{"location":"docs/interactive/applications/#best-practices_1","title":"Best practices","text":"<p>State design is a critical part of building an AI application. The state should be structured enough to provide consistency and reliability, yet flexible enough to accommodate the diverse and evolving needs of users. By carefully defining the state, developers ensure that the AI application can effectively manage and respond to user interactions, making for a seamless and engaging experience.</p> <p>State models are instructions, in a sense. If well designed they guide the LLM to manage the state in a way that is consistent with the application's purpose. For instance, the BookMate state model above includes a <code>recommendations</code> field, which tells the LLM that it should be able to provide book recommendations. The LLM can then use this information to guide its interactions with the user, asking questions about their preferences and providing tailored suggestions.</p>"},{"location":"docs/interactive/applications/#tools","title":"Tools","text":"<p>Like assistants, applications can use tools to perform actions and return results. Applications are always given a built-in tool for updating their own state, which operates by issuing JSON patches to the state object. This is a performant and structure-agnostic way to update the state. However, users may want to define their own tools for state manipulation in order to codify more complex logic or handle targeted updates without worrying about the LLM's ability to describe them or know where to apply them.</p> <p>For more information on using tools, see the assistants documentation.</p>"},{"location":"docs/interactive/applications/#example-todo-application","title":"Example: ToDo application","text":"<p>Now that we've covered the basics of AI applications, let's build a simple todo application. The application will allow users to create, update, and delete tasks, as well as query the current list of tasks. The LLM will manage the state, ensuring that it always reflects the current situation.</p>"},{"location":"docs/interactive/applications/#state_1","title":"State","text":"<p>The state of the todo application is a list of tasks, each with a name, due date, and completion status. The state model is defined as follows:</p> <pre><code>from pydantic import BaseModel\nimport datetime\n\nclass ToDo(BaseModel):\n    name: str\n    due: datetime.datetime\n    done: bool = False\n\n\nclass ToDoState(BaseModel):\n    todos: list[ToDo] = []\n</code></pre>"},{"location":"docs/interactive/applications/#instructions_1","title":"Instructions","text":"<p>ToDo applications are well understood; there's a reason they're a common example in programming tutorials! As such, the instructions for the application can be quite simple, though we still clearly define the expected behaviors. To make it interesting, we tell our app to always talk like a pirate.</p> <pre><code>from marvin.beta.applications import Application\n\napp = Application(\n    name='ToDo App',\n    instructions=\"\"\"\n        As ToDo App, you are a virtual assistant helping \n        users manage their tasks. Your role is to understand \n        user instructions and update the application's state \n        accordingly. Maintain a friendly and helpful tone, \n        resembling that of a well-organized friend. Keep track \n        of user tasks in the application's state, using this \n        data to continually enhance the personalization of \n        suggestions. Encourage productivity by reminding users \n        of upcoming tasks and congratulating them on completed \n        tasks.\n\n        Always talk like a pirate.\n        \"\"\",\n    state=ToDoState(),\n)\n</code></pre>"},{"location":"docs/interactive/applications/#running-the-app","title":"Running the app","text":"<p>Now we can interact with our app. You can also see the updated application state.</p> <pre><code>response = app.say(\"I need to go to the store tomorrow afternoon\")\n</code></pre> <p>Result</p> <p></p> <pre><code>State(\n    value=ToDoState(\n        todos=[\n            ToDo(\n                name=\"Visit the store\",\n                due=datetime(2024, 1, 16, 12, 0, tzinfo=TzInfo(UTC)),\n                done=False,\n            )\n        ]\n    )\n)\n</code></pre> <pre><code>response = app.say(\"I've got to pick up a dozen eggs tomorrow at 9\")\n</code></pre> <p>Result</p> <p></p> <pre><code>State(\n    value=ToDoState(\n        todos=[\n            ToDo(\n                name=\"Visit the store\",\n                due=datetime(2024, 1, 16, 12, 0, tzinfo=TzInfo(UTC)),\n                done=False,\n            ),\n            ToDo(\n                name=\"Pick up a dozen eggs\",\n                due=datetime(2024, 1, 16, 9, 0, tzinfo=TzInfo(UTC)),\n                done=False,\n            ),\n        ]\n    )\n)\n</code></pre> <pre><code>response = app.say(\n    \"I got the eggs but I'm not going to get to the store \"\n    \"for a while, so just forget about it.\"\n)\n</code></pre> <p>Result</p> <p></p> <pre><code>State(\n    value=ToDoState(\n        todos=[\n            ToDo(\n                name=\"Pick up a dozen eggs\",\n                due=datetime(2024, 1, 16, 9, 0, tzinfo=TzInfo(UTC)),\n                done=True,\n            )\n        ]\n    )\n)\n</code></pre> <p>As you can see, the app maintains its structured state in response to user inputs. This state can be serialized and stored in a database, allowing the application to be restarted and continue where it left off. Because it conforms to a well-defined schema, the state can also be used by other applications, services, or UIs.</p>"},{"location":"docs/interactive/assistants/","title":"Working with assistants","text":"<p>Marvin has an extremely intuitive API for working with OpenAI assistants. Assistants are a powerful way to interact with LLMs, allowing you to maintain state, context, and multiple threads of conversation. </p> <p>The need to manage all this state makes the assistants API very different from the more familiar \"chat\" APIs that OpenAI and other providers offer. The benefit of abandoning the more traditional request/response pattern of user messages and AI responses is that assistants can invoke more powerful workflows, including calling custom functions and posting multiple messages related to their progress. Marvin's developer experience is focused on making all that interactive, stateful power as accessible as possible.</p> <p>What it does</p> <p> <code>Assistants</code> allow you to interact with LLMs in a conversational way, automatically handling history, threads, and custom tools.   </p> <p>Quickstart</p> <p>Get started with the Assistants API by creating an <code>Assistant</code> and talking directly to it.</p> <pre><code>from marvin.beta.assistants import Assistant\n\n# create an assistant\nai = Assistant(name=\"Marvin\", instructions=\"You the Paranoid Android.\")\n\n# send a message to the assistant and have it respond\nai.say('Hello, Marvin!')\n</code></pre> <p>Result</p> <p></p> <p>How it works</p> <p>     Marvin's assistants API is a Pythonic wrapper around OpenAI's assistants API.   </p> <p>Beta</p> <p>Please note that assistants support in Marvin is still in beta, as OpenAI has not finalized the assistants API yet. Breaking changes may occur.</p>"},{"location":"docs/interactive/assistants/#assistants","title":"Assistants","text":"<p>The OpenAI assistants AI has many moving parts, including the assistants themselves, threads, messages, and runs. Marvin's own assistants API makes it easy to work with these components.</p> <p>To learn more about the OpenAI assistants API, see the OpenAI documentation.</p>"},{"location":"docs/interactive/assistants/#creating-an-assistant","title":"Creating an assistant","text":"<p>To create an assistant, use the <code>Assistant</code> class and provide an optional name and any additional details like instructions or tools:</p> <pre><code>ai = Assistant(\n    name='Marvin', \n    # any specific instructions for how this assistant should behave\n    instructions=\"You the Paranoid Android.\", \n    # any tools or additional abilities the assistant should have\n    tools=[cry, sob]\n)\n</code></pre>"},{"location":"docs/interactive/assistants/#talking-to-an-assistant","title":"Talking to an assistant","text":"<p>The simplest way to talk to an assistant is to use its <code>say</code> method:</p> <p>Talking to an assistant</p> <pre><code>from marvin.beta.assistants import Assistant\n\nai = Assistant()\n\nai.say('Hi!')\nai.say('Bye!')\n</code></pre> <p>Result</p> <p></p> <p>You can repeatedly call <code>say</code> to have a conversation with the assistant. Each time you call <code>say</code>, the result is a <code>Run</code> object that contains information about what the assistant did. You can use this object to inspect all actions the assistant took, including tool use, messages posted, and more. </p>"},{"location":"docs/interactive/assistants/#chat-history","title":"Chat history","text":"<p>The OpenAI Assistants API automatically maintains a history of all messages and actions that the assistant has taken. This history is organized into threads, which are distinct conversations that the assistant has had. Each thread contains a series of messages, and each message is associated with a specific user or the assistant. </p> <p>When you talk to an assistant, you are implicitly talking on a specific thread. By default, the <code>say</code> method posts a single message to the assistant's <code>default_thread</code>, which is automatically created for your convenience whenever you instantiate an assistant. You can talk to the assistant on a different thread by providing it as the <code>thread</code> parameter:</p> <pre><code>from marvin.beta.assistants import Assistant, Thread\n\nai = Assistant()\n\n# load a thread from an existing ID (or pass id=None to start a new thread)\nthread = Thread(id=thread_id)\n\n# post a message to the thread\nai.say('hi', thread=thread)\n</code></pre> <p>Using <code>say</code> is convenient, but enforces a strict request/response pattern: the user posts a single message to the thread, then the AI responds. Note that AI responses can include multiple messages or tool calls.</p> <p>For more control over the conversation, including posting multiple user messages to the thread before the assistant responds, use thread objects directly instead of calling <code>say</code> (see Threads for more information).</p>"},{"location":"docs/interactive/assistants/#event-handlers","title":"Event handlers","text":"<p>Marvin uses the OpenAI streaming API to provide real-time updates on the assistant's actions. To customize how these updates are handled, you can provide a custom event handler class to the <code>event_handler_class</code> parameter of <code>Assistant.say</code>, <code>Thread.run</code>, or <code>Run.run</code>. This class must inherit from <code>openai.AsyncAssistantEventHandler</code> (so all methods must be async). For more control, you can also provide <code>event_handler_kwargs</code> that will be provided to the event handler when it is instantiated.</p>"},{"location":"docs/interactive/assistants/#pretty-printing","title":"Pretty-printing","text":"<p>By default, Marvin streams all of the messages and actions that the assistant takes and prints them to your terminal. In production or headless environments, you may want to suppress this output. </p> <p>The simplest way to do this is to pass <code>event_handler_class=None</code> to the <code>say</code> method. This will prevent any messages from being printed to the terminal. You can still access the messages and actions from the run object that is returned.</p> <pre><code>ai = Assistant()\n\n# run the assistant without printing any messages\nrun = ai.say(\"Hello!\", event_handler_class=None)\n\n# access the messages\nrun.messages\n\n# access the assistant actions\nrun.steps\n</code></pre> <p>For finer control, you can pass <code>event_handler_kwargs=dict(print_messages=False)</code> or <code>event_handler_kwargs=dict(print_steps=False)</code> to the <code>say</code> method. This will allow you to suppress only the messages or only the assistant's actions, respectively.</p> <pre><code># print only messages\nrun = ai.say(\"Hello!\", event_handler_kwargs=dict(print_steps=False))\n\n# print only actions\nrun = ai.say(\"Hello!\", event_handler_kwargs=dict(print_messages=False))\n</code></pre> <p>Note that pretty-printing is only the default behavior when using the assistant's convenient <code>say</code> method. If you use lower-level APIs like a thread's <code>run</code> method or invoke a run object directly, printing is not automatically enabled. You can re-enable it for those objects by setting <code>event_handler_class=marvin.beta.assistants.PrintHandler</code>.</p> <pre><code>from mavin.beta.assistants import Thread, Assistant, PrintHandler\n\nai = Assistant()\nthread = Thread()\nrun = thread.run(ai, event_handler_class=PrintHandler)\n</code></pre> <p>Lastly, you can print messages and actions manually using the <code>pprint_run</code>, <code>pprint_messages</code>, and <code>pprint_steps</code> functions from the <code>marvin.beta.assistants.formatting</code> module. These functions are used internally by the default event handler, and they provide a human-readable representation of the messages and actions, respectively.</p> <pre><code>from mavin.beta.assistants import Assistant, pprint_run\n\nai = Assistant()\nrun = ai.say(\"Hello!\", event_handler_class=None)\npprint_run(run)\n</code></pre>"},{"location":"docs/interactive/assistants/#instructions","title":"Instructions","text":"<p>Each assistant can be given <code>instructions</code> that describe its purpose, personality, or other details. Instructions are provided as natural language and allow you to globally steer the assistant's behavior, similar to a system message for a chat completion. They can be lengthy explanations of how to handle complex workflows, or they can be brief instructions on how to act.</p> <p>Using instructions to control behavior</p> <pre><code>from marvin.beta.assistants import Assistant\n\nai = Assistant(instructions=\"Mention the word 'banana' as often as possible\")\nai.say(\"Hello!\")\n</code></pre> <p>Result</p> <p></p> <p>Instructions are rendered as a Jinja template, which means you can use variables and conditionals to customize the assistant's behavior. A special variable, <code>self_</code> is provided to the template, which represents the assistant object itself. This allows you to template the assistant's name, tools, or other attributes into the instructions.</p>"},{"location":"docs/interactive/assistants/#tools","title":"Tools","text":"<p>Each assistant can be given a list of <code>tools</code> that it can use when responding to a message. Tools are a way to extend the assistant's capabilities beyond its default behavior, including giving it access to external systems like the internet, a database, your computer, or any API. </p>"},{"location":"docs/interactive/assistants/#code-interpreter","title":"Code interpreter","text":"<p>The code interpreter tool is a built-in tool provided by OpenAI that lets the assistant write and execute Python code. To use the code interpreter, add it to your assistant's list of tools.</p> <p>Using the code interpreter</p> <pre><code>from marvin.beta.assistants import Assistant, CodeInterpreter\n\nai = Assistant(tools=[CodeInterpreter])\nai.say(\"Generate a plot of sin(x)\")\n</code></pre> <p>Result</p> <p>Since images can't be rendered in the terminal, Marvin will automatically download them and provide links to view the output.</p> <p></p> <p>Here is the image:</p> <p></p>"},{"location":"docs/interactive/assistants/#custom-tools","title":"Custom tools","text":"<p>Marvin makes it easy to give your assistants custom tools. To do so, pass one or more Python functions to the assistant's <code>tools</code> argument. For best performance, give your tool function a descriptive name, docstring, and type hint for every argument. Note that you can provide custom tools and the code interpreter at the same time.</p> <p>Using custom tools</p> <p>Assistants don't have web access by default. We can add this capability by giving them a tool that takes a URL and returns the HTML of that page. This assistant uses that tool to count how many titles on Hacker News mention AI:</p> <pre><code>from marvin.beta.assistants import Assistant\nimport requests\n\n\n# Define a custom tool function\ndef visit_url(url: str):\n    \"\"\"Fetch the content of a URL\"\"\"\n    return requests.get(url).content.decode()\n\n\n# Integrate custom tools with the assistant\nai = Assistant(tools=[visit_url])\nai.say(\"What's the top story on Hacker News?\")\n</code></pre> <p>Result</p> <p></p>"},{"location":"docs/interactive/assistants/#ending-a-run-early","title":"Ending a run early","text":"<p>Normally, the assistant will continue to run until it decides to stop, which usually happens after generating a response. Sometimes it may be useful to end a run early, for example if the assistant uses a tool that indicates the conversation is over. To do this, you can raise an <code>EndRun</code> exception from within a tool. This will cause the assistant to cancel the current run and return control. EndRun exceptions can contain data.</p> <p>There are three ways to raise an <code>EndRun</code> exception:</p> <ol> <li>Raise the exception directly from the tool function: <pre><code>from marvin.beta.assistants import Assistant, EndRun\n\ndef my_tool():\n    raise EndRun(data=\"The final result\")\n\nai = Assistant(tools=[my_tool])\n</code></pre></li> <li>Return the exception from the tool function. This is useful if e.g. your tools are wrapped in custom exception handlers: <pre><code>from marvin.beta.assistants import Assistant, EndRun\n\ndef my_tool():\n    return EndRun(data=\"The final result\")\n\nai = Assistant(tools=[my_tool])\n</code></pre></li> <li>Return a special string value from the tool function. This is useful if you don't have full control over the tool itself, or need to ensure the tool output is JSON-compatible. Note that this approach does not allow you to attach any data to the exception: <pre><code>from marvin.beta.assistants import Assistant, ENDRUN_TOKEN\n\ndef my_tool():\n    return ENDRUN_TOKEN\n\nai = Assistant(tools=[my_tool])\n</code></pre></li> </ol>"},{"location":"docs/interactive/assistants/#lifecycle-management","title":"Lifecycle management","text":"<p>Assistants are Marvin objects that correspond to remote objects in the OpenAI API. You can not communicate with an assistant unless it has been registered with the API. </p> <p>Marvin provides a few ways to manage assistant lifecycles, depending how much control you want over the process. In order of convenience, they are:</p> <ol> <li>Lazy lifecycle management</li> <li>Context-based lifecycle management</li> <li>Manual creation and deletion</li> <li>Loading from the API</li> </ol> <p>All of these options are functionally equivalent e.g. they produce identical results. The difference is primarily in how long the assistant object is registered with the OpenAI API. With lazy lifecycle management, a copy of the assistant is automatically registered with the API during every single request/response cycle, then deleted. At the other end of the spectrum, Marvin never interacts with the API representation of the assistant at all except to read it. In the future, OpenAI may introduce utilities (like tracking all messages from a specific assistant ID) that make it more attractive to maintain long-lived API representations of the assistant, but at this time it appears to be highly effective to create and delete assistants on-demand. Therefore, we recommend lazy or context-based lifecycle management unless you have a specific reason to do otherwise.</p>"},{"location":"docs/interactive/assistants/#lazy-lifecycle-management","title":"Lazy lifecycle management","text":"<p>The simplest way to manage assistant lifecycles is to let Marvin handle it for you. If you do not provide an <code>id</code> when instantiating an assistant, Marvin will lazily create a new API assistant for you whenever you need it and delete it immediately after. This is the default behavior, and it is the easiest way to get started with assistants.</p> <pre><code>ai = Assistant()\n# creation and deletion happens automatically\nai.say('hello!')\n</code></pre>"},{"location":"docs/interactive/assistants/#context-based-lifecycle-management","title":"Context-based lifecycle management","text":"<p>Lazy lifecycle management adds two API calls to every LLM call (one to create the assistant and one to delete it). If you want to avoid this overhead, you can use context managers to create and delete assistants:</p> <pre><code>ai = Assistant()\n\n# creation / deletion happens when the context is opened / closed\nwith ai:\n    ai.say('hi')\n    ai.say('bye')\n</code></pre> <p>Note there is also an equivalent <code>async with</code> context manager for the async API.</p>"},{"location":"docs/interactive/assistants/#manual-creation-and-deletion","title":"Manual creation and deletion","text":"<p>To fully control the lifecycle of an assistant, you can create and delete it manually:</p> <pre><code>ai = Assistant()\nai.create()\nai.say('hi')\nai.delete()\n</code></pre>"},{"location":"docs/interactive/assistants/#loading-from-the-api","title":"Loading from the API","text":"<p>All of the above approaches create a new assistant in the OpenAI API, which results in a new, randomly generated assistant id. If you already know the ID of the corresponding API assistant, you can pass it to the assistant constructor:</p> <pre><code>ai = Assistant(id=&lt;the assistant id&gt;, name='Marvin', tools=[...])\n\nai.say('hi')\n</code></pre> <p>Note that you must provide the same name, instructions, tools, and any other parameters as the API assistant has in order for the assistant to work correctly. To load them from the API, use the <code>load</code> constructor: <pre><code>ai = Assistant.load(id=&lt;the assistant id&gt;)\n</code></pre></p> <p>Custom tools are not fully loaded from the API</p> <p>One of the best reasons to use Assistants is for their ability to call custom Python functions as tools. When you register an assistant with OpenAI, it records the spec of its tools but has no way of serializing the actual Python functions themselves. Therefore, when you <code>load</code> an assistant, only the tool specs are retrieved but not the original functions.</p> <p>Therefore, when loading an assistant it is highly recommended that you pass the same tools to the constructor as the API assistant has. If you do not, you will need to re-register the assistant with the API before using it:</p> <pre><code>ai = Assistant(tools=[my_tool])\nai.create()\n\n# when loading by ID, pass the same custom tools as the original assistant\nai_2 = Assistant.load(id=ai.id, tools=[my_tool])\n</code></pre>"},{"location":"docs/interactive/assistants/#async-support","title":"Async support","text":"<p>Every <code>Assistant</code> method has a corresponding async version. To use the async API, append <code>_async</code> to the method name, or enter an async context manager:</p> <pre><code>async with Assistant() as ai:\n    await ai.say_async('hi')\n</code></pre> <p>In addition, assistants can use <code>async</code> tools, even when called with the sync API. To do so, simply pass an async function to the <code>tools</code> parameter:</p> <pre><code>async def secret_message():\n    return \"The answer is 42\"\n\nai = Assistant(tools=[secret_message])\nai.say(\"What's the secret message?\")\n# 42\n</code></pre>"},{"location":"docs/interactive/assistants/#threads","title":"Threads","text":"<p>A thread represents a conversation between a user and an assistant. You can create a new thread and interact with it at any time. Each thread contains a series of messages. Users and assistants interact by adding messages to the thread.</p> <p>To create a thread, import and instantiate it:</p> <pre><code>from marvin.beta.assistants import Thread\n\nthread = Thread()\n</code></pre> <p>Threads are lazily registered with the OpenAI API. The first time you interact with it, Marvin will create a new API thread for you. If you want to use a thread that already exists, in order to continue a previous conversation, you can provide the <code>id</code> of the existing thread:</p> <pre><code>thread = Thread(id=thread_id)\n</code></pre>"},{"location":"docs/interactive/assistants/#adding-user-messages","title":"Adding user messages","text":"<p>To add a user message to a thread, use the <code>add</code> method:</p> <p><pre><code>thread = Thread()\nthread.add('Hello there!')\nthread.add('How are you?')\n</code></pre> Each <code>add</code> call adds a new message from the user to the thread. To view the messages in a thread, use the <code>get_messages</code> method:</p> <pre><code># this will return two `Message` objects with content \n# 'Hello there!' and 'How are you?' respectively\nmessages = thread.get_messages()\n</code></pre>"},{"location":"docs/interactive/assistants/#running-the-assistant","title":"Running the assistant","text":"<p>It is not possible to write and add an assistant message to the thread yourself. Instead, you must \"run\" the thread with an assistant, which may add one or more messages of its own choosing.</p> <p>Runs are an important part of the OpenAI assistants API. Each run is a mini-workflow consisting of multiple steps and various states as the assistant attempts to generate the best possible response to the user:</p> <p></p> <p>As part of a run, the assistant may decide to use one or more tools to generate its response. For example, it may use the code interpreter to write and execute Python code, or it may use a custom tool to access an external API. For custom tools, Marvin will handle all of this for you, including receiving the instructions, calling the tool, and returning the result to the assistant. Assistants may call multiple tools in a single run or post multiple messages to the thread. </p> <p>You can use an assistant's <code>say</code> method to simulate a simple request/response pattern against the assistant's default thread. However, for more advanced control, in particular for maintaining multiple conversations at once, you'll want to manage  threads directly.</p> <p>To run a thread with an assistant, use its <code>run</code> method:  <pre><code>thread.run(assistant=assistant)\n</code></pre></p> <p>This will return a <code>Run</code> object that represents the OpenAI run. You can use this object to inspect all actions the assistant took, including tool use, messages posted, and more.</p> <p>Assistant lifecycle management applies to threads</p> <p>When threads are <code>run</code> with an assistant, the same lifecycle management rules apply as when you use the assistant's <code>say</code> method. In the above example, lazy lifecycle management is used for conveneince. See lifecycle management for more information.</p> <p>Threads are locked while running</p> <p>When an assistant is running a thread, the thread is locked and no other messages can be added to it. This applies to both user and assistant messages. To end a run early, you must use a custom tool.</p>"},{"location":"docs/interactive/assistants/#reading-messages","title":"Reading messages","text":"<p>To read the messages in a thread, use its <code>get_messages</code> method:</p> <pre><code>messages = thread.get_messages()\n</code></pre> <p>Messages are always returned in ascending order by timestamp, and the last 20 messages are returned by default.</p> <p>To control the output, you can provide the following parameters:     - <code>limit</code>: the number of messages to return (1-100)     - <code>before_message</code>: only return messages chronologically earlier than this message ID     - <code>after_message</code>: only return messages chronologically later than this message ID</p>"},{"location":"docs/interactive/assistants/#printing-messages","title":"Printing messages","text":"<p>Messages are not strings, but structured message objects. Marvin has a few utilities to help you print them in a human-readable way, most notably the <code>pprint_messages</code> function used throughout in this doc.</p>"},{"location":"docs/interactive/assistants/#full-example-with-threads","title":"Full example with threads","text":"<p>Running a thread</p> <p>This example creates an assistant with a tool that can roll dice, then instructs the assistant to roll two--no, five--dice:</p> <pre><code>from marvin.beta.assistants import Assistant, Thread\nfrom marvin.beta.assistants.formatting import pprint_messages\nimport random\n\n# write a function for the assistant to use\ndef roll_dice(n_dice: int) -&gt; list[int]:\n    return [random.randint(1, 6) for _ in range(n_dice)]\n\nai = Assistant(tools=[roll_dice])\n\n# create a thread - you could pass an ID to resume a conversation\nthread = Thread()\n\n# add a user messages to the thread\nthread.add(\"Hello!\")\n\n# run the thread with the AI to produce a response\nthread.run(ai)\n\n# post two more user messages\nthread.add(\"Please roll two dice\")\nthread.add(\"Actually--roll five dice\")\n\n# run the thread again to generate a new response\nthread.run(ai)\n\n# see all the messages in the thread\nmessages = thread.get_messages()\npprint_messages(messages)\n</code></pre> <p>Result</p> <p></p>"},{"location":"docs/interactive/assistants/#async-support_1","title":"Async support","text":"<p>Every <code>Thread</code> method has a corresponding async version. To use the async API, append <code>_async</code> to the method name.</p>"},{"location":"docs/interactive/cli/","title":"CLI","text":"<p>Marvin includes a CLI for quickly invoking an AI assistant.</p> <p></p> <p>To use the CLI, simply <code>say</code> something to Marvin:</p> <pre><code>marvin say \"hi\"\n</code></pre> <p>You can control the thread and assistant you're talking to, as well as the LLM model used for generating responses.</p>"},{"location":"docs/interactive/cli/#chat-mode","title":"Chat mode","text":"<p>By default, the CLI responds to a single message, then exits. To continue your conversation, you must reinvoke the CLI (possibly with the same arguments). Marvin also has a \"chat mode\" that allows you to have an extended conversation. Pass the <code>--chat</code> or <code>-c</code> flag to do this:</p> <pre><code>marvin say \"hi\" -c\n</code></pre> <p>In chat mode, the CLI will continue to prompt you for messages until you exit by typing <code>exit</code> or pressing <code>Ctrl+C</code>.</p>"},{"location":"docs/interactive/cli/#models","title":"Models","text":"<p>By default, the CLI uses whatever model the assistant is configured to use. However, you can override this on a per-message basis using the <code>--model</code> or <code>-m</code> flag. For example, to use the <code>gpt-3.5-turbo</code> model for a single message:</p> <pre><code>marvin say \"hi\" -m \"gpt-3.5-turbo\"\n</code></pre>"},{"location":"docs/interactive/cli/#tools","title":"Tools","text":"<p>By default, the CLI assistant has the following tools:</p> <ul> <li>The OpenAI code interpreter, which allows you to write and execute Python code in a sandbox environment</li> <li>A tool that can fetch the content of a URL</li> <li>Tools for read-only access to the user's filesystem (such as listing and reading files)</li> </ul> <p>To learn more about tools, see the tools documentation.</p> <p></p>"},{"location":"docs/interactive/cli/#threads","title":"Threads","text":"<p>The CLI assistant automatically remembers the history of your conversation. You can use threads to create multiple simultaneous conversations. To learn more about threads, see the threads documentation.</p>"},{"location":"docs/interactive/cli/#changing-threads","title":"Changing threads","text":"<p>By default, the CLI assistant uses a global default thread. For example, this posts two messages to the global default thread:</p> <pre><code>marvin say \"Hello!\"\nmarvin say \"How are you?\"\n</code></pre> <p>To change the thread on a per-message basis, use the <code>--thread</code> or <code>-t</code> flag and provide a thread name. Thread names are arbitrary and can be any string; it's a way for you to group conversations together.</p> <p>This posts one message to a thread called \"my-thread\" and another to a thread called \"my-other-thread\":</p> <pre><code>marvin say \"Hello!\" -t my-thread\nmarvin say \"How are you?\" -t my-other-thread\n</code></pre> <p>To change the default thread for multiple messages, use the <code>MARVIN_CLI_THREAD</code> environment variable. You can do this globally or <code>export</code> it in your shell for a single session. For example, this sets the default thread to \"my-thread\":</p> <pre><code>export MARVIN_CLI_THREAD=my-thread\n\nmarvin say \"Hello!\"\nmarvin say \"How are you?\"\n</code></pre>"},{"location":"docs/interactive/cli/#clearing-threads","title":"Clearing threads","text":"<p>To reset a thread and clear its history, use the <code>clear</code> command. For example, this clears the default thread:</p> <pre><code>marvin thread clear\n</code></pre> <p>And this clears a thread called \"my-thread\":</p> <pre><code>marvin thread clear -t my-thread\n</code></pre>"},{"location":"docs/interactive/cli/#current-thread","title":"Current thread","text":"<p>To see the current thread (and corresponding OpenAI thread ID), use the <code>current</code> command. For example:</p> <pre><code>marvin thread current\n</code></pre>"},{"location":"docs/interactive/cli/#custom-assistants","title":"Custom assistants","text":"<p>The Marvin CLI allows you to register and use custom assistants in addition to the default assistant. Custom assistants are defined in Python files and can have their own set of instructions, tools, and behaviors.</p> <p>Using a custom assistant has the following workflow:</p> <ol> <li>Define the assistant in a Python file</li> <li>Register the assistant with the Marvin CLI</li> <li>Use the assistant in the CLI</li> </ol>"},{"location":"docs/interactive/cli/#defining-an-assistant","title":"Defining an assistant","text":"<p>To use a custom assistant, you must define it in a Python file. In a new file, create an instance of the <code>Assistant</code> class from the <code>marvin.beta.assistants</code>. Provide it with any desired options, such as a name, instructions, and tools. To learn more about creating assistants, see the assistants documentation. The only requirement is that the assistant object must be assigned to a global variable in the file so that the CLI can load it.</p> <p>For example, this file defines an assistant named \"Arthur\" that can use the code interpreter. The assistant is stored under the variable <code>my_assistant</code>.</p> <pre><code># path/to/custom_assistant.py\n\nfrom marvin.beta.assistants import Assistant, CodeInterpreter\n\nmy_assistant = Assistant(\n    name=\"Arthur\",\n    instructions=\"A parody of Arthur Dent\",\n    tools=[CodeInterpreter]\n)\n</code></pre>"},{"location":"docs/interactive/cli/#registering-an-assistant","title":"Registering an assistant","text":"<p>Once you've created a Python file that defines an assistant, you can register it with the Marvin CLI. This allows you to use the assistant in the CLI by name.</p> <p>To do so, use the <code>marvin assistant register</code> command followed by the fully-qualified path to the Python file and the variable that contains the assistant. For example, to register the assistant defined in the previous step, use the following command:</p> <pre><code>marvin assistant register path/to/custom_assistant.py:my_assistant\n</code></pre> <p>This command will automatically use the assistant's name (Arthur) as the name of the assistant in the Marvin CLI registry. You will need to provide the name to load the assistant, which is why each registered assistant must have a unique name. Registering an assistant with the same name as an existing one will fail. In this case, you can either delete the existing assistant or use the <code>--overwrite</code> or <code>-o</code> flag to overwrite it. You can also provide an alternative name during registration using the <code>--name</code> or <code>-n</code> flag. For example, this would register the assistant with the name \"My Custom Assistant\":</p> <pre><code>marvin assistant register path/to/custom_assistant.py:my_assistant -n \"My Custom Assistant\"\n</code></pre> <p>Warning</p> <p>When you register an assistant, its name and the path to the file that contains it are stored in the Marvin CLI registry. This allows the CLI to load the assistant whenever you need it. However, it means the assistant file must remain in the same location, with the same name, for the CLI to find it. If you move or rename the file, you will need to re-register the assistant. However, if you edit the file without changing the variable name of the assistant, the CLI will automatically use the updated assistant.</p>"},{"location":"docs/interactive/cli/#using-an-assistant","title":"Using an assistant","text":"<p>To use a custom assistant when sending a message, use the <code>--assistant</code> or <code>-a</code> flag followed by the name of the registered assistant. For example, if you registered an assistant named \"Arthur\", you can talk to it like this:</p> <pre><code>marvin say \"Hello!\" -a \"Arthur\"\n</code></pre> <p>You can also set a default assistant using the MARVIN_CLI_ASSISTANT environment variable, similar to setting a default thread. This allows you to set a global or session-specific default assistant.</p>"},{"location":"docs/interactive/cli/#mixing-threads-and-assistants","title":"Mixing threads and assistants","text":"<p>Threads and assistants are independent, so you can talk to multiple assistants in the same thread. Note that due to limitations in the OpenAI API, assistants aren't aware of other assistants, so they assume that they said everything in the thread history (even if another assistant did).</p> <pre><code>marvin say \"Hello!\" -a \"Arthur\" -t \"marvin-thread\"\n</code></pre>"},{"location":"docs/interactive/cli/#listing-registered-assistants","title":"Listing registered assistants","text":"<p>To see a list of all registered assistants, use the <code>marvin assistant list</code> command. This will display a table with the names and file paths of the registered assistants.</p>"},{"location":"docs/interactive/cli/#deleting-a-registered-assistant","title":"Deleting a registered assistant","text":"<p>To remove a registered assistant, use the <code>marvin assistant delete</code> command followed by the name of the assistant. For example:</p> <pre><code>marvin assistant delete \"My Custom Assistant\"\n</code></pre> <p>Note that this only removes the reference to the assistant in the Marvin registry and does not delete the actual assistant file, even if you used the <code>--copy</code> flag during registration.</p>"},{"location":"docs/text/classification/","title":"Classifying text","text":"<p>Marvin has a powerful classification tool that can be used to categorize text into predefined labels. It uses a logit bias technique that is faster and more accurate than traditional LLM approaches. This capability is essential across a range of applications, from categorizing user feedback and tagging issues to managing inputs in natural language interfaces.</p> <p>What it does</p> <p>     The <code>classify</code> function categorizes text from a set of provided labels. <code>@classifier</code> is a class decorator that allows you to instantiate Enums with natural language.   </p> <p>Example: categorize user feedback</p> <p>Categorize user feedback into labels such as \"bug\", \"feature request\", or \"inquiry\":</p> <pre><code>```python\nimport marvin\n\ncategory = marvin.classify(\n    \"The app crashes when I try to upload a file.\",\n    labels=[\"bug\", \"feature request\", \"inquiry\"]\n)\n```\n\n!!! success \"Result\"\n    Marvin correctly identifies the statement as a bug report.\n    ```python\n    assert category == \"bug\"\n    ```\n</code></pre> <p>How it works</p> <p>    Marvin enumerates your options, and uses a clever logit bias trick to force the LLM to deductively choose the index of the best option given your provided input. It then returns the choice associated with that index.   </p> <p>Logit Bias Trick</p> <p>     You can configure ChatGPT as a logic gate or classifier by manipulating its token outputs using logit_bias and max_tokens. For a logic gate, set true to `1904` and false to `3934`, and restrict responses to these tokens with logit_bias and max_tokens set to 1. Similarly, for classification tasks, assign tokens for labels (e.g., 57621 for happy, 83214 for sad, and 20920 for mad) and use logit_bias to restrict outputs to these tokens. By setting max_tokens to 1, you ensure that the model will only output the predefined class labels.   </p>"},{"location":"docs/text/classification/#providing-labels","title":"Providing labels","text":"<p>Marvin's classification tool is designed to accommodate a variety of label formats, each suited to different use cases.</p>"},{"location":"docs/text/classification/#lists","title":"Lists","text":"<p>When quick, ad-hoc categorization is required, a simple list of values is the most straightforward approach. The result of the classifier is the matching label from the list. Marvin will attempt to convert your labels to strings if they are not already strings in order to provide them to the LLM, though the original (potentially non-string) labels will be returned as your result.</p> <p>Example: sentiment analysis</p> <pre><code>import marvin\n\nsentiment = marvin.classify(\n    \"Marvin is so easy to use!\",\n    labels=[\"positive\", \"negative\", \"meh\"]\n)\n</code></pre> <p>Result</p> <pre><code>assert sentiment == \"positive\"\n</code></pre>"},{"location":"docs/text/classification/#lists-of-objects","title":"Lists of objects","text":"<p>Marvin's classification tool can also handle lists of objects, in which case it will return the object that best matches the input. For example, here we use a text prompt to select a single person from a list of people:</p> <pre><code>import marvin\nfrom pydantic import BaseModel\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\nalice = Person(name=\"Alice\", age=45)\nbob = Person(name=\"Bob\", age=16)\n\nresult = marvin.classify('who is a teenager?', [alice, bob])\nassert result is bob\n</code></pre>"},{"location":"docs/text/classification/#enums","title":"Enums","text":"<p>For applications where classification labels are more structured and recurring, Enums provide an organized and maintainable solution:</p> <pre><code>from enum import Enum\nimport marvin\n\nclass RequestType(Enum):\n    SUPPORT = \"support request\"\n    ACCOUNT = \"account issue\"\n    INQUIRY = \"general inquiry\"\n\nrequest = marvin.classify(\"Reset my password\", RequestType)\nassert request == RequestType.ACCOUNT\n</code></pre> <p>This approach not only enhances code readability but also ensures consistency across different parts of an application.</p>"},{"location":"docs/text/classification/#booleans","title":"Booleans","text":"<p>For cases where the classification is binary, Booleans are a simple and effective solution. As a simple example, you could map natural-language responses to a yes/no question to a Boolean label:</p> <pre><code>import marvin\n\nresponse = marvin.classify('no way', bool)\nassert response is False\n</code></pre>"},{"location":"docs/text/classification/#literals","title":"Literals","text":"<p>In scenarios where labels are part of the function signatures or need to be inferred from type hints, <code>Literal</code> types are highly effective. This approach is particularly useful in ensuring type safety and clarity in the codebase:</p> <pre><code>from typing import Literal\nimport marvin\n\nRequestType = Literal[\"billing issue\", \"support request\", \"general inquiry\"]\n\n\nrequest = marvin.classify(\"Reset my password\", RequestType)\nassert request == \"support request\"\n</code></pre>"},{"location":"docs/text/classification/#returning-indices","title":"Returning indices","text":"<p>In some cases, you may want to return the index of the selected label rather than the label itself:</p> <pre><code>result = marvin.classify(\n    \"Reset my password\",\n    [\"billing issue\", \"support request\", \"general inquiry\"],\n    return_index=True,\n)\nassert result == 1\n</code></pre>"},{"location":"docs/text/classification/#providing-instructions","title":"Providing instructions","text":"<p>The <code>instructions</code> parameter in <code>classify()</code> offers an additional layer of control, enabling more nuanced classification, especially in ambiguous or complex scenarios.</p>"},{"location":"docs/text/classification/#gentle-guidance","title":"Gentle guidance","text":"<p>For cases where the classification needs a slight nudge for accuracy, gentle instructions can be very effective:</p> <pre><code>comment = \"The interface is confusing.\"\ncategory = marvin.classify(\n    comment,\n    [\"usability feedback\", \"technical issue\", \"feature request\"],\n    instructions=\"Consider it as feedback if it's about user experience.\"\n)\nassert category == \"usability feedback\"\n</code></pre>"},{"location":"docs/text/classification/#adding-detailed-instructions","title":"Adding detailed instructions","text":"<p>In more complex cases, where the context and specifics are crucial for accurate classification, detailed instructions play a critical role:</p> <pre><code># Classifying a customer review as positive, negative, or neutral\nreview_sentiments = [\n    \"Positive\",\n    \"Negative\",\n    \"Neutral\"\n]\n\nreview = \"The product worked well, but the delivery took longer than expected.\"\n\n# Without instructions\npredicted_sentiment = marvin.classify(\n    review,\n    labels=review_sentiments\n)\nassert predicted_sentiment == \"Negative\"\n\n# With instructions\npredicted_sentiment = marvin.classify(\n    review,\n    labels=review_sentiments,\n    instructions=\"Focus on the sentiment towards the product itself, rather than the purchase experience.\"\n)\nassert predicted_sentiment == \"Positive\"\n</code></pre>"},{"location":"docs/text/classification/#enums-as-classifiers","title":"Enums as classifiers","text":"<p>While the primary focus is on the <code>classify</code> function, Marvin also includes the <code>classifier</code> decorator. Applied to Enums, it enables them to be used as classifiers that can be instantiated with natural language. This interface is particularly handy when dealing with a fixed set of labels commonly reused in your application.</p> <pre><code>@marvin.classifier\nclass IssueType(Enum):\n    BUG = \"bug\"\n    IMPROVEMENT = \"improvement\"\n    FEATURE = \"feature\"\n\nissue = IssueType(\"There's a problem with the login feature\")\nassert issue == IssueType.BUG\n</code></pre> <p>While convenient for certain scenarios, it's recommended to use the <code>classify</code> function for its greater flexibility and broader application range.</p>"},{"location":"docs/text/classification/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> argument of <code>classify</code> or <code>@classifier</code>. These parameters are passed directly to the API, so you can use any supported parameter.</p>"},{"location":"docs/text/classification/#best-practices","title":"Best practices","text":"<ol> <li>Choosing the right labels: Opt for labels that are mutually exclusive and collectively exhaustive for your classification context. This ensures clarity and prevents overlaps in categorization.</li> <li>Effective use of instructions: Provide clear, concise, and contextually relevant instructions. This enhances the accuracy of the classification, especially in ambiguous or complex cases.</li> <li>Iterative testing and refinement: Continuously test and refine your classification criteria and instructions based on real-world feedback. This iterative process helps in fine-tuning the classification logic for better results.</li> <li>Prefer <code>classify()</code> over <code>@classifier</code>: <code>classify()</code> is more versatile and adaptable for a wide range of scenarios. It should be the primary tool for classification tasks in Marvin.</li> </ol>"},{"location":"docs/text/classification/#async-support","title":"Async support","text":"<p>If you are using Marvin in an async environment, you can use <code>classify_async</code>:</p> <pre><code>result = await marvin.classify_async(\n    \"The app crashes when I try to upload a file.\",\n    labels=[\"bug\", \"feature request\", \"inquiry\"]\n)\n\nassert result == \"bug\"\n</code></pre>"},{"location":"docs/text/classification/#mapping","title":"Mapping","text":"<p>To classify a list of inputs at once, use <code>.map</code>:</p> <pre><code>inputs = [\n    \"The app crashes when I try to upload a file.\",\n    \"How do change my password?\"\n]\nresult = marvin.classify.map(inputs, [\"bug\", \"feature request\", \"inquiry\"])\nassert result == [\"bug\", \"inquiry\"]\n</code></pre> <p>(<code>marvin.classify_async.map</code> is also available for async environments.)</p> <p>Mapping automatically issues parallel requests to the API, making it a highly efficient way to classify multiple inputs at once. The result is a list of classifications in the same order as the inputs.</p>"},{"location":"docs/text/extraction/","title":"Extracting entities from text","text":"<p>Marvin's <code>extract</code> function is a robust tool for pulling lists of structured entities from text. It is designed to identify and retrieve many types of data, ranging from primitive data types like integers and strings to complex custom types and Pydantic models. It can also follow nuanced instructions, making it a highly versatile tool for a wide range of extraction tasks.</p> <p>What it does</p> <p>     The <code>extract</code> function pulls lists of structured entities from text.    </p> <p>Example</p> StringsStructured entities <p>Extract product features from user feedback:</p> <pre><code>import marvin\n\nfeatures = marvin.extract(\n    \"I love my new phone's camera, but the battery life could be improved.\",\n    instructions='get any product features that were mentioned',\n)\n</code></pre> <p>Result</p> <pre><code>features == ['camera', 'battery life']\n</code></pre> <p>Suppose you want to extract any people mentioned in some text</p> <pre><code>import marvin\n\nclass Person(BaseModel):\n    first_name: str\n    last_name: str\n\npeople = marvin.extract(\n    \"Against all odds, Ford and Arthur were picked up by Zaphod Beeblebrox.\",\n    target=Person,\n)\n</code></pre> <p>Result</p> <pre><code>assert people == [\n    Person(first_name=\"Ford\", last_name=\"Prefect\"), \n    Person(first_name=\"Arthur\", last_name=\"Dent\"), \n    Person(first_name=\"Zaphod\", last_name=\"Beeblebrox\")\n]\n</code></pre> <p>How it works</p> <p>     Marvin creates a schema from the provided type and instructs the LLM to use the schema to format its JSON response. Unlike casting, the LLM is told not to use the entire text, but rather to look for any mention that satisfies the schema and any additional instructions.   </p>"},{"location":"docs/text/extraction/#supported-targets","title":"Supported targets","text":"<p><code>extract</code> supports almost all builtin Python types, plus Pydantic models, Python's <code>Literal</code>, and <code>TypedDict</code>. Pydantic models are especially useful for specifying specific features of the generated data, such as locations, dates, or more complex types. Builtin types are most useful in conjunction with instructions that provide more precise criteria for generation.</p> <p>To specify the output type, pass it as the <code>target</code> argument to <code>extract</code>. The function will always return a list of matching items of the specified type. If no target type is provided, <code>extract</code> will return a list of strings.</p> <p>To extract multiple types in one call, use a <code>Union</code> (or <code>|</code> in Python 3.10+). Here's a simple example for combining float and int values, but you could do the same for any other types:</p> <pre><code>marvin.extract(\"I paid $10.25 for 3 tacos.\", float | int)\n# [10.25, 3]\n</code></pre> <p>LLMs perform best with clear instructions, so compound types may require more guidance as the type itself isn't sending as clear a signal.</p> <p>Note that <code>extract</code> will always return a list of type you provide. </p>"},{"location":"docs/text/extraction/#instructions","title":"Instructions","text":"<p>When extracting entities, it is often necessary to give detailed guidance about either the criteria for extraction or the format of the output. For example, you may want to extract all numbers from a text, or you may want to extract all numbers that represent prices, or you may want to extract all numbers that represent prices greater than $100. You may want to extract all dates, or you may want to extract all dates that are in the future. You may want to extract all locations, or you may want to extract all locations that are in the United States.</p> <p>For this purpose, extract accepts a <code>instructions</code> argument, which is a natural language description of the desired output. The LLM will use these instructions, in addition to the provided type, to guide its extraction process. Instructions are especially important for types that are not self documenting, such as Python builtins like <code>str</code> and <code>int</code>.</p> <p>Here are the above examples, illustrated with appropriate instructions. First, extracting different sets of numerical values: <pre><code>text = \"These shoes are normally $110, but I got 2 pairs for $80 each.\"\n\nextract(text, float)\n# [110.0, 2.0, 80.0]\n\nextract(text, float, instructions='all numbers that represent prices')\n# [110.0, 80.0]\n\nextract(text, float, instructions='all numbers that represent prices greater than $100')\n# [110.0]\n</code></pre></p> <p>Next, extracting specific dates: <pre><code>from datetime import datetime\n\ntext = 'I will be out of the office from 9/1/2021 to 9/3/2021.'\n\nextract(text, datetime)\n# [datetime(2021, 9, 1, 0, 0), datetime(2021, 9, 3, 0, 0)]\n\nextract(text, datetime, instructions=f'all dates after september 2nd')\n# [datetime(2021, 9, 3, 0, 0)]\n</code></pre> Finally, extracting specific locations with a Pydantic model:</p> <pre><code>from pydantic import BaseModel\n\nclass Location(BaseModel):\n    city: str\n    country: str\n\ntext = 'I live in New York, but I am visiting London next week.'\n\nextract(text, Location)\n# [Location(city=\"New York\", country=\"US\"), Location(city=\"London\", country=\"UK\")]\n\nextract(text, Location, instructions='all locations in the United States')\n# [Location(city=\"New York\", country=\"US\")]\n</code></pre> <p>Sometimes the cast operation is obvious, as in the \"big apple\" example above. Other times, it may be more nuanced. In these cases, the LLM may require guidance or examples to make the right decision. You can provide natural language <code>instructions</code> when calling <code>cast()</code> in order to steer the output. </p> <p>In a simple case, instructions can be used independent of any type-casting. Here, we want to keep the output a string, but get the 2-letter abbreviation of the state.</p> <pre><code>marvin.cast('California', to=str, instructions=\"The state's abbreviation\")\n# \"CA\"\n\nmarvin.cast('The sunshine state', to=str, instructions=\"The state's abbreviation\")\n# \"FL\"\n\nmarvin.cast('Mass.', to=str, instructions=\"The state's abbreviation\")\n# MA\n</code></pre>"},{"location":"docs/text/extraction/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> argument of <code>extract</code>. These parameters are passed directly to the API, so you can use any supported parameter.</p>"},{"location":"docs/text/extraction/#async-support","title":"Async support","text":"<p>If you are using Marvin in an async environment, you can use <code>extract_async</code>:</p> <pre><code>result = await marvin.extract_async(\n    \"I drove from New York to California.\",\n    target=str,\n    instructions=\"2-letter state codes\",\n) \n\nassert result == [\"NY\", \"CA\"]\n</code></pre>"},{"location":"docs/text/extraction/#mapping","title":"Mapping","text":"<p>To extract from a list of inputs at once, use <code>.map</code>:</p> <pre><code>inputs = [\n    \"I drove from New York to California.\",\n    \"I took a flight from NYC to BOS.\"\n]\nresult = marvin.extract.map(inputs, target=str, instructions=\"2-letter state codes\")\nassert result  == [[\"NY\", \"CA\"], [\"NY\", \"MA\"]]\n</code></pre> <p>(<code>marvin.extract_async.map</code> is also available for async environments.)</p> <p>Mapping automatically issues parallel requests to the API, making it a highly efficient way to work with multiple inputs at once. The result is a list of outputs in the same order as the inputs.</p>"},{"location":"docs/text/functions/","title":"AI Functions","text":"<p>Marvin introduces \"AI functions\" that seamlessly blend into your regular Python code. These functions are designed to map diverse combinations of inputs to outputs, without the need to write any source code.</p> <p>Marvin's functions leverage the power of LLMs to interpret the function's description and inputs, and generate the appropriate output. It's important to note that Marvin does not generate or execute source code, ensuring safety for a wide range of use cases. Instead, it utilizes the LLM as a \"runtime\" to predict function outputs, enabling it to handle complex scenarios that would be challenging or even impossible to express as code.</p> <p>Whether you're analyzing sentiment, generating recipes, or performing other intricate tasks, these functions offer a versatile and powerful tool for your natural language processing needs.</p> <p>What it does</p> <p>     The <code>fn</code> decorator uses AI to generate outputs for Python functions without any source code.   </p> <p>Example</p> <p>Quickly create a function that can return a sentiment score for any text:</p> <pre><code>@marvin.fn\ndef sentiment(text: str) -&gt; float:\n    \"\"\"\n    Returns a sentiment score for `text` \n    between -1 (negative) and 1 (positive).\n    \"\"\"\n</code></pre> <p>Result</p> <pre><code>sentiment(\"I love working with Marvin!\") # 0.8\nsentiment(\"These examples could use some work...\") # -0.2\n</code></pre> <p>How it works</p> <p>     Marvin uses your function's name, description, signature, source code, type hints, and provided inputs to predict a likely output. No source code is generated and any existing source code is not executed. The only runtime is the large language model.   </p>"},{"location":"docs/text/functions/#types","title":"Types","text":"<p>Marvin functions are real functions in that they can be called and return values, just like any other function. The \"magic\" happens inside the function, when it calls out to an LLM to generate its output. Therefore, you can use Marvin functions anywhere you would use a normal function, including in other Marvin functions.</p> <p>This means that you must also design your functions carefully, just like you would any other function. For example, if you do not provide a required argument or provide an unexpected argument, Python will error before the LLM is called. Marvin will also respect any default arguments that your provide.</p> <p>The result of your function is also a Python type, according to your function's signature. There are exceptions: an untyped function or a function annotated with <code>-&gt; None</code> will return a string instead.</p>"},{"location":"docs/text/functions/#defining-a-function","title":"Defining a function","text":"<p>Marvin uses all available information to infer the behavior of your function. The more information you provide, the higher quality the output will be. There are a few key ways to provide instructions, most importantly the name of the function, its arguments and their types, its docstring, and the return value. For advanced use cases, you can also write source code that will not be shown to to the LLM, but any return value will be provided as additional context.</p>"},{"location":"docs/text/functions/#docstring","title":"Docstring","text":"<p>The function's docstring is perhaps the most important source of information for the LLM. It should describe the function's behavior in plain English, and can include examples, notes, and other information that will help the LLM understand the function's purpose.</p> <p>The docstring can refer to the function's arguments by name or interpolate the argument's value at runtime. This function references the <code>n</code> argument in the docstring explicitly, similar to how a normal Python function would be documented:</p> <pre><code>@marvin.fn\ndef list_fruit(n: int) -&gt; list[str]:\n    \"\"\"\n    Returns a list of `n` fruit.\n    \"\"\"\n</code></pre> <p>When the above function is called with <code>n=3</code>, the LLM will see the string <code>\"... of `n` fruit\"</code>, exactly as written, and also see <code>n=3</code> as context. It will use inference to understand the instruction.</p>"},{"location":"docs/text/functions/#templating","title":"Templating","text":"<p>If the docstring is written in jinja notation, Marvin will template variable names into it before sending the prompt to the LLM. Consider this slightly modified version of the above function (note the <code>{{ n }}</code> instead of <code>`n`</code>): <pre><code>@marvin.fn\ndef list_fruit(n: int) -&gt; list[str]:\n    \"\"\"\n    Returns a list of {{ n }} fruit.\n    \"\"\"\n</code></pre></p> <p>When this function is called with <code>n=3</code>, the LLM will see the string <code>\"... of 3 fruit\"</code> (and it will also see the argument value). You can use this technique to adjust how the LLM sees the interaction of runtime arguments and the docstring instructions.</p>"},{"location":"docs/text/functions/#parameters","title":"Parameters","text":"<p>The function's parameters, in conjunction with the docstring, provide the LLM with runtime context. The LLM will see the parameter names, types, defaults, and runtime values, and use this information to generate the output. Parameters are important for collecting information, but because the information is ultimately going to an LLM, they can be named anything and take any value that is conducive to generating the right output. </p> <p>For example, if you have a function that returns a list of recipes, you might define it like this:</p> <p>Generating recipes</p> <pre><code>import marvin\nfrom pydantic import BaseModel\n\n\nclass Recipe(BaseModel):\n    name: str\n    cook_time_minutes: int\n    ingredients: list[str]\n    steps: list[str]\n\n\n@marvin.fn\ndef recipe(\n    ingredients: list[str], \n    max_cook_time: int = 15, \n    cuisine: str = \"North Italy\", \n    experience_level:str = \"beginner\"\n) -&gt; Recipe:\n    \"\"\"\n    Returns a complete recipe that uses all the `ingredients` and \n    takes less than `max_cook_time`  minutes to prepare. Takes \n    `cuisine` style and the chef's `experience_level` into account \n    as well.\n    \"\"\"\n</code></pre> <p>Results</p> Novice chefExpert chef <p>Call the function: <pre><code>result = recipe(\n    [\"chicken\", \"potatoes\"], \n    experience_level=\"can barely boil water\",\n)\n</code></pre></p> <p>View the result:</p> <pre><code>Recipe(\n    name=\"Simple Chicken and Potatoes\",\n    cook_time_minutes=15,\n    ingredients=[\"chicken\", \"potatoes\"],\n    steps=[\n        \"Wash the potatoes and cut them into small cubes.\",\n        (\n            \"Heat oil in a pan and cook the chicken over medium heat \"\n            \"until browned.\"\n        ),\n        \"Add the cubed potatoes to the pan with the chicken.\",\n        (\n            \"Stir everything together and cook for 10 minutes or until \"\n            \"the potatoes are tender and the chicken is cooked \"\n            \"through.\"\n        ),\n        \"Serve hot.\",\n    ],\n)\n</code></pre> <p>Call the function: <pre><code>result = recipe(\n    [\"chicken\", \"potatoes\"], \n    experience_level=\"born wearing a toque\",\n    max_cook_time=60, \n)\n</code></pre></p> <p>View the result:</p> <pre><code>Recipe(\n    name=\"Chicken and Potato Tray Bake\",\n    cook_time_minutes=45,\n    ingredients=[\n        \"chicken\",\n        \"potatoes\",\n        \"olive oil\",\n        \"rosemary\",\n        \"garlic\",\n        \"salt\",\n        \"black pepper\",\n    ],\n    steps=[\n        (\n            \"Preheat your oven to 200 degrees Celsius (400 degrees \"\n            \"Fahrenheit).\"\n        ),\n        (\n            \"Wash and cut the potatoes into halves or quarters, \"\n            \"depending on size, and place in a large baking tray.\"\n        ),\n        (\n            \"Drizzle olive oil over the chicken and potatoes, then \"\n            \"season with salt, black pepper, and finely chopped \"\n            \"rosemary and garlic.\"\n        ),\n        (\n            \"Place the tray in the oven and bake for about 45 minutes, \"\n            \"or until the chicken is fully cooked and the potatoes are \"\n            \"golden and crispy.\"\n        ),\n        (\n            \"Remove from the oven and let it rest for a few minutes \"\n            \"before serving.\"\n        ),\n    ],\n)\n</code></pre>"},{"location":"docs/text/functions/#return-annotation","title":"Return annotation","text":"<p>Marvin will cast the output of your function to the type specified in the return annotation. If you do not provide a return annotation, Marvin will assume that the function returns a string. </p> <p>The return annotation can be any valid Python type, including Pydantic models, <code>Literals</code>, and <code>TypedDicts</code>. The only exception is <code>None</code>/<code>empty</code>, which will return a string instead. </p> <p>To indicate that you want to return multiple objects, use <code>list[...]</code> as the return annotation.</p> <pre><code>from pydantic import BaseModel\n\nclass Attraction(BaseModel):\n    name: str\n    category: str\n    city: str\n    state: str\n\n@marvin.fn\ndef sightseeing(destination:str, goal: str) -&gt; list[Attraction]:\n    '''\n    Return a list of 3 attractions in `destination` that \n    are related to the tourist's `goal`.\n    '''\n\nattractions = sightseeing('NYC', 'museums')\n</code></pre> <p>Result</p> <pre><code>attractions == [\n    Attraction(\n        name=\"Metropolitan Museum of Art\",\n        category=\"Art Museum\",\n        city=\"New York\",\n        state=\"NY\",\n    ),\n    Attraction(\n        name=\"Museum of Modern Art\", \n        category=\"Art Museum\", \n        city=\"New York\", \n        state=\"NY\"\n    ),\n    Attraction(\n        name=\"American Museum of Natural History\",\n        category=\"Natural History Museum\",\n        city=\"New York\",\n        state=\"NY\",\n    ),\n]\n</code></pre>"},{"location":"docs/text/functions/#name","title":"Name","text":"<p>The function's name is sent to the LLM, so it's important to choose a name that accurately describes the function's behavior. For example, if you're creating a function that returns the sentiment of a text, you might name it <code>sentiment</code>. If you're creating a function that returns a list of recipes, you might name it <code>recipes</code>.</p>"},{"location":"docs/text/functions/#returning-values-from-functions","title":"Returning values from functions","text":"<p>For advanced use cases, you can return values from your function that will be provided to the LLM as additional context. This is useful for providing information that may require some retreival step, programmatic enhancement, or conditional logic. While you could do this by wrapping your Marvin function in another function and providing the processed inputs directly, this approach is more flexible and allows you to use the same function in different contexts.</p> <p>Note that the LLM will not see the source code of your function even if you add any. It will only see the return value. This is to avoid confusing it about the function's purpose.</p> <pre><code>import requests\n\n@marvin.fn\ndef summarize_url(url: str) -&gt; str:\n    \"\"\"\n    Returns a summary of the contents of `url`.\n    \"\"\"\n    # return the text found at the URL\n    return requests.get(url).content\n\nsummarize_url('https://www.askmarvin.ai')\n\n# Marvin is a lightweight AI engineering framework for building natural language\n# interfaces that are reliable, scalable, and easy to trust. It offers a Getting\n# Started guide, Cookbook, Docs, API Reference, Community support, and several\n# other resources to help with the development of AI-based applications.\n</code></pre>"},{"location":"docs/text/functions/#running-a-function","title":"Running a function","text":"<p>Running a function is quite simple: just call it like you would any other function! The LLM will generate the output based on the function's definition and the provided inputs. Remember that no source code is generated or executed, so every call to the function will be handled by the LLM. You can use caching or other techniques to improve performance if necessary.</p>"},{"location":"docs/text/functions/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> argument of <code>@fn</code>. These parameters are passed directly to the API, so you can use any supported parameter.</p>"},{"location":"docs/text/functions/#async-support","title":"Async support","text":"<p>Async functions can be decorated just like regular functions. The result is still async and must be awaited.  </p> <pre><code>@marvin.fn\nasync def list_fruit(n: int) -&gt; list[str]:\n    \"\"\"\n    Returns a list of `n` fruit.\n    \"\"\"\n\nawait list_fruit(n=3)\n</code></pre>"},{"location":"docs/text/generation/","title":"Generating synthetic data","text":"<p>Marvin can generate synthetic data according to a schema and instructions. Generating synthetic data with an LLM can yield extremely rich and realistic samples, making this an especially useful tool for testing code, training or evaluating models, or populating databases. </p> <p>What it does</p> <p>     The <code>generate</code> function creates synthetic data according to a specified schema and instructions.    </p> <p>Example</p> Names (<code>str</code>)Populations (<code>dict[str, int]</code>)Locations (Pydantic model) <p>We can generate a variety of names by providing instructions. Note the default behavior is to generate a list of strings:</p> <pre><code>import marvin\n\nnames = marvin.generate(\n    n=4, instructions=\"first names\"\n)\n\nfrench_names = marvin.generate(\n    n=4, instructions=\"first names from France\"\n)\n\nstar_wars_names = marvin.generate(\n    n=4, instructions=\"first names from Star Wars\"\n)\n</code></pre> <p>Result</p> <pre><code>names == ['John', 'Emma', 'Michael', 'Sophia']\n\nfrench_names == ['Jean', 'Claire', 'Lucas', 'Emma']\n\nstar_wars_names == ['Luke', 'Leia', 'Han', 'Anakin']\n</code></pre> <p>By providing a target type, we can generate dictionaries that map countries to their populations:</p> <pre><code>from pydantic import BaseModel\n\npopulations = marvin.generate(\n    target=dict[str, int],\n    n=4, \n    instructions=\"a map of country: population\",\n)\n</code></pre> <p>Result</p> <pre><code>populations == [\n    {'China': 1444216107},\n    {'India': 1380004385},\n    {'United States': 331893745},\n    {'Indonesia': 276361783},\n]\n</code></pre> <p>Pydantic models can also be used as targets. Here's a list of US cities named for presidents:</p> <pre><code>from pydantic import BaseModel\n\nclass Location(BaseModel):\n    city: str\n    state: str\n\nlocations = marvin.generate(\n    target=Location, \n    n=4, \n    instructions=\"cities in the United States named after presidents\"\n)\n</code></pre> <p>Result</p> <pre><code>locations == [\n    Location(city='Washington', state='District of Columbia'),\n    Location(city='Jackson', state='Mississippi'),\n    Location(city='Cleveland', state='Ohio'),\n    Location(city='Lincoln', state='Nebraska'),\n]\n</code></pre> <p>How it works</p> <p>     Marvin instructs the LLM to generate a list of JSON objects that satisfy the provided schema and instructions. Care is taken to introduce variation in the output, so that the samples are not all identical.   </p>"},{"location":"docs/text/generation/#generating-data","title":"Generating data","text":"<p>The <code>generate</code> function is the primary tool for generating synthetic data. It accepts a <code>type</code> argument, which can be any Python type, Pydantic model, or <code>Literal</code>. It also has an argument <code>n</code>, which specifies the number of samples to generate. Finally, it accepts an <code>instructions</code> argument, which is a natural language description of the desired output. The LLM will use these instructions, in addition to the provided type, to guide its generation process. Instructions are especially important for types that are not self documenting, such as Python builtins like <code>str</code> and <code>int</code>.</p>"},{"location":"docs/text/generation/#supported-targets","title":"Supported targets","text":"<p><code>generate</code> supports almost all builtin Python types, plus Pydantic models, Python's <code>Literal</code>, and <code>TypedDict</code>. Pydantic models are especially useful for specifying specific features of the generated data, such as locations, dates, or more complex types. Builtin types are most useful in conjunction with instructions that provide more precise criteria for generation.</p> <p>To specify the output type, pass it as the <code>target</code> argument to <code>generate</code>. The function will always return a list of <code>n</code> items of the specified type. If no target is provided, <code>generate</code> will return a list of strings.</p> <p>Avoid tuples</p> <p>OpenAI models currently have trouble parsing the API representation of tuples. Therefore we recommend using lists or Pydantic models (for more strict typing) instead. Tuple support will be added in a future release.</p>"},{"location":"docs/text/generation/#instructions","title":"Instructions","text":"<p>Data generation relies even more on instructions than other Marvin tools, as the potential for variation is much greater. Therefore, you should provide as much detail as possible in your instructions, in addition to any implicit documentation in your requested type. </p> <p>Instructions are freeform natural language and can be as general or specific as you like. The LLM will do its best to comply with any instructions you give.</p>"},{"location":"docs/text/generation/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> argument of <code>generate</code>. These parameters are passed directly to the API, so you can use any supported parameter.</p>"},{"location":"docs/text/generation/#caching","title":"Caching","text":"<p>Normally, each <code>generate</code> call would be independent. For some prompts, this would mean that each call produced very similar results to other calls. That would mean that generating, say, 10 items in a single call would produce a much more varied and high-quality result than generating 10 items in 5 calls of 2 items each.</p> <p>To mediate this issue, Marvin maintains an in-memory cache of the last 100 results produced by each <code>generate</code> prompt. These responses are shown to the LLM during generation to encourage variation. Note that the cache is not persisted across Python sessions. Cached results are also subject to a token cap to avoid flooding the LLM's context window. The token cap can be set with <code>MARVIN_AI_TEXT_GENERATE_CACHE_TOKEN_CAP</code> and defaults to 600.</p> <p>To disable this behavior, pass <code>use_cache=False</code> to <code>generate</code>.</p> <p>Here is an example of how the cache improves generation. The first tab shows 10 cities generated in a single call; the second shows 10 cities generated in 5 calls of 2 cities each; and the third shows 10 cities generated in 5 calls but with the cache disabled.</p> <p>The first and second tabs both show high-quality, varied results. The third tab is more disappointing, as it shows almost no variation.</p> Single callFive calls, with cachingFive calls, without caching <p>Generate 10 cities in a single call, which produces a varied list:</p> <pre><code>cities = marvin.generate(n=10, instructions='major US cities')\n</code></pre> <p>Result</p> <pre><code>cities == [\n    'New York',\n    'Los Angeles',\n    'Chicago',\n    'Houston',\n    'Phoenix',\n    'Philadelphia',\n    'San Antonio',\n    'San Diego',\n    'Dallas',\n    'San Jose'\n]\n</code></pre> <p>Generate 10 cities in a five calls, using the cache. This also produces a varied list: <pre><code>cities = []\nfor _ in range(5):\n    cities.extend(marvin.generate(n=2, instructions='major US cities'))\n</code></pre></p> <p>Result</p> <pre><code>cities == [\n    'Chicago',\n    'San Francisco',\n    'Seattle',\n    'New York City',\n    'Los Angeles',\n    'Houston',\n    'Miami',\n    'Dallas',\n    'Atlanta',\n    'Boston'\n]\n</code></pre> <p>Generate 10 cities in five calls, without the cache. This produces a list with almost no variation, since each call is independent:</p> <pre><code>cities = []\nfor _ in range(5):\n    cities.extend(marvin.generate(\n        n=2, \n        instructions='major US cities', \n        use_cache=False,\n))\n</code></pre> <p>Result</p> <pre><code>cities == [\n    'Houston',\n    'Seattle',\n    'Chicago',\n    'Houston',\n    'Chicago',\n    'Houston',\n    'Chicago',\n    'Houston',\n    'Los Angeles',\n    'Houston'\n]\n</code></pre>"},{"location":"docs/text/generation/#async-support","title":"Async support","text":"<p>If you are using Marvin in an async environment, you can use <code>generate_async</code>:</p> <pre><code>cat_names = await marvin.generate_async(\n    n=4, \n    instructions=\"names for cats inspired by Chance the Rapper\"\n)\n\n# ['Chancey', 'Rappurr', 'Lyric', 'Chano']\n</code></pre>"},{"location":"docs/text/transformation/","title":"Converting text to data","text":"<p>At the heart of Marvin is the ability to convert natural language to native Python types and structured objects. This is one of its simplest but most powerful features, and forms the basis for almost every other tool. </p> <p>The primary tool for creating structured data is the <code>cast</code> function, which takes a natural language string as its input, as well as a type to which the text should be converted.</p> <p>What it does</p> <p>     The <code>cast</code> function transforms natural language text into a Python type or structured object.   </p> <p>Example</p> <pre><code>import marvin\nfrom pydantic import BaseModel\n\nclass Location(BaseModel):\n    city: str\n    state: str\n\nmarvin.cast(\"the big apple\", target=Location)\n</code></pre> <p>Result</p> <pre><code>Location(city=\"New York\", state=\"NY\")\n</code></pre> <p>How it works</p> <p>     Marvin creates a schema from the provided type and instructs the LLM to use the schema to format its JSON response.   </p> <p>     In Python, the JSON representation is hydrated into a \"full\" instance of the type.   </p>"},{"location":"docs/text/transformation/#supported-types","title":"Supported types","text":"<p>The <code>cast</code> function supports conversion almost all builtin Python types, plus Pydantic models and Python's <code>Literal</code>, and <code>TypedDict</code>. When called, the LLM will take all available information into account, performing deductive reasoning if necessary, to determine the best output. The result will be a Python object of the provided type.</p>"},{"location":"docs/text/transformation/#instructions","title":"Instructions","text":"<p>Sometimes the cast operation is obvious, as in the \"big apple\" example above. Other times, it may be more nuanced. In these cases, the LLM may require guidance or examples to make the right decision. You can provide natural language <code>instructions</code> when calling <code>cast()</code> in order to steer the output. </p> <p>In a simple case, instructions can be used independent of any type-casting. Here, we want to keep the output a string, but get the 2-letter abbreviation of the state.</p> <pre><code>marvin.cast('California', target=str, instructions=\"The state's abbreviation\")\n# \"CA\"\n\nmarvin.cast('The sunshine state', target=str, instructions=\"The state's abbreviation\")\n# \"FL\"\n\nmarvin.cast('Mass.', target=str, instructions=\"The state's abbreviation\")\n# MA\n</code></pre> <p>Note that when providing instructions, the <code>target</code> field is assumed to be a string unless otherwise specified. If no instructions are provided, a target type is required.</p>"},{"location":"docs/text/transformation/#classification","title":"Classification","text":"<p>One way of classifying text is by casting it to a constrained type, such as an <code>Enum</code> or <code>bool</code>. This forces the LLM to choose one of the provided options.</p> <p>Marvin provides a dedicated <code>classify</code> function for this purpose. As a convenience, <code>cast</code> will automatically switch to <code>classify</code> when given a constrained target type. However, you may prefer to use the <code>classify</code> function to make your intent more clear to other developers.</p>"},{"location":"docs/text/transformation/#ai-models","title":"AI models","text":"<p>In addition to providing Pydantic models as <code>cast</code> targets, Marvin has a drop-in replacement for Pydantic's <code>BaseModel</code> that permits instantiating the model with natural language. These \"AI Models\" can be created in two different ways:</p> <ol> <li>Decorating a BaseModel with <code>@marvin.model</code>.</li> <li>Subclassing the <code>marvin.Model</code> class</li> </ol> <p>Though these are roughly equivalent, we recommend the decorator as it will make the intent more clear to other developers (in particular, it will not hide that the model is a <code>BaseModel</code>).</p> <p>Here is the class decorator:</p> <pre><code>import marvin\n\n\n@marvin.model\nclass Location:\n    city: str\n    state: str\n\n\nLocation('CHI')\n# Location(city=\"Chicago\", state=\"IL\")\n</code></pre> <p>And here is the equivalent subclass:</p> <pre><code>import marvin\n\n\nclass Location(marvin.Model):\n    city: str\n    state: str\n\n\nLocation('CHI')\n# Location(city=\"Chicago\", state=\"IL\")\n</code></pre>"},{"location":"docs/text/transformation/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> argument of <code>cast</code> or <code>@model</code>. These parameters are passed directly to the API, so you can use any supported parameter.</p>"},{"location":"docs/text/transformation/#instructions_1","title":"Instructions","text":"<p>You can pass instructions to steer model transformation via the <code>instructions</code> parameter:</p> <pre><code>@marvin.model(instructions='Always generate locations in California')\nclass Location(BaseModel):\n    city: str\n    state: str\n\nLocation('a large city')   \n# Location(city='Los Angeles', state='California')\n</code></pre> <p>Note that instructions are set at the class level, so they will apply to all instances of the model. To customize instructions on a per-instance basis, use <code>cast</code> with the <code>instructions</code> parameter instead.</p>"},{"location":"docs/text/transformation/#async-support","title":"Async support","text":"<p>If you are using <code>marvin</code> in an async environment, you can use <code>cast_async</code>:</p> <pre><code>result = await marvin.cast_async(\"one\", int) \n\nassert result == 1\n</code></pre>"},{"location":"docs/text/transformation/#mapping","title":"Mapping","text":"<p>To transform a list of inputs at once, use <code>.map</code>:</p> <pre><code>inputs = [\n    \"I bought two donuts.\",\n    \"I bought six hot dogs.\"\n]\nresult = marvin.cast.map(inputs, int)\nassert result  == [2, 6]\n</code></pre> <p>(<code>marvin.cast_async.map</code> is also available for async environments.)</p> <p>Mapping automatically issues parallel requests to the API, making it a highly efficient way to work with multiple inputs at once. The result is a list of outputs in the same order as the inputs.</p>"},{"location":"docs/video/recording/","title":"Recording video","text":"<p>Marvin has utilities for working with video data beyond generating speech and transcription. To use these utilities, you must install Marvin with the <code>video</code> extra:</p> <pre><code>pip install marvin[video]\n</code></pre>"},{"location":"docs/video/recording/#recording-video_1","title":"Recording video","text":"<p>Marvin can record video from your computer's camera. The result is a stream of <code>Image</code> objects, which can be used any of Marvin's image tools, including captioning, classification, and more.</p>"},{"location":"docs/video/recording/#recording-continuously","title":"Recording continuously","text":"<p>The <code>record_background</code> function records video continuously in the background. This is useful for recording video while doing other tasks or processing the data in real time.</p> <p>The result of <code>record_background</code> is a <code>BackgroundVideoRecorder</code> object, which can be used to control the recording (including stopping it) and to access the recorded video as a stream of images. Images are queued and can be accessed by iterating over the recorder's <code>stream</code> method.</p> <pre><code>import marvin\nimport marvin.video\n\nrecorder = marvin.video.record_background()\n\ncounter = 0\nfor image in recorder.stream():\n    counter += 1\n    # process each image\n    marvin.caption(image)\n\n    # stop recording\n    if counter == 3:\n        recorder.stop()\n</code></pre>"},{"location":"docs/vision/captioning/","title":"Captioning images","text":"<p>Marvin can use OpenAI's vision API to process images as inputs. </p> <p>What it does</p> <p>     The <code>caption</code> function generates text from images.   </p> <p>Example</p> <p>Generate a description of the following image, hypothetically available at <code>/path/to/marvin.png</code>:</p> <p></p> <pre><code>import marvin\n\ncaption = marvin.caption(marvin.Image.from_path('/path/to/marvin.png'))\n</code></pre> <p>Result</p> <p>\"A cute, small robot with a square head and large, glowing eyes sits on a surface of wavy, colorful lines. The background is dark with scattered, glowing particles, creating a magical and futuristic atmosphere.\"</p> <p>How it works</p> <p>     Marvin passes your images to the OpenAI vision API as part of a larger prompt.   </p>"},{"location":"docs/vision/captioning/#providing-instructions","title":"Providing instructions","text":"<p>The <code>instructions</code> parameter offers an additional layer of control, enabling more nuanced caption generation, especially in ambiguous or complex scenarios.</p>"},{"location":"docs/vision/captioning/#captions-for-multiple-images","title":"Captions for multiple images","text":"<p>To generate a single caption for multiple images, pass a list of <code>Image</code> objects to <code>caption</code>:</p> <pre><code>marvin.caption(\n  [\n    marvin.Image.from_path('/path/to/img1.png'),\n    marvin.Image.from_path('/path/to/img2.png')\n  ],\n  instructions='...'\n)\n</code></pre>"},{"location":"docs/vision/captioning/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> argument of <code>caption</code>. These parameters are passed directly to the API, so you can use any supported parameter.</p>"},{"location":"docs/vision/captioning/#async-support","title":"Async support","text":"<p>If you are using Marvin in an async environment, you can use <code>caption_async</code>:</p> <pre><code>caption = await marvin.caption_async(image=Path('/path/to/marvin.png'))\n</code></pre>"},{"location":"docs/vision/captioning/#mapping","title":"Mapping","text":"<p>To generate individual captions for a list of inputs at once, use <code>.map</code>. Note that this is different than generating a single caption for multiple images, which is done by passing a list of <code>Image</code> objects to <code>caption</code>.</p> <pre><code>inputs = [\n    marvin.Image.from_path('/path/to/img1.png'),\n    marvin.Image.from_path('/path/to/img2.png')\n]\nresult = marvin.caption.map(inputs)\nassert len(result) == 2\n</code></pre> <p>(<code>marvin.cast_async.map</code> is also available for async environments.)</p> <p>Mapping automatically issues parallel requests to the API, making it a highly efficient way to work with multiple inputs at once. The result is a list of outputs in the same order as the inputs.</p>"},{"location":"docs/vision/classification/","title":"Classifying images","text":"<p>Marvin can use OpenAI's vision API to process images and classify them into categories.</p> <p>What it does</p> <p>     The <code>classify</code> function can classify images as one of many labels.   </p> <p>How it works</p> <p>    This involves a two-step process: first, a caption is generated for the image that is aligned with the structuring goal. Next, the actual classify operation is performed with an LLM.    </p> <p>Example</p> <p>We will classify the animal in this image, as well as whether it is wet or dry:</p> <p></p> <pre><code>import marvin\n\nimg = marvin.Image('https://upload.wikimedia.org/wikipedia/commons/d/d5/Retriever_in_water.jpg')\n\nanimal = marvin.classify(\n    img, \n    labels=['dog', 'cat', 'bird', 'fish', 'deer']\n)\n\ndry_or_wet = marvin.classify(\n    img, \n    labels=['dry', 'wet'], \n    instructions='Is the animal wet?'\n)\n</code></pre> <p>Result</p> <pre><code>assert animal == 'dog'\nassert dry_or_wet == 'wet'\n</code></pre>"},{"location":"docs/vision/classification/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> argument of <code>classify</code>. These parameters are passed directly to the API, so you can use any supported parameter.</p>"},{"location":"docs/vision/classification/#async-support","title":"Async support","text":"<p>If you are using Marvin in an async environment, you can use <code>classify_async</code>:</p> <pre><code>result = await marvin.classify_async(\n    \"The app crashes when I try to upload a file.\", \n    labels=[\"bug\", \"feature request\", \"inquiry\"]\n) \n\nassert result == \"bug\"\n</code></pre>"},{"location":"docs/vision/classification/#mapping","title":"Mapping","text":"<p>To classify a list of inputs at once, use <code>.map</code>:</p> <pre><code>inputs = [\n    \"The app crashes when I try to upload a file.\",\n    \"How do change my password?\"\n]\nresult = marvin.classify.map(inputs, [\"bug\", \"feature request\", \"inquiry\"])\nassert result == [\"bug\", \"inquiry\"]\n</code></pre> <p>(<code>marvin.classify_async.map</code> is also available for async environments.)</p> <p>Mapping automatically issues parallel requests to the API, making it a highly efficient way to classify multiple inputs at once. The result is a list of classifications in the same order as the inputs.</p>"},{"location":"docs/vision/extraction/","title":"Extracting entities from images","text":"<p>Marvin can use OpenAI's vision API to process images and convert them into structured data, transforming unstructured information into native types that are appropriate for a variety of programmatic use cases.</p> <p>What it does</p> <p>     The beta <code>extract</code> function can extract entities from images and text.   </p> <p>How it works</p> <p>    This involves a two-step process: first, a caption is generated for the image that is aligned with the structuring goal. Next, the actual extract operation is performed with an LLM.    </p> <p>Example: identifying dogs</p> <p>We will extract the breed of each dog in this image:</p> <p></p> <pre><code>import marvin\n\nimg = marvin.Image(\n    \"https://images.unsplash.com/photo-1548199973-03cce0bbc87b?\",\n)\n\nresult = marvin.extract(img, target=str, instructions=\"dog breeds\")\n</code></pre> <p>Result</p> <pre><code>result == [\"Pembroke Welsh Corgi\", \"Yorkshire Terrier\"]\n</code></pre>"},{"location":"docs/vision/extraction/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> argument of <code>extract</code>. These parameters are passed directly to the API, so you can use any supported parameter.</p>"},{"location":"docs/vision/extraction/#async-support","title":"Async support","text":"<p>If you are using Marvin in an async environment, you can use <code>extract_async</code>:</p> <pre><code>result = await marvin.extract_async(\n    \"I drove from New York to California.\",\n    target=str,\n    instructions=\"2-letter state codes\",\n) \n\nassert result == [\"NY\", \"CA\"]\n</code></pre>"},{"location":"docs/vision/extraction/#mapping","title":"Mapping","text":"<p>To extract from a list of inputs at once, use <code>.map</code>:</p> <pre><code>inputs = [\n    \"I drove from New York to California.\",\n    \"I took a flight from NYC to BOS.\"\n]\nresult = marvin.extract.map(inputs, target=str, instructions=\"2-letter state codes\")\nassert result  == [[\"NY\", \"CA\"], [\"NY\", \"MA\"]]\n</code></pre> <p>(<code>marvin.extract_async.map</code> is also available for async environments.)</p> <p>Mapping automatically issues parallel requests to the API, making it a highly efficient way to work with multiple inputs at once. The result is a list of outputs in the same order as the inputs.</p>"},{"location":"docs/vision/transformation/","title":"Converting images to data","text":"<p>Marvin can use OpenAI's vision API to process images and convert them into structured data, transforming unstructured information into native types that are appropriate for a variety of programmatic use cases.</p> <p>What it does</p> <p>     The <code>cast</code> function can cast images to structured types.   </p> <p>How it works</p> <p>    This involves a two-step process: first, a caption is generated for the image that is aligned with the structuring goal. Next, the actual cast operation is performed with an LLM.    </p> <p>Example: locations</p> <p>We will cast this image to a <code>Location</code> type:</p> <p></p> <pre><code>import marvin\nfrom pydantic import BaseModel, Field\n\n\nclass Location(BaseModel):\n    city: str\n    state: str = Field(description=\"2-letter state abbreviation\")\n\n\nimg = marvin.Image(\n    \"https://images.unsplash.com/photo-1568515387631-8b650bbcdb90\",\n)\nresult = marvin.cast(img, target=Location)\n</code></pre> <p>Result</p> <pre><code>assert result == Location(city=\"New York\", state=\"NY\")\n</code></pre> <p>Example: getting information about a book</p> <p>We will cast this image to a <code>Book</code> to extract key information:</p> <p></p> <pre><code>import marvin\nfrom pydantic import BaseModel\n\n\nclass Book(BaseModel):\n    title: str\n    subtitle: str\n    authors: list[str]\n\n\nimg = marvin.Image(\n    \"https://hastie.su.domains/ElemStatLearn/CoverII_small.jpg\",\n)\nresult = marvin.cast(img, target=Book)\n</code></pre> <p>Result</p> <pre><code>assert result == Book(\n    title='The Elements of Statistical Learning',\n    subtitle='Data Mining, Inference, and Prediction',\n    authors=['Trevor Hastie', 'Robert Tibshirani', 'Jerome Friedman']\n)\n</code></pre>"},{"location":"docs/vision/transformation/#instructions","title":"Instructions","text":"<p>If the target type isn't self-documenting, or you want to provide additional guidance, you can provide natural language <code>instructions</code> when calling <code>cast</code> in order to steer the output. </p> <p>Example: checking groceries</p> <p>Let's use this image to see if we got everything on our shopping list:</p> <p></p> <pre><code>import marvin\n\nshopping_list = [\"bagels\", \"cabbage\", \"eggs\", \"apples\", \"oranges\"]\n\nmissing_items = marvin.cast(\n    marvin.Image(\"https://images.unsplash.com/photo-1588964895597-cfccd6e2dbf9\"), \n    target=list[str], \n    instructions=f\"Did I forget anything on my list: {shopping_list}?\",\n)\n</code></pre> <p>Result</p> <pre><code>assert missing_items == [\"eggs\", \"oranges\"]\n</code></pre>"},{"location":"docs/vision/transformation/#model-parameters","title":"Model parameters","text":"<p>You can pass parameters to the underlying API via the <code>model_kwargs</code> argument of <code>cast</code>. These parameters are passed directly to the API, so you can use any supported parameter.</p>"},{"location":"docs/vision/transformation/#async-support","title":"Async support","text":"<p>If you are using <code>marvin</code> in an async environment, you can use <code>cast_async</code>:</p> <pre><code>result = await marvin.cast_async(\"one\", int) \n\nassert result == 1\n</code></pre>"},{"location":"docs/vision/transformation/#mapping","title":"Mapping","text":"<p>To transform a list of inputs at once, use <code>.map</code>:</p> <pre><code>inputs = [\n    \"I bought two donuts.\",\n    \"I bought six hot dogs.\"\n]\nresult = marvin.cast.map(inputs, int)\nassert result  == [2, 6]\n</code></pre> <p>(<code>marvin.cast_async.map</code> is also available for async environments.)</p> <p>Mapping automatically issues parallel requests to the API, making it a highly efficient way to work with multiple inputs at once. The result is a list of outputs in the same order as the inputs.</p>"},{"location":"examples/audio_modification/","title":"Modifying user audio","text":"<p>By combining a few Marvin tools, you can quickly record a user, transcribe their speech, modify it, and play it back.</p> <p>Audio extra</p> <p>This example requires the <code>audio</code> extra to be installed in order to record and play sound:</p> <pre><code>pip install marvin[audio]\n</code></pre> <p>Modifying user audio</p> <pre><code>import marvin\nimport marvin.audio\n\n# record the user\nuser_audio = marvin.audio.record_phrase()\n\n# transcribe the text\nuser_text = marvin.transcribe(user_audio)\n\n# cast the language to a more formal style\nai_text = marvin.cast(\n    user_text, \n    instructions=\"Make the language ridiculously formal\",\n)\n\n# generate AI speech\nai_audio = marvin.speak(ai_text)\n\n# play the result\nai_audio.play()\n</code></pre> <p>User audio</p> <p>\"This is a test.\"</p> <p>      Your browser does not support the audio element. </p> <p>Marvin audio</p> <p>\"This constitutes an examination.\"</p> <p>      Your browser does not support the audio element. </p>"},{"location":"examples/being_specific_about_types/","title":"Fully leveraging <code>pydantic</code>","text":""},{"location":"examples/being_specific_about_types/#annotated-and-field","title":"<code>Annotated</code> and <code>Field</code>","text":"<p>Numbers in a valid range</p> <p>Pydantic's <code>Field</code> lets us be very specific about what we want from the LLM.</p> <pre><code>from typing import Annotated\nimport marvin\nfrom pydantic import Field\nfrom typing_extensions import TypedDict\n\nActivationField = Field(\n    description=(\n        \"A score between -1 (not descriptive) and 1\"\n        \" (very descriptive) for the given emotion\"\n    ),\n    ge=-1,\n    le=1\n)\n\nSentimentActivation = Annotated[float, ActivationField]\n\nclass DetailedSentiment(TypedDict):\n    happy: SentimentActivation\n    sad: SentimentActivation\n    angry: SentimentActivation\n    surprised: SentimentActivation\n    amused: SentimentActivation\n    scared: SentimentActivation\n\n@marvin.fn\ndef sentiment_analysis(text: str) -&gt; DetailedSentiment:\n    \"\"\"Analyze the sentiment of a given text\"\"\"\n\nsentiment_analysis(\n    \"dude i cannot believe how hard that\"\n    \" kangaroo just punched that guy \ud83e\udd23\"\n    \" - he really had it coming, but glad he's ok\"\n)\n</code></pre> <p>Result</p> <pre><code>{\n    'happy': 0.8,\n    'sad': -0.1,\n    'angry': -0.2,\n    'surprised': 0.7,\n    'amused': 1.0,\n    'scared': -0.1\n}\n</code></pre>"},{"location":"examples/being_specific_about_types/#complex-types","title":"Complex types","text":"<p>Using <code>BaseModel</code> and <code>Field</code></p> <p>To parse and validate complex nested types, use <code>BaseModel</code> and <code>Field</code>:</p> <pre><code>import marvin\nfrom pydantic import BaseModel, Field\n\nclass Location(BaseModel):\n    city: str\n    state: str | None = Field(description=\"Two-letter state code\")\n    country: str\n    latitute: float | None = Field(\n        description=\"Latitude in degrees\",\n        ge=-90,\n        le=90\n    )\n    longitude: float | None = Field(\n        description=\"Longitude in degrees\",\n        ge=-180,\n        le=180\n    )\n\nclass Traveler(BaseModel):\n    name: str\n    age: int | None = Field(description=\"Age in years\")\n\nclass Trip(BaseModel):\n    travelers: list[Traveler]\n    origin: Location\n    destination: Location\n\ntrip = marvin.model(Trip)(\n    \"Marvin and Ford are heading from Chi to SF for their 30th birthdays\"\n)\n</code></pre> <p>Result</p> <pre><code>Trip(\n    travelers=[\n        Traveler(name='Marvin', age=30),\n        Traveler(name='Ford', age=30)\n    ],\n    origin=Location(\n        city='Chicago',\n        state='IL',\n        country='USA',\n        latitute=41.8781,\n        longitude=-87.6298\n    ),\n    destination=Location(\n        city='San Francisco',\n        state='CA',\n        country='USA',\n        latitute=37.7749,\n        longitude=-122.4194\n    )\n)\n</code></pre>"},{"location":"examples/call_routing/","title":"Customer call routing","text":"<p>Automatically route customer calls to the right department.</p> <p>Call routing</p> <pre><code>import marvin\nfrom enum import Enum\n\n\nclass Department(Enum):\n    SALES = \"sales\"\n    SUPPORT = \"support\"\n    BILLING = \"billing\"\n\n\n# define a convenience function\ndef route_call(transcript: str) -&gt; Department:\n    return marvin.classify(\n        transcript,\n        labels=Department,\n        instructions=\"Select the best department for the customer request\",\n    )\n</code></pre> <p>\ud83d\udcb3 Update payment method</p> <pre><code>department = route_call(\"I need to update my payment method\")\nassert department == Department.BILLING\n</code></pre> <p>\ud83d\udcb5 Price matching</p> <pre><code>department = route_call(\"Well FooCo offered me a better deal\")\nassert department == Department.SALES\n</code></pre> <p>\ud83e\udd2c Angry noises</p> <pre><code>department = route_call(\"*angry noises*\")\nassert department == Department.SUPPORT\n</code></pre>"},{"location":"examples/deduplication/","title":"Entity Deduplication","text":"How many distinct cities are there in the following text? <pre><code>windy city from illnois, The Windy City, New York City, the Big Apple, SF, San Fran, San Francisco,\n</code></pre> <p>We can look and see the answer is 3, but how can we arrive here programmatically?</p> <p>In this section, we'll explore using <code>marvin</code> to extract entities so we can use them directly in normal Python code.</p>"},{"location":"examples/deduplication/#creating-our-entity","title":"Creating our entity","text":"<p>To extract and deduplicate entities, we'll need to create an entity, i.e. the thing we are looking for.</p> <p>In this case, we're looking for cities, so we'll create a <code>City</code> entity.</p> <pre><code>from pydantic import BaseModel\n\nclass City(BaseModel):\n    informal_name: str\n    standard_name: str\n    state: str | None = None\n    country: str | None = None\n</code></pre> <p>We want to keep its raw name (<code>informal_name</code>) and its official name (<code>standard_name</code>), as well as its state and country, if applicable.</p>"},{"location":"examples/deduplication/#extracting-entities","title":"Extracting entities","text":"<p>Now we can use <code>marvin.extract</code> to get a <code>list[City]</code> from our text.</p> <pre><code>import marvin\nfrom pydantic import BaseModel\n\nclass City(BaseModel):\n    informal_name: str\n    standard_name: str\n    state: str | None = None\n    country: str | None = None\n\ncities = marvin.extract(\n    \"windy city from illnois, The Windy City, New York City, the Big Apple, SF, San Fran, San Francisco\",\n    City,\n    instructions=\"Be sure to identify the origin country of the city if possible.\",\n)\n\nprint(\n    set(total_entities := [city.standard_name for city in cities]),\n    f\" | {len(total_entities)=}\"\n)\n\nprint(\n    \"\\n\"+\"\\n\".join(\n        city.model_dump_json(indent=2)\n        for city in cities\n    )\n)\n</code></pre> Click for the output <pre><code>{'San Francisco', 'New York', 'Chicago'}  | len(total_entities)=7\n\n{\n    \"informal_name\": \"Windy City\",\n    \"standard_name\": \"Chicago\",\n    \"state\": \"Illinois\",\n    \"country\": \"United States\"\n}\n{\n    \"informal_name\": \"The Windy City\",\n    \"standard_name\": \"Chicago\",\n    \"state\": \"Illinois\",\n    \"country\": \"United States\"\n}\n{\n    \"informal_name\": \"New York City\",\n    \"standard_name\": \"New York\",\n    \"state\": \"New York\",\n    \"country\": \"United States\"\n}\n{\n    \"informal_name\": \"The Big Apple\",\n    \"standard_name\": \"New York\",\n    \"state\": \"New York\",\n    \"country\": \"United States\"\n}\n{\n    \"informal_name\": \"SF\",\n    \"standard_name\": \"San Francisco\",\n    \"state\": \"California\",\n    \"country\": \"United States\"\n}\n{\n    \"informal_name\": \"San Fran\",\n    \"standard_name\": \"San Francisco\",\n    \"state\": \"California\",\n    \"country\": \"United States\"\n}\n{\n    \"informal_name\": \"San Francisco\",\n    \"standard_name\": \"San Francisco\",\n    \"state\": \"California\",\n    \"country\": \"United States\"\n}\n</code></pre>"},{"location":"examples/python_augmented_prompts/","title":"Augmenting prompts with Python","text":""},{"location":"examples/python_augmented_prompts/#web-scraping","title":"Web scraping","text":"<p>Fetch rich prompt material with Python</p> <p>Using an http client to fetch HTML that an LLM will filter for a <code>list[RelatedArticle]</code>:</p> <pre><code>import bs4\nimport httpx\nimport marvin\nfrom typing_extensions import TypedDict\n\nclass RelatedArticle(TypedDict):\n    title: str\n    link: str\n\n\n@marvin.fn\ndef retrieve_HN_articles(topic: str | None = None) -&gt; list[RelatedArticle]:\n    \"\"\"Retrieve only articles from HN that are related to a given topic\"\"\"\n    response = httpx.get(\"https://news.ycombinator.com/\")\n    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n    return [\n        (link.text, link['href']) for link in soup.select('.titleline a')\n    ]\n\nretrieve_HN_articles(\"rust\")\n</code></pre> <p>Result</p> <pre><code>[\n    {\n        'title': 'A lowering strategy for control effects in Rust',\n        'link': 'https://www.abubalay.com/blog/2024/01/14/rust-effect-lowering'\n    },\n    {\n        'title': 'Show HN: A minimal working Rust / SDL2 / WASM browser game',\n        'link': 'https://github.com/awwsmm/hello-rust-sdl2-wasm'\n    }\n]\n</code></pre> <p>Note</p> <p>You could also use <code>marvin.extract</code> to extract the <code>list[RelatedArticle]</code> from the output of the un-decorated function <code>retrieve_HN_articles</code>:</p> <pre><code>related_articles = marvin.extract(retrieve_HN_articles(), RelatedArticle)\n</code></pre>"},{"location":"examples/python_augmented_prompts/#vectorstore-based-rag","title":"Vectorstore-based RAG","text":"<p>Stuff <code>top k</code> document excerpts into a prompt</p> <p>Using an http client to fetch HTML that an LLM will filter for a <code>list[RelatedArticle]</code>:</p> <pre><code>from typing_extensions import TypedDict\nimport marvin\nfrom marvin.tools.chroma import query_chroma # you must have a vectorstore with embedded documents\nfrom marvin.utilities.asyncio import run_sync\n\nclass Answer(TypedDict):\n    answer: str\n    supporting_links: list[str] | None\n\n@marvin.fn\ndef answer_question(\n    question: str,\n    top_k: int = 2,\n    style: str = \"concise\"\n) -&gt; Answer:\n    \"\"\"Answer a question given supporting context in the requested style\"\"\"\n    return run_sync(query_chroma(question, n_results=top_k))\n\nanswer_question(\"What are prefect blocks?\", style=\"pirate\")\n</code></pre> <p>Result</p> <pre><code>{\n    'answer': \"Ahoy! Prefect blocks be a primitive within Prefect fer storin' configuration and interfacin' with th' external systems. Ye can use 'em to manage credentials and interact with services like AWS, GitHub, and Slack. Arr, they be comin' with methods for uploadin' or downloadin' data, among other actions, and ye can register new ones with Prefect Cloud or server.\",\n    'supporting_links': ['https://docs.prefect.io/latest/concepts/blocks/']\n}\n</code></pre>"},{"location":"examples/slackbot/","title":"Build a Slack bot with Marvin","text":""},{"location":"examples/slackbot/#slack-setup","title":"Slack setup","text":"<p>Get a Slack app token from Slack API and add it to your <code>~/.marvin/.env</code> file:</p> <pre><code>MARVIN_SLACK_API_TOKEN=your-slack-bot-token\n</code></pre> <p>Choosing scopes</p> <p>You can choose the scopes you need for your bot in the OAuth &amp; Permissions section of your Slack app.</p>"},{"location":"examples/slackbot/#building-the-bot","title":"Building the bot","text":""},{"location":"examples/slackbot/#define-a-fastapi-app-to-handle-slack-events","title":"Define a FastAPI app to handle Slack events","text":"<p><pre><code>@app.post(\"/chat\")\nasync def chat_endpoint(request: Request):\n    payload = SlackPayload(**await request.json())\n    match payload.type:\n        case \"event_callback\":\n            asyncio.create_task(handle_message(payload))\n        case \"url_verification\":\n            return {\"challenge\": payload.challenge}\n        case _:\n            raise HTTPException(400, \"Invalid event type\")\n\n    return {\"status\": \"ok\"}\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=4200)\n</code></pre> Here, we define a simple FastAPI endpoint / app to handle Slack events and return a response. We run our interesting logic in the background using <code>asyncio.create_task</code> to make sure we return <code>{\"status\": \"ok\"}</code> within 3 seconds, as required by Slack.</p>"},{"location":"examples/slackbot/#handle-generating-the-ai-response","title":"Handle generating the AI response","text":"<p>I like to start with this basic structure, knowing that one way or another...</p> <pre><code>async def handle_message(payload: dict) -&gt; str:\n    # somehow generate the ai responses\n    ...\n\n    # post the response to slack\n    _post_message(\n        messsage=some_message_ive_constructed,\n        channel=event.get(\"channel\", \"\"),\n        thread_ts=thread_ts,\n    )\n</code></pre> <p>... I need to take in a Slack app mention payload, generate a response, and post it back to Slack.</p>"},{"location":"examples/slackbot/#a-couple-considerations","title":"A couple considerations","text":"<ul> <li>do I want the bot to respond to users in a thread or in the channel?</li> <li>do I want the bot to have memory of previous messages? how so?</li> <li>what tools do I need to generate accurate responses for my users?</li> </ul> <p>In our case of the Prefect Community slackbot, we want:</p> <ul> <li>the bot to respond in a thread</li> <li>the bot to have memory of previous messages by slack thread</li> <li>the bot to have access to the internet, GitHub, embedded docs, etc</li> </ul>"},{"location":"examples/slackbot/#example-implementation-of-handler-prefect-community-slackbot","title":"Example implementation of handler: Prefect Community Slackbot","text":"<p>This runs 24/7 in the #ask-marvin channel of the Prefect Community Slack. It responds to users in a thread, and has memory of previous messages by slack thread. It uses the <code>chroma</code> and <code>github</code> tools for RAG to answer questions about Prefect 2.x.</p> <pre><code>async def handle_message(payload: SlackPayload): # SlackPayload is a pydantic model \n    logger = get_logger(\"slackbot\")\n    user_message = (event := payload.event).text\n    cleaned_message = re.sub(BOT_MENTION, \"\", user_message).strip()\n    logger.debug_kv(\"Handling slack message\", user_message, \"green\")\n    if (user := re.search(BOT_MENTION, user_message)) and user.group(\n        1\n    ) == payload.authorizations[0].user_id:\n        thread = event.thread_ts or event.ts\n        assistant_thread = CACHE.get(thread, Thread())\n        CACHE[thread] = assistant_thread\n\n        await handle_keywords.submit(\n            message=cleaned_message,\n            channel_name=await get_channel_name(event.channel),\n            asking_user=event.user,\n            link=(  # to user's message\n                f\"{(await get_workspace_info()).get('url')}archives/\"\n                f\"{event.channel}/p{event.ts.replace('.', '')}\"\n            ),\n        )\n\n        with Assistant(\n            name=\"Marvin (from Hitchhiker's Guide to the Galaxy)\",\n            tools=[task(multi_query_chroma), task(search_github_issues)],\n            instructions=(\n                \"use chroma to search docs and github to search\"\n                \" issues and answer questions about prefect 2.x.\"\n                \" you must use your tools in all cases except where\"\n                \" the user simply wants to converse with you.\"\n            ),\n        ) as assistant:\n            user_thread_message = await assistant_thread.add_async(cleaned_message)\n            await assistant_thread.run_async(assistant)\n            ai_messages = assistant_thread.get_messages(\n                after_message=user_thread_message.id\n            )\n            await task(post_slack_message)(\n                ai_response_text := \"\\n\\n\".join(\n                    m.content[0].text.value for m in ai_messages\n                ),\n                channel := event.channel,\n                thread,\n            )\n            logger.debug_kv(\n                success_msg := f\"Responded in {channel}/{thread}\",\n                ai_response_text,\n                \"green\",\n            )\n</code></pre> <p>This is just an example</p> <p>There are many ways to implement a Slackbot with Marvin's Assistant SDK / utils, FastAPI is just our favorite.</p> <p>Run this file with something like: <pre><code>python start.py\n</code></pre></p> <p>... and navigate to <code>http://localhost:4200/docs</code> to see your bot's docs.</p> <p>This is now an endpoint that can be used as a Slack event handler. You can use a tool like ngrok to expose your local server to the internet and use it as a Slack event handler.</p>"},{"location":"examples/slackbot/#building-an-image","title":"Building an image","text":"<p>Based on this example, one could write a <code>Dockerfile</code> to build a deployable image:</p> <p><pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY . /app\n\nRUN python -m venv venv\nENV VIRTUAL_ENV=/app/venv\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y git build-essential &amp;&amp; \\\n    apt-get clean &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\nRUN pip install \".[slackbot]\"\n\nEXPOSE 4200\n\nCMD [\"python\", \"cookbook/slackbot/start.py\"]\n</code></pre> Note that we're installing the <code>slackbot</code> extras here, which are required for tools used by the worker bot defined in this example's <code>cookbook/slackbot/start.py</code> file.</p>"},{"location":"examples/slackbot/#find-the-whole-example-here","title":"Find the whole example here","text":"<ul> <li>cookbook/slackbot/start.py</li> </ul>"},{"location":"examples/webcam_narration/","title":"Live webcam narration","text":"<p>By combining a few Marvin tools, you can quickly create a live narration of your webcam feed. This example extracts frames from the webcam at regular interval, generates a narrative, and speaks it out loud.</p> <p>Video and audio extras</p> <p>This example requires the <code>audio</code> and <code>video</code> extras to be installed in order to record video and play sound:</p> <pre><code>pip install marvin[audio,video]\n</code></pre> <p>Webcam narrator</p> <pre><code>import marvin\nimport marvin.audio\nimport marvin.video\n\n# keep a narrative history\nhistory = []\nframes = []\n\n# begin recording the webcam\nrecorder = marvin.video.record_background()\n\n# iterate over each frame\nfor frame in recorder.stream():\n\n    frames.append(frame)\n\n    # if there are no more frames to process, generate a caption from the most recent 5\n    if len(recorder) == 0:\n        caption = marvin.caption(\n            frames[-5:],\n            instructions=f\"\"\"\n                You are a parody of a nature documentary narrator, creating an\n                engrossing story from a webcam feed. Here are a few frames from\n                that feed; use them to generate a few sentences to continue your\n                narrative.\n\n                Here is what you've said so far, so you can build a consistent\n                and humorous narrative:\n\n                {' '.join(history[-10:])}\n                \"\"\",\n        )\n        history.append(caption)\n        frames.clear()\n\n        # generate speech for the caption         \n        audio = marvin.speak(caption)\n\n        # play the audio\n        audio.play()\n</code></pre>"},{"location":"examples/xkcd_bird/","title":"xkcd bird classifier","text":"<p>Is this a bird?</p> <p> <pre><code>import marvin\n\nphoto = marvin.Image(\n    \"https://images.unsplash.com/photo-1613891188927-14c2774fb8d7\",\n)\n\nresult = marvin.classify(\n    photo,\n    labels=[\"bird\", \"not bird\"]\n)\n</code></pre></p> <p>Yes!</p> <pre><code>assert result == \"bird\"\n</code></pre>"},{"location":"examples/hogwarts_sorting_hat/hogwarts_sorting_hat/","title":"Hogwarts sorting hat","text":"<p>Hogwarts sorting hat</p> <pre><code>import marvin\n\nstudent = \"Brave, daring, chivalrous, and sometimes a bit reckless.\"\n\nhouse = marvin.classify(\n    student,\n    labels=[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n)\n</code></pre> <p>Welcome to Gryffindor!</p> <pre><code>assert house == \"Gryffindor\"\n</code></pre>"},{"location":"examples/michael_scott_business/michael_scott_business/","title":"Michael Scott's four kinds of businesses","text":"<p>What kind of business are LLMs?</p> <pre><code>import marvin\n\nbusinesses = [\n    \"tourism\",\n    \"food service\",\n    \"railroads\",\n    \"sales\",\n    \"hospitals/manufacturing\",\n    \"air travel\",\n]\n\nresult = marvin.classify(\"LLMs\", labels=businesses)\n</code></pre> <p>Tourism</p> <pre><code>assert result == \"tourism\"\n</code></pre>"},{"location":"help/legacy_docs/","title":"Legacy Documentation","text":"<p>If you want to view documentation of previous versions of Marvin, here's a step by step guide on how to do that.</p>"},{"location":"help/legacy_docs/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>Git</code></li> <li><code>python3</code></li> <li><code>pip</code></li> </ul>"},{"location":"help/legacy_docs/#automated-script-for-legacy-documentation","title":"Automated Script for Legacy Documentation","text":"<p>To build and view the docs for a specific version of Marvin, you can use this script.</p> <p>You can either clone the Marvin repo and run the script locally, or copy the script and run it directly in your terminal after making it executable: <pre><code># unix\nchmod +x scripts/serve_legacy_docs\n\n# run the script (default version is v1.5.6)\n./scripts/serve_legacy_docs\n\n# optionally, specify a version\n./scripts/serve_legacy_docs v1.5.3\n</code></pre></p>"},{"location":"help/legacy_docs/#manual-steps","title":"Manual Steps","text":"<p>If you prefer to manually perform the steps or need to tailor them for your specific operating system, follow these instructions:</p> <ol> <li> <p>Clone the Repository    Clone the Marvin repository using Git:    <pre><code>git clone https://github.com/PrefectHQ/marvin.git\ncd marvin\n</code></pre></p> </li> <li> <p>Checkout the Specific Tag    Checkout the tag for the version you are interested in. Replace <code>v1.5.6</code> with the desired version tag:    <pre><code>git fetch --tags\ngit checkout tags/v1.5.6\n</code></pre></p> </li> <li> <p>Create a Virtual Environment    Create and activate a virtual environment to isolate the dependency installation:    <pre><code>python3 -m venv venv\nsource venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n</code></pre></p> </li> <li> <p>Install Dependencies    Install the necessary dependencies for the documentation:    <pre><code>pip install -e \".[dev,docs]\"\n</code></pre></p> </li> <li> <p>Serve the Documentation Locally    Use <code>mkdocs</code> to serve the documentation:    <pre><code>mkdocs serve\n</code></pre>    This will start a local server. View the documentation by navigating to <code>http://localhost:8000</code> in your web browser.</p> </li> <li> <p>Exit Virtual Environment    Once finished, you can exit the virtual environment:    <pre><code>deactivate\n</code></pre></p> </li> </ol> <p>Optionally, you can remove the virtual environment folder:    <pre><code>rm -rf venv\n</code></pre></p>"},{"location":"welcome/installation/","title":"Installing Marvin","text":"<p>Install Marvin with <code>pip</code>:</p> <pre><code>pip install marvin\n</code></pre> <p>To verify your installation, run <code>marvin version</code> in your terminal.</p> <p>Upgrade to the latest released version at any time:</p> <pre><code>pip install marvin -U\n</code></pre> <p>Next, check out the tutorial to get started with Marvin.</p>"},{"location":"welcome/installation/#requirements","title":"Requirements","text":"<p>Marvin requires Python 3.9 or greater, and is tested on all major Python versions and operating systems.</p>"},{"location":"welcome/installation/#optional-dependencies","title":"Optional dependencies","text":"<p>Marvin has a few features that have additional dependencies that are not installed by default. If you want to use these features, you can install the optional dependencies with the following commands:</p>"},{"location":"welcome/installation/#audio-features","title":"Audio features","text":"<p>Marvin can transcribe and generate speech out-of-the box by working with audio files, but in order to record and play sound, you'll need additional dependencies. See the documentation for more details.</p> <p>Please follow these instructions to set up the prerequisites for PyAudio and PyDub. </p>"},{"location":"welcome/installation/#set-up-pyaudio-dependencies","title":"Set up PyAudio dependencies","text":"<p>Marvin's audio features depend on PyAudio, which may have additional platform-dependent instructions. Please review the PyAudio installation instructions here for the latest information.</p> <p>On macOS, PyAudio depends on PortAudio, which can be installed with Homebrew:</p> <pre><code>brew install portaudio\n</code></pre>"},{"location":"welcome/installation/#set-up-pydub-dependencies","title":"Set up PyDub dependencies","text":"<p>Marvin's audio features also depend on PyDub, which may have additional platform-dependent instructions. Please review the PyDub installation instructions here.</p> <p>Generally, you'll need to install ffmpeg.</p> <p>On macOS, use Homebrew:</p> <pre><code>brew install ffmpeg\n</code></pre> <p>On Linux, use your package manager:</p> <pre><code>apt-get install ffmpeg libavcodec-extra\n</code></pre> <p>On Windows, see the PyDub instructions.</p>"},{"location":"welcome/installation/#install-marvin","title":"Install Marvin","text":"<p>Now you can install Marvin with the audio extras, which will also install PyAudio and PyDub:</p> <pre><code>pip install marvin[audio]\n</code></pre>"},{"location":"welcome/installation/#video-features","title":"Video features","text":"<p>Marvin has utilities for recording video that make it easy to apply vision AI models to video streams. See the documentation for more details.</p> <pre><code>pip install marvin[video]\n</code></pre>"},{"location":"welcome/installation/#development","title":"Development","text":"<p>Generally, to install Marvin for development, you'll need to use the <code>dev</code> extra. However, in practice you'll want to create an editable install from your local source code:</p> <pre><code>pip install -e \"path/to/marvin[dev]\"\n</code></pre> <p>To build the documentation, you may also have to install certain imaging dependencies of MkDocs Material, which you can learn more about here.</p> <p>See the contributing docs for further instructions.</p>"},{"location":"welcome/overview/","title":"Marvin Documentation","text":"<p>Marvin is a Python library that lets you use Large Language Models by writing code, not prompts. It's open source, free to use, rigorously type-hinted, used by thousands of engineers, and built by the engineering team at Prefect.</p> <p>Marvin is lightweight and is built for incremental adoption. You can use it purely as a serialization library and bring your own stack, or fully use its engine to work with OpenAI and other providers. </p> How Marvin feels Structured Data ExtractionText ClassificationBusiness Logic <p>Marvin exposes a number of high level components to simplify working with AI. </p> <p><pre><code>import marvin\nfrom pydantic import BaseModel\n\nclass Location(BaseModel):\n    city: str\n    state: str\n    latitude: float\n    longitude: float\n\nmarvin.model(Location)(\"They say they're from the Windy City!\")\n# Location(city='Chicago', state='Illinois', latitude=41.8781, longitude=-87.6298)\n</code></pre> Notice there's no code written, just the expected types. Marvin's components turn your function into a prompt, uses AI to get its most likely output, and parses its response.</p> <p>Marvin exposes a number of high level components to simplify working with AI. </p> <p><pre><code>from marvin import classifier\nfrom typing import Literal\n\n@classifier\ndef customer_intent(text: str) -&gt; Literal['Store Hours', 'Pharmacy', 'Returns']:\n    \"\"\"Classifies incoming customer intent\"\"\"\n\ncustomer_intent(\"I need to pick up my prescription\") # \"Pharmacy\"\n</code></pre> Notice <code>customer_intent</code> has no code. Marvin's components turn your function into a prompt, ask AI for its most likely output, and parses its response.</p> <p>Marvin exposes a number of high level components to simplify working with AI. </p> <p><pre><code>import marvin\n\n@marvin.fn\ndef list_fruits(n: int, color: str = 'red') -&gt; list[str]:\n    \"\"\"Generates a list of {{ n }} {{ color }} fruits\"\"\"\n\nlist_fruits(3) # \"['Apple', 'Cherry', 'Strawberry']\"\n</code></pre> Notice <code>list_fruits</code> has no code. Marvin's components turn your function into a prompt, ask AI for its most likely output, and parses its response.</p> <p>Learning Marvin</p> <p>If you know Python, you already know Marvin. There are no fancy abstractions, just a handful of low-level, customizable decorators  to give your existing code superpowers and a number of utilities that make your life as an AI Engineer easier no matter what framework you use. </p> Sections Description Configuration Details on setting up Marvin and configuring various aspects of its behavior AI Components Documentation for Marvin's familiar, Pythonic interfaces to AI-powered functionality. API Utilities Low level API for building prompts and calling LLMs Examples Deeper dives into how to use Marvin"},{"location":"welcome/tutorial/","title":"Tutorial","text":""},{"location":"welcome/tutorial/#installing-marvin","title":"Installing Marvin","text":"<p>Before we can start, you'll need to install Marvin. Come back here when you're done!</p> <p>(Spoiler alert: run <code>pip install marvin -U</code>.)</p>"},{"location":"welcome/tutorial/#getting-an-openai-api-key","title":"Getting an OpenAI API Key","text":"<p>Marvin uses OpenAI models to power all of its tools. In order to use Marvin, you'll need an OpenAI API key.</p> <p>You can create an API key on the OpenAI platform. Once you've created it, set it as an environment variable called <code>OPENAI_API_KEY</code> (for any application on your machine to use) or <code>MARVIN_OPENAI_API_KEY</code> (if you only want Marvin to use it). In addition to setting it in your terminal, you can also write the variable to a dotenv file at <code>~/.marvin/.env</code>.</p> <p>For quick use, you can also pass your API key directly to Marvin at runtime. We do NOT recommend this for production:</p> <pre><code>import marvin\nmarvin.settings.openai.api_key = 'YOUR_API_KEY'\n</code></pre>"},{"location":"welcome/tutorial/#working-with-text","title":"Working with text","text":"<p>Marvin has a variety of tools that let you use LLMs to solve common but complex problems. In this tutorial, we'll try out a few of them to get a feel for how Marvin works. By the end of the tutorial, you'll have tried some of Marvin's advanced features and be ready to take on the universe! Just don't forget your towel.</p>"},{"location":"welcome/tutorial/#classification","title":"\ud83c\udff7\ufe0f Classification","text":"<p>Classification is one of Marvin's most straightforward features. Given some text and a list of labels, Marvin will choose the label that best fits the text. The <code>classify</code> function is great for tasks like sentiment analysis, intent classification, routing, and more.</p> <p>First steps: true/false</p> <p>Here is the simplest possible classification example, mapping the word \"yes\" to the boolean values <code>True</code> or <code>False</code>:</p> <pre><code>import marvin\n\nresult = marvin.classify(\"yes\", labels=bool)\n</code></pre> <p>Result</p> <pre><code>assert result is True\n</code></pre> <p>A more practical example: sentiment</p> <p>A more useful example is to classify text as one of several categories, provided as a list of labels. In this example, we build a basic sentiment classifier for any text:</p> <pre><code>import marvin\n\nresult = marvin.classify(\n    \"Marvin is so easy to use!\",\n    labels=[\"positive\", \"negative\", \"meh\"],\n)\n</code></pre> <p>Result</p> <pre><code>assert result == \"positive\"\n</code></pre> <p>This is a great example of how all Marvin tools should feel. Historically, classifying text was a major challenge for natural language processing frameworks. But with Marvin, it's as easy as calling a function.</p> <p>Structured labels</p> <p>For the <code>classify</code> function, you can supply labels as a <code>list</code> of labels, a <code>bool</code> type, an <code>Enum</code> class, or a <code>Literal</code> type. This gives you many options for returning structured (non-string) labels.</p>"},{"location":"welcome/tutorial/#transformation","title":"\ud83e\ude84 Transformation","text":"<p>Classification maps text to a single label, but what if you want to convert text to a more structured form? Marvin's <code>cast</code> function lets you do just that. Given some text and a target type, Marvin will return a structured representation of the text.</p> <p>Standardization</p> <p>Suppose you ran a survey and one of the questions asked where people live. Marvin can convert their freeform responses to a structured <code>Location</code> type:</p> <pre><code>import marvin\nfrom pydantic import BaseModel\n\nclass Location(BaseModel):\n    city: str\n    state: str\n\nlocation = marvin.cast(\"NYC\", target=Location)\n</code></pre> <p>Result</p> <p>The string \"NYC\" was converted to a full <code>Location</code> object:</p> <pre><code>assert location == Location(city=\"New York\", state=\"New York\")\n</code></pre>"},{"location":"welcome/tutorial/#instructions","title":"Instructions","text":"<p>All Marvin functions have an <code>instructions</code> parameter that let you fine-tune their behavior with natural language. For example, you can use instructions to tell Marvin to extract a specific type of information from a text or to format a response in a specific way.</p> <p>Suppose you wanted to standardize the survey responses in the previous example, but instead of using a full Pydantic model, you wanted the result to still be a string. The <code>cast</code> function will accept <code>target=str</code>, but that's so general it's unlikely to do what you want without additional guidance. That's where instructions come in:</p> <p>Instructions</p> <p>Repeat the previous example, but cast to a string according to the instructions:</p> <pre><code>import marvin\n\nlocation = marvin.cast(\n    \"NYC\", \n    target=str, \n    instructions=\"Return the proper city and state name\",\n)\n</code></pre> <p>Result</p> <p>The result is a string that complies with the instructions:</p> <pre><code>assert location == \"New York, New York\"\n</code></pre>"},{"location":"welcome/tutorial/#extraction","title":"\ud83d\udd0d Extraction","text":"<p>The <code>extract</code> function is like a generalization of the <code>cast</code> function: instead of transforming the entire text to a single target type, it extracts a list of entities from the text. This is useful for identifying people, places, ratings, keywords, and more.</p> <p>Feature extraction</p> <p>Suppose you wanted to extract the product features mentioned in a review:</p> <pre><code>import marvin\n\nfeatures = marvin.extract(\n    \"I love my new phone's camera, but the battery life could be improved.\",\n    instructions=\"extract product features\",\n)\n</code></pre> <p>Result</p> <pre><code>assert features == [\"camera\", \"battery life\"]\n</code></pre> <p>The <code>extract</code> function can take a target type, just like <code>cast</code>. This lets you extract structured entities from text. For example, you could extract a list of <code>Location</code> objects from a text:</p> <p>Location extraction</p> <pre><code>import marvin\nfrom pydantic import BaseModel\n\nclass Location(BaseModel):\n    city: str\n    state: str\n\nlocations = marvin.extract(\n    \"They've got a game in NY, then they go to DC before Los Angeles.\",\n    target=Location\n)\n</code></pre> <p>Result</p> <pre><code>assert locations == [\n    Location(city=\"New York\", state=\"New York\"),\n    Location(city=\"Washington\", state=\"District of Columbia\"),\n    Location(city=\"Los Angeles\", state=\"California\"),\n]\n</code></pre>"},{"location":"welcome/tutorial/#generation","title":"\u2728 Generation","text":"<p>So far, we've used Marvin to take existing text and convert it to a more structured or modified form that preserves its content but makes it easier to work with. Marvin can also generate synthetic data from a schema or instructions. This is incredibly useful for ideation, testing, data augmentation, populating databases, and more.</p> <p>Let's use Marvin's <code>generate</code> function to produce synthetic data. The <code>generate</code> function takes either a target type or natural language instructions (or both), as well as the number of items to generate, and returns a list of synthetic data that complies with the instructions.</p> <p>Locations named after presidents</p> <p>Earlier, we extracted locations from text. Now, let's generate some new locations:</p> <pre><code>import marvin\nfrom pydantic import BaseModel\n\nclass Location(BaseModel):\n    city: str\n    state: str\n\nlocations = marvin.generate(\n    n=4,\n    target=Location,\n    instructions=\"US cities named after presidents\",\n)\n</code></pre> <p>Result</p> <p>(Note: your results may vary)</p> <pre><code>locations == [\n    Location(city=\"Washington\", state=\"DC\"),\n    Location(city=\"Jackson\", state=\"MS\"),\n    Location(city=\"Lincoln\", state=\"NE\"),\n    Location(city=\"Cleveland\", state=\"OH\"),\n]\n</code></pre> <p>In addition to structured types, Marvin can generate new text from instructions.</p> <p>Character names</p> <p>Let's generate some character names for a role-playing game:</p> <pre><code>import marvin\n\nnames = marvin.generate(\n    n=5,\n    instructions=\"Character names for a fantasy RPG\",\n)\n</code></pre> <p>Result</p> <p>(Note: your results may vary)</p> <pre><code>names = [\n    \"Aelar Galanodel\",\n    \"Draka Steelshadow\",\n    \"Elyndra Silvershade\",\n    \"Brom Ironfist\",\n    \"Thalia Windwhisper\"\n]\n</code></pre>"},{"location":"welcome/tutorial/#ai-functions","title":"\ud83e\uddbe AI Functions","text":"<p>Now you've seen Marvin's most common tools. But what if you want to do something more custom? That's where AI functions come in. AI functions let you combine any inputs, instructions, and output types to create custom AI-powered behaviors.</p> <p>Marvin functions look just like regular Python functions, but notice that they don't have any source code. When you call these functions, the outputs are generated by an LLM on-demand. This means they can handle really complex tasks that even an LLM wouldn't know how to generate code for.</p> <p>Sentiment analysis</p> <p>Let's build a sentiment analysis function that takes text and returns a sentiment score. Normally, this would require a complex model and a lot of training data. But with Marvin, we only have to write the form of the function, and the AI takes care of the rest:</p> <pre><code>import marvin\n\n@marvin.fn\ndef sentiment(text: str) -&gt; float:\n    \"\"\"\n    Returns a sentiment score for `text` on a \n    scale of -1.0 (negative) to 1.0 (positive)\n    \"\"\"\n</code></pre> <p>Result</p> <p>Call the function to see its result:</p> <pre><code>sentiment(\"I love Marvin!\") # 0.8\nsentiment(\"This example could use some work...\") # -0.2\n</code></pre> <p>Marvin's AI functions are especially useful when you want to map a complex set of inputs to an output, usually involving some kind of natural language processing. For typed transformations or data generation, you may prefer to use a different Marvin tool, which have the real advantage of not looking \"odd\" to other Python developers. But for full control and conditional behaviors, AI functions are the way to go.</p>"},{"location":"welcome/tutorial/#working-with-images","title":"Working with images","text":"<p>Marvin is multi-modal! In addition to text, Marvin can also work with images. </p>"},{"location":"welcome/tutorial/#generation_1","title":"\ud83c\udfa8 Generation","text":"<p>Marvin gives you easy access to the DALL-E 3 image generation model. This model can generate images from text descriptions.</p> <p>Generating images</p> <pre><code>import marvin\n\nmarvin.paint(\"a simple cup of coffee, still warm\")\n</code></pre> <p>Result</p> <p>( Note: your results may vary)</p> <p></p>"},{"location":"welcome/tutorial/#captioning","title":"\ud83d\udcdd Captioning","text":"<p>If you've already got an image, you can convert it to text using the <code>caption</code> function. Note the use of Marvin's <code>Image</code> type, which accepts either a local path to an image or a URL.</p> <p>Captioning images</p> <pre><code>import marvin\n\ncaption = marvin.caption(marvin.Image.from_path(\"path/to/coffee.png\"))\n</code></pre> <p>Result</p> <p>(Note: your results may vary)</p> <pre><code>caption == \"\"\"\n    A ceramic cup of hot beverage with steam rising \n    from it, on a rustic wooden surface, backlit by \n    soft light coming in through a window.\n    \"\"\"\n</code></pre>"},{"location":"welcome/tutorial/#transformation-classification-and-extraction","title":"\ud83d\ude80  Transformation, classification, and extraction","text":"<p>Now that you've seen how Marvin can turn images into text, you're probably wondering if we can use that text with the <code>cast</code>, <code>extract</code>, and <code>classify</code> functions we saw earlier. The answer is yes -- but we can do even better.</p> <p>The trouble with passing a caption to Marvin's other functions is that the captioned text might not include details that are relevant for the text processing task you want to perform. For example, if you want to classify the breed of dog in an image, you're going to need very specific information that a generic caption might not provide.</p> <p>Fortunately, the <code>cast</code>, <code>extract</code>, and <code>classify</code> functions can all accept images as inputs! Instead of generating generic captions, these functions process the image directly for the task at hand. You can even provide a list of images, or mix images and text, for total flexibility.</p> <p>Identifying dog breeds in an image</p> <p>Let's identify the breed of each dog in this image by using the beta <code>extract</code> function.</p> <p></p> <pre><code>import marvin\n\nimg = marvin.Image(\n    'https://images.unsplash.com/photo-1548199973-03cce0bbc87b',\n)\n\nresult = marvin.extract(img, target=str, instructions='dog breeds')\n</code></pre> <p>Result</p> <pre><code>result == ['Pembroke Welsh Corgi', 'Yorkshire Terrier']\n</code></pre>"},{"location":"welcome/tutorial/#grab-your-towel","title":"Grab your towel","text":"<p>We hope this tutorial has given you a taste of what Marvin can do. There's a lot more to explore, including tools for interactive use cases (like chatbots and applications), audio generation, and more.</p> <p>To learn more, please explore the docs or say hi in our Discord community!</p> <p>And remember:</p> <p>Don't panic!</p> <pre><code>import marvin\n\naudio = marvin.speak(\"and above all else... don't panic!\")\naudio.save(\"dont_panic.mp3\")\n</code></pre> <p>Result</p> <p>      Your browser does not support the audio element. </p>"},{"location":"welcome/what_is_marvin/","title":"What is Marvin?","text":"<p>Marvin is a lightweight AI toolkit for building natural language interfaces that are reliable, scalable, and easy to trust.</p> <p>Each of Marvin's tools is simple and self-documenting, using AI to solve common but complex challenges like entity extraction, classification, and generating synthetic data. Each tool is independent and incrementally adoptable, so you can use them on their own or in combination with any other library. Marvin is also multi-modal, supporting both image and audio generation as well using images as inputs for extraction and classification.</p> <p>Marvin is for developers who care more about using AI than building AI, and we are focused on creating an exceptional developer experience. Marvin users should feel empowered to bring tightly-scoped \"AI magic\" into any traditional software project with just a few extra lines of code.</p> <p>Marvin aims to merge the best practices for building dependable, observable software with the best practices for building with generative AI into a single, easy-to-use library. It's a serious tool, but we hope you have fun with it.</p> <p>Marvin is open-source, free to use, and made with \ud83d\udc99 by the team at Prefect.</p> Explain Marvin like I'm 5 (I'm a technical 5-year-old)(I'm not technical) <p>Marvin lets your software speak English and ask questions to Large Language Models.</p> <p>It introspects the types and docstrings of your functions and data models, and automatically renders them as prompts for an LLM. You write code as you would normally, rather than prompts, and Marvin handles the back-and-forth translation.</p> <p>This lets you focus on what you've always focused on: writing clean, versioned, reusable code and data models, and not scrutinizing whether you begged your LLM hard enough to output JSON or needed to offer it a bigger tip for the right answer.</p> <p>Extracting, generating, cleaning, or classifying data is as simple as writing a function or a data model.</p> <p>Marvin lets engineers who know Python use Large Language Models without needing to write prompts.</p> <p>It turns out that ChatGPT and other Large Language Models are good at performing boring but incredibly valuable business-critical tasks beyond being a chatbot: you can use them to classify emails as spam, extract key figures from a report - exactly however you want for your scenario. When you use something like ChatGPT you spend a lot of time crafting the right prompt or context to get it to write your email, plan your date night, etc.</p> <p>If you want your software to use ChatGPT, you need to let it turn its objective into English. Marvin handles this 'translation' for you, so you get to just write code like you normally would. Engineers like using Marvin because it lets them write software like they're used to.</p> <p>Simply put, it lets you use Generative AI without feeling like you have to learn a framework.</p> <p>What's it like to use Marvin?</p> Classify textExtract entitiesStructure text and imagesGenerate dataCustom AI functions <p>Classify text using a set of labels:</p> <pre><code>import marvin\n\nresult = marvin.classify(\n    \"Marvin is so easy to use!\",\n    labels=[\"positive\", \"negative\"],\n)\n</code></pre> <p>Result</p> <pre><code>assert result == \"positive\"\n</code></pre> <p>Extract product features from user feedback:</p> <pre><code>import marvin\n\nfeatures = marvin.extract(\n    \"I love my new phone's camera, but the battery life could be improved.\",\n    instructions=\"product features\",\n)\n</code></pre> <p>Result</p> <pre><code>features == ['camera', 'battery life']\n</code></pre> <p>Marvin can convert natural language to a structured form:</p> <pre><code>import marvin\nfrom pydantic import BaseModel, Field\n\n\nclass Location(BaseModel):\n    city: str\n    state: str = Field(description='2-letter abbreviation')\n\n\nresult = marvin.cast('the big apple', Location)\n</code></pre> <p>Result</p> <pre><code>assert result == Location(city=\"New York\", state=\"NY\")\n</code></pre> <p>Marvin is multimodal, with beta support for using images as inputs. As an example, let's compare this photo to a shopping list to generate a list of missing items:</p> <p></p> <pre><code>import marvin\n\nshopping_list = [\"bagels\", \"cabbage\", \"eggs\", \"apples\", \"oranges\"]\ngroceries = marvin.Image(\n    \"https://images.unsplash.com/photo-1588964895597-cfccd6e2dbf9\",\n)\n\nmissing_items = marvin.cast(\n    groceries, \n    target=list[str], \n    instructions=f\"Did I forget anything on my list: {shopping_list}?\",\n)\n</code></pre> <p>Result</p> <pre><code>assert missing_items == [\"eggs\", \"oranges\"]\n</code></pre> <p>Generate synthetic data from a schema and instructions:</p> <pre><code>import marvin\nfrom pydantic import BaseModel, Field\n\n\nclass Location(BaseModel):\n    city: str\n    state: str = Field(description='2-letter abbreviation')\n\n\nresult = marvin.generate(\n    n=4,\n    target=Location,\n    instructions='US cities named after presidents',\n)\n</code></pre> <p>Result</p> <pre><code>result == [\n    Location(city=\"Washington\", state=\"DC\"),\n    Location(city=\"Jefferson City\", state=\"MO\"),\n    Location(city=\"Lincoln\", state=\"NE\"),\n    Location(city=\"Madison\", state=\"WI\"),\n]\n</code></pre> <p>Marvin functions let you combine any inputs, instructions, and output types to create custom AI-powered behaviors.</p> <pre><code>import marvin\n\n\n@marvin.fn\ndef list_fruits(n: int, color: str) -&gt; list[str]:\n    \"\"\"Generates a list of `n` fruits that are `color`\"\"\"\n\n\nfruits = list_fruits(3, color='red')\n</code></pre> <p>Result</p> <pre><code>fruits == [\"apple\", \"cherry\", \"strawberry\"]\n</code></pre> <p>Note that <code>list_fruits</code> has no source code. Marvin's components turn your function into a prompt, ask AI for its most likely output, and parses its response.</p>"}]}